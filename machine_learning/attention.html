<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Attention</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Attention</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org000004f">1. Attention</a>
<ul>
<li><a href="#org0000016">1.1. Self Attention</a>
<ul>
<li><a href="#org0000001">1.1.1. Multi-Head Self Attention</a></li>
<li><a href="#org000003b">1.1.2. Cross Attention</a></li>
<li><a href="#org0000010">1.1.3. Self Attention vs. RNN/CNN/FFN</a></li>
<li><a href="#org0000013">1.1.4. Computational Complexity</a></li>
</ul>
</li>
<li><a href="#org0000027">1.2. Bahdanau Attention</a>
<ul>
<li><a href="#ID-2c8d1d7d-388d-4734-b55d-e50ad26a511b">1.2.1. Seq2Seq Model</a></li>
<li><a href="#org0000024">1.2.2. Attention as Decoder</a></li>
</ul>
</li>
<li><a href="#org0000040">1.3. Transformer</a>
<ul>
<li><a href="#org000002b">1.3.1. InputEmbedding</a></li>
<li><a href="#org000002e">1.3.2. OutputEmbedding</a></li>
<li><a href="#org0000031">1.3.3. FeedForward</a></li>
<li><a href="#org0000034">1.3.4. Add&amp;Norm</a></li>
<li><a href="#org0000037">1.3.5. Masked Self Attention</a></li>
<li><a href="#org000003a">1.3.6. Cross Attention</a></li>
<li><a href="#org000003d">1.3.7. Positional Encoding</a></li>
</ul>
</li>
<li><a href="#org000004c">1.4. BERT</a>
<ul>
<li><a href="#org0000043">1.4.1. Train BERT</a></li>
<li><a href="#org0000046">1.4.2. BERT Model Size</a></li>
<li><a href="#org0000049">1.4.3. Fine Tune</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org000004f" class="outline-2">
<h2 id="org000004f"><span class="section-number-2">1</span> Attention</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>
</p>

<p>
<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=23">李宏毅机器学习</a>
</p>

<p>
<a href="https://d2l.ai/chapter_attention-mechanisms/index.html">https://d2l.ai/chapter_attention-mechanisms/index.html</a>
</p>

<p>
<a href="https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a">https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a</a>
</p>
</div>

<div id="outline-container-org0000016" class="outline-3">
<h3 id="org0000016"><span class="section-number-3">1.1</span> Self Attention</h3>
<div class="outline-text-3" id="text-1-1">

<div id="org0000000" class="figure">
<p><img src="../extra/attention.png" alt="attention.png" />
</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">AttentionLayer</span>(nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-comment-delimiter"># </span><span class="org-comment">(I, O)</span>
        <span class="org-keyword">self</span>.Q = nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-keyword">self</span>.Q = nn.init.xavier_uniform_(<span class="org-keyword">self</span>.Q)
        <span class="org-comment-delimiter"># </span><span class="org-comment">K</span>
        <span class="org-keyword">self</span>.K = nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-keyword">self</span>.K = nn.init.xavier_uniform_(<span class="org-keyword">self</span>.K)
        <span class="org-comment-delimiter"># </span><span class="org-comment">V</span>
        <span class="org-keyword">self</span>.V = nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-keyword">self</span>.V = nn.init.xavier_uniform_(<span class="org-keyword">self</span>.V)

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-comment-delimiter"># </span><span class="org-comment">input: (B, N, I)</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">Q: (B, N, O)</span>
        <span class="org-variable-name">Q</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.Q)
        <span class="org-comment-delimiter"># </span><span class="org-comment">K: (B, N, O)</span>
        <span class="org-variable-name">K</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.K)
        <span class="org-comment-delimiter"># </span><span class="org-comment">V: (B, N, O)</span>
        <span class="org-variable-name">V</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.V)
        <span class="org-comment-delimiter"># </span><span class="org-comment">dot_product(Q, K)</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">dot product &#21487;&#20197;&#29992;&#26469;&#24230;&#37327;&#20004;&#20010;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;, dot</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">product &#36234;&#22823;, &#30456;&#20284;&#24230;&#36234;&#22823;. dot_product(a,b)=|a|*|b|*cos(a,b)</span>
        <span class="org-variable-name">dot</span> = torch.bmm(Q, K.transpose(1, 2))
        <span class="org-comment-delimiter"># </span><span class="org-comment">dot: (B, N, N)</span>
        <span class="org-variable-name">score</span> = F.softmax(dot, dim=-1)
        <span class="org-variable-name">out</span> = torch.bmm(score, V)
        <span class="org-comment-delimiter"># </span><span class="org-comment">out: (B, N, O)</span>
        <span class="org-keyword">return</span> out
</pre>
</div>

<p>
Self Attention 的输入是 (B,N,I), 输出为 (B,N,O), 其中:
</p>

<ul class="org-ul">
<li>B 为 batch</li>
<li>N 为序列长度</li>
<li>I,O 为序列中每个向量的长度</li>
</ul>
</div>

<div id="outline-container-org0000001" class="outline-4">
<h4 id="org0000001"><span class="section-number-4">1.1.1</span> Multi-Head Self Attention</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
使用多个 self.{K,Q,V}, 计算出多个 attention score, 然后把这些 score 做 concat 后再用一个 linear 层转换为单个 score, 即为 multi-head self attention.
</p>

<p>
为什么要用多个 head? 大概和 conv 为什么要用多个 channel 差不多吧
</p>
</div>
</div>

<div id="outline-container-org000003b" class="outline-4">
<h4 id="org000003b"><span class="section-number-4">1.1.2</span> Cross Attention</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
self attention 是 cross attention 的特例. 
</p>

<p>
正常情况下我们是计算 source 和 target 两个序列的 cross attention, 其中 Q 属于
source, {K,V} 属于target.
</p>

<p>
当 source 与 target 相同时, 即为 self attention.
</p>
</div>
</div>

<div id="outline-container-org0000010" class="outline-4">
<h4 id="org0000010"><span class="section-number-4">1.1.3</span> Self Attention vs. RNN/CNN/FFN</h4>
<div class="outline-text-4" id="text-1-1-3">
</div>
<div id="outline-container-org0000007" class="outline-5">
<h5 id="org0000007"><span class="section-number-5">1.1.3.1</span> FFN</h5>
<div class="outline-text-5" id="text-1-1-3-1">
<p>
self attention 与 ffn (feed forward network) 也类似, 因为它能看到整个序列的数据,
但它的参数量很小
</p>
</div>
</div>

<div id="outline-container-org000000a" class="outline-5">
<h5 id="org000000a"><span class="section-number-5">1.1.3.2</span> CNN</h5>
<div class="outline-text-5" id="text-1-1-3-2">
<p>
self attention 与 cnn 类似, 能共享参数 (self.{K,Q,V}), 处理变长数据. 如果把
attention score 类比成 cnn 的 receptive field, 是否可以粗略的认为 multi-head
self attention 是 receptive field 会变化的 cnn?
</p>

<p>
有研究表明 multi-head self attention 包含任意 cnn 的能力: 
<a href="https://arxiv.org/pdf/1911.03584.pdf">ON THE RELATIONSHIP BETWEEN SELF-ATTENTION AND CONVOLUTIONAL LAYERS</a>
</p>
</div>
</div>

<div id="outline-container-org000000d" class="outline-5">
<h5 id="org000000d"><span class="section-number-5">1.1.3.3</span> RNN</h5>
<div class="outline-text-5" id="text-1-1-3-3">
<p>
self attention 与 rnn 类似, 可以处理变长序列, 甚至它的输入输出的 shape 与标准的
rnn 都是相同的. 但 self attention 可以解决 rnn 的几个问题:
</p>

<ol class="org-ol">
<li>basic rnn 一般只能看到序列中前面部分的数据</li>

<li>rnn 通过 hidden state 能传递的 context 有限, 因为 <a href="rnn.html#ID-5bba409c-0a66-4fba-9925-961db87e0e58">Basic RNN 难以训练</a>, 无法处理太长的序列</li>

<li><b>rnn 只能串行的处理序列</b></li>
</ol>

<p>
(ps. lstm 和 bi-directional rnn 一定程度上可以解决前两个问题, 所以并没有说 lstm
一定没有 attention 好用)
</p>

<p>
用 rnn 处理序列时最大的问题是记不住太长的上下文, 例如下面的翻译问题:
</p>

<p>
"it is raining cats and dogs" -&gt; "天下着倾盆大雨 "
</p>

<p>
当 rnn 处理到 `dogs`, 它需要 `记住` 前面有过 `raining` 和 `cats`, 才不会把 `dog`
翻译成 `狗`.
</p>

<p>
self attention 与 rnn 相比:
</p>

<ol class="org-ol">
<li>它可以看到整个序列的信息, 所以也没有单向 rnn 的问题.</li>

<li>它可以看到整个序列的信息, 相当于把很 `深` 的 rnn 变成很 `宽` 的网络, 避免 rnn
序列过长时的性能问题</li>

<li>self attention 可以并行</li>

<li>通过 attention score, self attention 可以知道 `dogs` 与 `raining` 之间的
score 更高, 所以它不仅能看到整个序列, 还能知道序列中哪些数据更重要.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org0000013" class="outline-4">
<h4 id="org0000013"><span class="section-number-4">1.1.4</span> Computational Complexity</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>
假设 {input, outut} vector 的长度均为 d, 序列长度为 n:
</p>

<ul class="org-ul">
<li><p>
self attention 的计算量大约为 \(n^2*d\)
</p>

<p>
\(n^2\): 序列中任何两个 item 之间都需要计算 attention
</p>

<p>
\(d\): 单个 attention 主要的计算量是 dot product
</p></li>

<li><p>
rnn 的计算量大约是 \(n*d^2\)
</p>

<p>
\(n\): rnn 是顺序计算 item
</p>

<p>
\(d^2\): rnn cell 里用了多个 ffn 把 input vector 转换为 output vector
</p></li>

<li><p>
cnn 的计算量大约是 \(k*n*d^2\)
</p>

<p>
为了处理序列, 假设 cnn 是 cnn1d, input/output channel 为 d, k 为 kernel 大小,
n 为序列长度
</p>

<p>
\(n\): 一共 n 个输出
</p>

<p>
\(k*d^2\): 每个输出需要的计算量
</p></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org0000027" class="outline-3">
<h3 id="org0000027"><span class="section-number-3">1.2</span> Bahdanau Attention</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a> 2016/5
</p>

<p>
bahdanau attention 是把 cross attention 应用到 seq2seq 的尝试.
</p>
</div>

<div id="outline-container-ID-2c8d1d7d-388d-4734-b55d-e50ad26a511b" class="outline-4">
<h4 id="ID-2c8d1d7d-388d-4734-b55d-e50ad26a511b"><span class="section-number-4">1.2.1</span> Seq2Seq Model</h4>
<div class="outline-text-4" id="text-1-2-1">

<div id="org0000019" class="figure">
<p><img src="../extra/seq2seq.png" alt="seq2seq.png" />
</p>
</div>
</div>

<div id="outline-container-org000001a" class="outline-5">
<h5 id="org000001a"><span class="section-number-5">1.2.1.1</span> test seq2seq model</h5>
<div class="outline-text-5" id="text-1-2-1-1">
<ol class="org-ol">
<li>encoder 输入序列为 `hello world`, 产生一个 hidden state (上图中的 thought
vector)</li>
<li>decoder 每一个输入为 &lt;START&gt;, 产生 x</li>
<li>decoder 每二个输入为 argmax(x), 产生 y</li>
<li>decoder 每二个输入为 argmax(y), 产生 z</li>
<li>&#x2026;</li>
</ol>

<p>
对于一个训练好的模型, 我们期望 argmax(x), argmax(y), argmax(z), argmax(w),
argmax(k), 分别是 `你`,`好`,`世`,`界`,&lt;END&gt;
</p>

<p>
ps. 每次输出都选 argmax 做为下一次的输入实际上并不一定是最好的结果, 这种做法相当于 greedy search, 还有一种称为 beam search 的方法 (实际上就是 DP)
</p>
</div>
</div>

<div id="outline-container-org000001d" class="outline-5">
<h5 id="org000001d"><span class="section-number-5">1.2.1.2</span> train seq2seq model</h5>
<div class="outline-text-5" id="text-1-2-1-2">
<p>
假设一条输入样本为 `hello world`, 标签为 `你好世界`
</p>

<ol class="org-ol">
<li>encoder 输入序列为 `hello world`, 产生一个 hidden state</li>
<li>decoder 每一个输入为 &lt;START&gt;, 产生数据需要与 `你` 来计算 loss</li>
<li>decoder 第二个输入为 `你`, 产生数据需要与 `好` 来计算 loss</li>
<li>`世` -&gt; `界`</li>
<li>`界` -&gt; &lt;END&gt;</li>
</ol>

<p>
可见 train 与 inference 的方法并完全不相同, 因为训练时没有把第一个输入的输出做为第二个输入, 而是直接使用了标签中的数据, 这种方法叫 teacher forcing. teacher
forcing 可以让 seq2seq 训练的快一些, 它意在避免训练时 `错上加错` 的情形.
</p>
</div>
</div>



<div id="outline-container-org0000050" class="outline-5 references">
<h5 id="org0000050">Backlinks</h5>
<div class="outline-text-5" id="text-org0000050">
<p>
<a href="rnn.html#ID-8f0c1075-5073-4219-b3d9-5088c8729c6a">RNN</a>
(<i>RNN &gt; Seq2Seq Model</i>):   <a href="#ID-2c8d1d7d-388d-4734-b55d-e50ad26a511b">Seq2Seq Model</a>
</p>
</div>
</div>
</div>


<div id="outline-container-org0000024" class="outline-4">
<h4 id="org0000024"><span class="section-number-4">1.2.2</span> Attention as Decoder</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
seq2seq 的问题是 hidden state 做为 decoder context 无法保存很多信息.
</p>

<p>
bahdanau attention 的做法是保留 encoder 的所有输出做为 context, 在 decoder 端计算 cross attention (Q 来自 decoder, K, V 来自 context) 做为 `hidden state`
</p>


<div id="org0000023" class="figure">
<p><img src="../extra/bahdanau_attention.png" alt="bahdanau_attention.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000040" class="outline-3">
<h3 id="org0000040"><span class="section-number-3">1.3</span> Transformer</h3>
<div class="outline-text-3" id="text-1-3">
<p>
2017 年 google 提出了 transformer: <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>, google translate
基于 transformer 实现. 与 bahdanau attention 有些类似:
</p>

<ol class="org-ol">
<li>transformer 也是一个 encoder-decoder 结构</li>

<li>transformer 也使用了 attention 做为 decoder</li>

<li>不同的是 transformer 使用了 self attention 做为 encoder, 代替原来的 rnn</li>
</ol>


<div id="org000002a" class="figure">
<p><img src="../extra/transformer.png" alt="transformer.png" />
</p>
</div>
</div>

<div id="outline-container-org000002b" class="outline-4">
<h4 id="org000002b"><span class="section-number-4">1.3.1</span> InputEmbedding</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
InputEmbedding 通常是一个 word embedding (<a href="word2vec.html#ID-164e18f2-14b2-486b-8e39-8c69d9f09bc1">Word2vec</a>), 与 rnn 不同的是,
InputEmbedding 可以被并行处理.
</p>
</div>
</div>

<div id="outline-container-org000002e" class="outline-4">
<h4 id="org000002e"><span class="section-number-4">1.3.2</span> OutputEmbedding</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
这里的 OutputEmbedding 指的是训练时的 teacher forcing 方法使用的输入, 即标签数据.
OutputEmbedding 在 train 时可以被并行处理.
</p>
</div>
</div>

<div id="outline-container-org0000031" class="outline-4">
<h4 id="org0000031"><span class="section-number-4">1.3.3</span> FeedForward</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
FeedForward 指 Position-Wise Feed Forward Network, 实际是就是 (fc+fc+relu) 组成的网络, 不同的是指针对序列中的每一项都使用同一个 ffn 去处理 (position-wise), 所以 ffn 的输入输出的规格与 rnn, self attention 是类似的.
</p>

<p>
为什么要 position-wise? 因为输入序列是变长的, 而 ffn 无法处理变长的数据.
</p>
</div>
</div>

<div id="outline-container-org0000034" class="outline-4">
<h4 id="org0000034"><span class="section-number-4">1.3.4</span> Add&amp;Norm</h4>
<div class="outline-text-4" id="text-1-3-4">
<ul class="org-ul">
<li>Add 是 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 中的 skip connection</li>
<li>Norm 是 <a href="batchnorm.html#ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4">Layer Normalization</a></li>
</ul>
</div>
</div>

<div id="outline-container-org0000037" class="outline-4">
<h4 id="org0000037"><span class="section-number-4">1.3.5</span> Masked Self Attention</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
decoder 的第一个 self attention 称为 masked self attention: 对于 \(x_n\), 它只会计算 \(x_n\) 与 \(\{x_i|i <=n\}\) 的 attention.
</p>

<p>
decoder 是一个 auto-regressive (自回归) 模型: 在 test 时 decoder 一步一步产生输出, 且上一步的输出会做为下一步的输入. 所以 self attention 在 test 时无法一次性
`看到` 所有的输入.
</p>

<p>
test 时 self attention 的计算过程为:
</p>

<ol class="org-ol">
<li>output embedding input_1 = &lt;START&gt;, 计算 &lt;START&gt; 和 {&lt;START&gt;} 的 attention,
decode_1 = `你`</li>
<li>output embedding input_2 = `你`, 计算 `你` 和 {&lt;START&gt;, `你`} 的 attention,
decode_2 = `好`</li>
<li>output embedding input_3 = `好`, 计算 `好` 和 {&lt;START&gt;, `你`, `好`} 的
attention, decode_3 = '世'</li>
<li>&#x2026;</li>
</ol>

<p>
通过 mask 可以保证 {train, test} 时计算 attention 的行为是一致的.
</p>
</div>
</div>


<div id="outline-container-org000003a" class="outline-4">
<h4 id="org000003a"><span class="section-number-4">1.3.6</span> Cross Attention</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
Decoder 的第二个 attention 是一个 cross attention (而非 self attention), 其中:
</p>

<ul class="org-ul">
<li>Q 来自 decoder</li>
<li>K,V 来自 encoder</li>
</ul>

<p>
通过这个 attention, encoder 的输出被导入到 decoder 中 (相当于 seq2seq 中的
hidden state, 但比单一的 hidden state 包含更多的信息), 所以有时也把它叫做
encoder-decoder attention
</p>
</div>
</div>

<div id="outline-container-org000003d" class="outline-4">
<h4 id="org000003d"><span class="section-number-4">1.3.7</span> Positional Encoding</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
对于 self attention 来说, 针对 `bug me not` 和 `bug not me` 两个序列分别计算
attention 时, 对应于 `bug` 的 attention 结果是没有区别的, 因为 attention 只是加权求和, 并没有考虑序列的顺序.
</p>

<p>
因此, self attention 会把序列中的每个元素的索引 encoding 后加进去.
</p>

<ol class="org-ol">
<li>positional encoding 的长度与 embedding 相同, 以便可以直接相加</li>
<li><p>
positional encoding 的计算方法有很多, 例如如下的方法:
</p>

<p>
\(PE(pos,2i)=sin(pos/10000^{2i/embedding\_size})\)
\(PE(pos,2i+1)=cos(pos/10000^{2i/embedding\_size})\)   
</p></li>
</ol>

<p>
例如:
</p>

<ol class="org-ol">
<li>假设 `hello word` 之前的 embedding 是 ((1,2,3,1),(2,1,8,6)),</li>
<li>手工定义 0 对应的 encoding 为 (e00,e01,e02,e03), 1 对应 (e10,e11,e12,e13)</li>
<li>加入 positional encoding 后 hello world 会变成 ((1+e00,2+e01,3+e02,1+e03),(2+e10,1+e11,8+e12,6+e13))</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org000004c" class="outline-3">
<h3 id="org000004c"><span class="section-number-3">1.4</span> BERT</h3>
<div class="outline-text-3" id="text-1-4">
<p>
BERT 使用 transformer 的 encoder 部分, 使用自监督学习的方法训练出来一个通用的
pre-train 模型.
</p>

<p>
其它基于 BERT 的下游任务会使用用监督学习的方法训练出许多针对不同任务的模型.
</p>
</div>

<div id="outline-container-org0000043" class="outline-4">
<h4 id="org0000043"><span class="section-number-4">1.4.1</span> Train BERT</h4>
<div class="outline-text-4" id="text-1-4-1">
<ol class="org-ol">
<li>给定大量的文本, 例如 `how are you`</li>
<li>随机把某个词 mask 掉 (例如把那个词替换成某个特殊的标记 &lt;MASK&gt;), 变成 `how
&lt;MASK&gt; you`</li>
<li>训练时它的输出序列的第二个输出的标签是 `are`</li>
</ol>

<p>
BERT 实际上类似于 word2vec, 随机 mask 的作法和 <a href="word2vec.html#ID-8cf28ef6-4d2d-45aa-9bba-f8b65de82b8f">cbow</a> 也有些类似, 但它用起来与
word2vec 并不完全相同:
</p>

<ol class="org-ol">
<li>word2vec 输入是一个词, 针对同一个 word 会产生同样的 embedding</li>

<li>bert 的输入是一个序列, 不同的输入序列例如 `running dog` 和 `running cat` 针对
`running` 产生的 embedding 未必一模一样</li>
</ol>
</div>
</div>

<div id="outline-container-org0000046" class="outline-4">
<h4 id="org0000046"><span class="section-number-4">1.4.2</span> BERT Model Size</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
BERT base 模型的大小:
</p>

<ul class="org-ul">
<li>12 layers (transformer blocks)</li>
<li>12 attention heads</li>
<li>110 million parameters</li>
<li>output size of 768-dimensions.</li>
</ul>

<p>
BERT large 模型的大小:
</p>

<ul class="org-ul">
<li>24 layers (transformer blocks)</li>
<li>16 attention heads</li>
<li>340 million parameters</li>
<li>output size of 1024-dimensions.</li>
</ul>

<p>
BERT base 训练:
</p>

<ul class="org-ul">
<li>3 billion words</li>
<li>1 million epoch</li>
<li>8 days with google TPU</li>
</ul>
</div>
</div>

<div id="outline-container-org0000049" class="outline-4">
<h4 id="org0000049"><span class="section-number-4">1.4.3</span> Fine Tune</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
训练好的 BERT 相当于一个能够做填空题的 encoder, 然后许多任务可以写成这种 `填空题
` 的形式, 先定义一个模型, 在 BERT 之上加上一个简单的 linear 层, 然后提供标签数据在 BERT 基础之上再训练 (fine-tune), 例如:
</p>

<ol class="org-ol">
<li><p>
判断句子的 class
</p>

<p>
输入为: &lt;MASK&gt; hello world, 标签是 class
</p>

<p>
虽然 BERT 这里会输出 3 个值, 但只需要关注第一个对应 &lt;MASK&gt; 的值
</p></li>

<li><p>
词性标著
</p>

<p>
输入为: hello world, 标签是 `vt. n.`
</p></li>

<li><p>
判断两个句子是否有矛盾
</p>

<p>
输入为: &lt;MASK&gt; he is drinking &lt;SEP&gt; he is dead, 标签是 `Y`
</p></li>
</ol>
</div>
</div>
</div>
</div>


<div id="outline-container-org0000052" class="outline-2 references">
<h2 id="org0000052">Backlinks</h2>
<div class="outline-text-2" id="text-org0000052">
<p>
<a href="cnn.html#ID-bad26e8b-7690-4654-9ccc-c1e9044113a1">CNN</a>
(<i>CNN &gt; Overview</i>):  cnn 适用于图像识别是因为通过 filter 可以找到符合某种空间结构的 local pattern, 所 以 cnn 工作时有依赖于一个基本假设: 数据有空间结构. 如果给一个 cnn 模型输入的图像 是按像素随机 shuffle 过的图像, 则 cnn 无法工作. 类似 <a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">attention</a> 的模型也许能处理 这种不包含空间结构的数据.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2020-09-25 Fri 00:00<br />
Last updated: 2022-02-07 Mon 20:25</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
