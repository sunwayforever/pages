<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>ResNet</title>


           <link rel="stylesheet" type="text/css" href="/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="./htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="../htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="/readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="./readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="../readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
</head>
<body>
<div id="content" class="content">
<h1 class="title">ResNet</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org000000e">1. ResNet</a>
<ul>
<li><a href="#org0000001">1.1. Overview</a></li>
<li><a href="#org0000008">1.2. 不同大小的 ResNet</a>
<ul>
<li><a href="#ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b">1.2.1. bottleneck</a></li>
</ul>
</li>
<li><a href="#org000000b">1.3. 示例代码</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org000000e" class="outline-2">
<h2 id="org000000e"><span class="section-number-2">1.</span> ResNet</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://arxiv.org/pdf/1512.03385.pdf">https://arxiv.org/pdf/1512.03385.pdf</a> 2015/12, msra
</p>
</div>

<div id="outline-container-org0000001" class="outline-3">
<h3 id="org0000001"><span class="section-number-3">1.1.</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">

<div id="org0000000" class="figure">
<p><img src="../extra/resnet.png" alt="resnet.png" />
</p>
</div>

<p>
以 ResNet-18 为例:
</p>

<ul class="org-ul">
<li>18 指一共 18 层</li>

<li>输入为 224x224x3 的图片</li>

<li>conv1: 7x7 conv,64,/2 加一个 3x3 max pool, /2, 输出为 224/2/2=56x56x64</li>

<li>conv2_x: (3x3 conv, 64, /1 * 2) * 2, 输出为 56x56x64, 共 4 层</li>

<li>conv3_x: (3x3 conv, 128 * 2) *2, 其中 第一层的 stride 为 2, 其它层为 1, 输出为
28x28x128, 共 4 层</li>

<li>conv4_x: (3x3 conv, 256 * 2) *2, 其中 第一层的 stride 为 2, 其它层为 1, 输出为
14x14x256, 共 4 层</li>

<li>conv5_x: (3x3 conv, 512 * 2) *2, 其中 第一层的 stride 为 2, 其它层为 1, 输出为
7x7x512,共 4 层</li>

<li>fc: 首先一个 (7x7 avg pooling), 输出为 1x1x512, 然后一个 (512, 1000) 的 fc, 输出为 1000, 然后一个 softmax</li>
</ul>

<p>
共 18 层
</p>

<p>
conv{2,3,4,5}_x 四个 layer 结构类似:
</p>

<ol class="org-ol">
<li>每个 layer 都包含两个 block</li>

<li>每个 block 包含两个 3x3 conv.</li>

<li><p>
conv{3,4,5}_x layer 的 block_0 的 conv_0 为 (3x3, input*2, /2), (H,W) shape
减半, output_channel 倍增
</p>

<p>
其它 conv 均为 (3x3, input, /1), 即 output shape 保持不变
</p></li>

<li>每个 layer 的第一个 block 的 input 需要通过一个 (1x1, 2*input_channel, /2) 的
conv 转换以后再和 block 的 output 相加, 保持 shape 一致. conv2_x layer 除外,
因为它的 block_0 的 input 与 output 大小相同</li>

<li>其它 block 的 input 直接加到 block 的 output 即可</li>
</ol>

<p>
上图中, 虚线的 shortcut 表示 block input 需要先经过一个 (1x1 conv, 2*input, /2)
处理后再加到 block 的 output 上, 因为 shape 不匹配无法直接相加
</p>

<p>
实线表示 shortcut input 可以直接加到 block output 上
</p>

<p>
相同的颜色表示同一个 layer, 除了 conv2_x, 其它 layer 的一个 conv 的 stride 为 2,
且 output_channel 倍增
</p>
</div>
</div>

<div id="outline-container-org0000008" class="outline-3">
<h3 id="org0000008"><span class="section-number-3">1.2.</span> 不同大小的 ResNet</h3>
<div class="outline-text-3" id="text-1-2">
<p>
所有 ResNet 均包含四个 layer, 每个 layer 有 N 个不同的 block, block 之间有
shortcut connection (或者叫 skip connection)
</p>

<p>
根据 block 的类型和个数, 有 18, 34, 50, 101 和 152 几种不同的配置.
</p>

<p>
18, 34 使用的 block 包含两层 conv, 称为 basic block
</p>

<p>
大部分 conv 的 stride 为 1, 只有两个 layer 交界处的 conv 的 stride 会是 2, 相应的其 output_channel 会变成 input_channel 的两倍
</p>


<div id="org0000004" class="figure">
<p><img src="../extra/resnet_all.png" alt="resnet_all.png" />
</p>
</div>
</div>

<div id="outline-container-ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b" class="outline-4">
<h4 id="ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b"><span class="section-number-4">1.2.1.</span> bottleneck</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
50, 101, 152 使用的 block 包含三层 conv, 称为 bottleneck
</p>

<p>
bottleneck block 把 basic block (3x3,output_channel; 3x3, output_channel) 变成
(1x1, output_channel/4; 3x3, output_channel/4; 1x1, output_channel)
</p>

<p>
resnet 的 bottleneck 是先用 1x1 减小 channel, 然后 conv 完再用 1x1 增大 channel,
以减小计算.
</p>
</div>


<div id="outline-container-org0000005" class="outline-5 references">
<h5 id="org0000005">Backlinks</h5>
<div class="outline-text-5" id="text-org0000005">
<p>
<a href="enet.html#ID-f44e5065-f0ab-4f6c-bb43-1baba0e62326">ENet</a>
(<i>ENet &gt; Network &gt; bottleneck</i>):  bottleneck 类似于 resnet 的 <a href="#ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b">bottleneck</a> 结构, 做了一点修改:
</p>

<p>
<a href="mobilenet.html#ID-95b517db-933c-4029-ad48-c11ae0dd0719">MobileNetV2</a>
(<i>MobileNet &gt; MobileNetV2</i>):  MobileNetV2 引入了 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 的 <a href="#ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b">bottleneck</a>, 但做了一点修改, 称为 `linear bottleneck`
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org000000b" class="outline-3">
<h3 id="org000000b"><span class="section-number-3">1.3.</span> 示例代码</h3>
<div class="outline-text-3" id="text-1-3">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">BasicBlock</span>(tf.keras.Model):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_channels, out_channels, strides<span class="org-operator">=</span>1):
        <span class="org-builtin">super</span>(BasicBlock, <span class="org-keyword">self</span>).__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">conv1</span> <span class="org-operator">=</span> tf.keras.layers.Conv2D(
            out_channels,
            kernel_size<span class="org-operator">=</span>3,
            strides<span class="org-operator">=</span>strides,
            padding<span class="org-operator">=</span><span class="org-string">"same"</span>,
            use_bias<span class="org-operator">=</span><span class="org-constant">False</span>,
        )
        <span class="org-keyword">self</span>.<span class="org-variable-name">bn1</span> <span class="org-operator">=</span> tf.keras.layers.BatchNormalization()

        <span class="org-keyword">self</span>.<span class="org-variable-name">conv2</span> <span class="org-operator">=</span> tf.keras.layers.Conv2D(
            out_channels,
            kernel_size<span class="org-operator">=</span>3,
            strides<span class="org-operator">=</span>1,
            padding<span class="org-operator">=</span><span class="org-string">"same"</span>,
            use_bias<span class="org-operator">=</span><span class="org-constant">False</span>,
        )
        <span class="org-keyword">self</span>.<span class="org-variable-name">bn2</span> <span class="org-operator">=</span> tf.keras.layers.BatchNormalization()

        <span class="org-doc">"""</span>
<span class="org-doc">        Adds a shortcut between input and residual block and merges them with "sum"</span>
<span class="org-doc">        """</span>
        <span class="org-keyword">if</span> strides <span class="org-operator">!=</span> 1 <span class="org-keyword">or</span> in_channels <span class="org-operator">!=</span> out_channels:
            <span class="org-keyword">self</span>.<span class="org-variable-name">shortcut</span> <span class="org-operator">=</span> tf.keras.Sequential(
                [
                    tf.keras.layers.Conv2D(
                        out_channels,
                        kernel_size<span class="org-operator">=</span>1,
                        strides<span class="org-operator">=</span>strides,
                        use_bias<span class="org-operator">=</span><span class="org-constant">False</span>,
                    ),
                    tf.keras.layers.BatchNormalization(),
                ]
            )
        <span class="org-keyword">else</span>:
            <span class="org-keyword">self</span>.<span class="org-variable-name">shortcut</span> <span class="org-operator">=</span> <span class="org-keyword">lambda</span> x, _: x

    <span class="org-keyword">def</span> <span class="org-function-name">call</span>(<span class="org-keyword">self</span>, x, training<span class="org-operator">=</span><span class="org-constant">False</span>):
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.bn1(<span class="org-keyword">self</span>.conv1(x), training<span class="org-operator">=</span>training)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.nn.relu(out)
        <span class="org-keyword">if</span> training:
            <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.nn.dropout(out, 0.1)
            <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.bn2(<span class="org-keyword">self</span>.conv2(out), training<span class="org-operator">=</span>training)
            <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.nn.relu(out)
        <span class="org-keyword">if</span> training:
            <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.nn.dropout(out, 0.1)
            <span class="org-variable-name">out</span> <span class="org-operator">+=</span> <span class="org-keyword">self</span>.shortcut(x, training)
        <span class="org-keyword">return</span> tf.nn.relu(out)


<span class="org-keyword">class</span> <span class="org-type">ResNet</span>(tf.keras.Model):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, block, num_blocks, num_classes<span class="org-operator">=</span>512):
        <span class="org-builtin">super</span>(ResNet, <span class="org-keyword">self</span>).__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">in_channels</span> <span class="org-operator">=</span> 32

        <span class="org-keyword">self</span>.<span class="org-variable-name">conv1</span> <span class="org-operator">=</span> tf.keras.layers.Conv2D(
            64,
            3,
            1,
            padding<span class="org-operator">=</span><span class="org-string">"same"</span>,
            use_bias<span class="org-operator">=</span><span class="org-constant">False</span>,
        )
        <span class="org-keyword">self</span>.<span class="org-variable-name">bn1</span> <span class="org-operator">=</span> tf.keras.layers.BatchNormalization()

        <span class="org-keyword">self</span>.<span class="org-variable-name">layer1</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>._make_layer(block, 64, num_blocks[0], stride<span class="org-operator">=</span>1)
        <span class="org-keyword">self</span>.<span class="org-variable-name">layer2</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>._make_layer(block, 128, num_blocks[1], stride<span class="org-operator">=</span>2)
        <span class="org-keyword">self</span>.<span class="org-variable-name">layer3</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>._make_layer(block, 256, num_blocks[2], stride<span class="org-operator">=</span>2)
        <span class="org-keyword">self</span>.<span class="org-variable-name">layer4</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>._make_layer(block, 512, num_blocks[3], stride<span class="org-operator">=</span>2)

        <span class="org-keyword">self</span>.<span class="org-variable-name">avg_pool2d</span> <span class="org-operator">=</span> tf.keras.layers.AveragePooling2D(7)
        <span class="org-keyword">self</span>.<span class="org-variable-name">linear</span> <span class="org-operator">=</span> tf.keras.layers.Dense(units<span class="org-operator">=</span>num_classes, activation<span class="org-operator">=</span><span class="org-string">"softmax"</span>)

    <span class="org-keyword">def</span> <span class="org-function-name">_make_layer</span>(<span class="org-keyword">self</span>, block, out_channels, num_blocks, stride):
        <span class="org-variable-name">strides</span> <span class="org-operator">=</span> [stride] <span class="org-operator">+</span> [1] <span class="org-operator">*</span> (num_blocks <span class="org-operator">-</span> 1)
        <span class="org-variable-name">layers</span> <span class="org-operator">=</span> []
        <span class="org-keyword">for</span> stride <span class="org-keyword">in</span> strides:
            layers.append(block(<span class="org-keyword">self</span>.in_channels, out_channels, stride))
            <span class="org-keyword">self</span>.<span class="org-variable-name">in_channels</span> <span class="org-operator">=</span> out_channels
        <span class="org-keyword">return</span> tf.keras.Sequential(layers)

    <span class="org-keyword">def</span> <span class="org-function-name">call</span>(<span class="org-keyword">self</span>, x, training<span class="org-operator">=</span><span class="org-constant">False</span>):
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.nn.relu(<span class="org-keyword">self</span>.bn1(<span class="org-keyword">self</span>.conv1(x), training))
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.layer1(out, training<span class="org-operator">=</span>training)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.layer2(out, training<span class="org-operator">=</span>training)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.layer3(out, training<span class="org-operator">=</span>training)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.layer4(out, training<span class="org-operator">=</span>training)

        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.avg_pool2d(out)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> tf.keras.layers.Flatten()(out)
        <span class="org-variable-name">out</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.linear(out)
        <span class="org-keyword">return</span> out


<span class="org-keyword">def</span> <span class="org-function-name">ResNet18</span>(nclass):
    <span class="org-keyword">return</span> ResNet(BasicBlock, [2, 2, 2, 2], num_classes<span class="org-operator">=</span>nclass)

</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org0000011" class="outline-2 references">
<h2 id="org0000011">Backlinks</h2>
<div class="outline-text-2" id="text-org0000011">
<p>
<a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">Attention</a>
(<i>Attention &gt; Transformer &gt; Add&amp;Norm</i>):  - Add 是 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 中的 skip connection - Norm 是 <a href="batchnorm.html#ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4">Layer Normalization</a>
</p>

<p>
<a href="deeplab.html#ID-976a44ab-2450-49ba-8f7b-8f257a3af005">DeepLab</a>
(<i>DeepLab &gt; DeepLabV3 Network</i>):     1. block{1,2,3} 都是 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">resnet</a> 的 block, 并且通过指定 stride = 2 达到 downsample 的       效果, 所以 output stride 依次是 4, 8, 16    
</p>

<p>
<a href="enet.html#ID-f44e5065-f0ab-4f6c-bb43-1baba0e62326">ENet</a>
(<i>ENet</i>):  ENet 做为一个 semantic segmentation 模型, 基本就是一个标准的 encoder-decoder 结 构, 并且参考了 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 和 <a href="inception.html#ID-c8e4ddb9-1372-4a28-99c3-93f7f3e56bdf">Inception</a> 的设计
</p>

<p>
<a href="image_classification.html#ID-187044e1-7a7a-4ec4-9608-65d31d394a8b">Image Classification</a>
(<i>Image Classification &gt; ResNet</i>):   <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a>
</p>

<p>
<a href="mobilenet.html#ID-95b517db-933c-4029-ad48-c11ae0dd0719">MobileNetV2</a>
(<i>MobileNet &gt; MobileNetV2</i>):  MobileNetV2 引入了 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 的 <a href="#ID-f64572b2-99aa-40a2-aeb9-12a3a3a0b32b">bottleneck</a>, 但做了一点修改, 称为 `linear bottleneck`
</p>

<p>
<a href="pspnet.html#ID-5ab05d23-629f-4f08-929a-9d52513c1ef4">PSPNet</a>
(<i>PSPNet &gt; Network &gt; backbone</i>):  pspnet 使用 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 50/101 做为 backbone 生成 feature map
</p>

<p>
<a href="unet.html#ID-568627dd-dea0-42f1-a19e-be76683cb9f6">UNet</a>
(<i>UNet &gt; Network</i>):      2. 灰色的 `copy and crop` 类似于 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">resnet</a> 的 skip connection, 但使用的是针对    channel 的 concat 而不是直接相加. 另外, 因为 concat 的两部分的 shape 不一致,    所以需要把大的 crop 成小的
</p>

<p>
<a href="vgg.html#ID-8b9be85a-94c9-4ab5-8da5-cb8fba14aa9a">Vgg</a>
(<i>Vgg &gt; Very Deep Convolutional Networks</i>):  当然 16 层的深度与 152 层的 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 相比还是小很多. vgg 之后, 更深的网络因为梯度 消失的问题无法工作, 才有了 ResNet.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2020-12-11 Fri 00:00<br />
Last updated: 2022-01-25 Tue 15:43</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>