<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-15 六 11:03 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Batch Normalization</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="../stylesheets/main.css" media="screen" />
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Batch Normalization</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org71350ad">1. Batch Normalization</a>
<ul>
<li><a href="#orgc989c8d">1.1. Overview</a></li>
<li><a href="#orgd625af2">1.2. DNN 的 batchnorm</a></li>
<li><a href="#org36cca9e">1.3. CNN 的 batchnorm</a></li>
<li><a href="#orgb3c270f">1.4. BatchNorm 导致很大的量化误差</a></li>
<li><a href="#org163f40f">1.5. batchnorm-&gt;relu 还是 relu-&gt;batchnorm</a>
<ul>
<li><a href="#orgd1e9e03">1.5.1. batchnorm-&gt;relu 的优点</a></li>
<li><a href="#org8caf8e7">1.5.2. relu-&gt;batchnorm 优点</a></li>
</ul>
</li>
<li><a href="#org69975b9">1.6. Layer Normalization</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org71350ad" class="outline-2">
<h2 id="org71350ad"><span class="section-number-2">1</span> Batch Normalization</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgc989c8d" class="outline-3">
<h3 id="orgc989c8d"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
我们调节 \(w\) 的目的是为了保证输出在一定的范围内, 例如符合标准正态分布.
</p>

<p>
batch normalization 不调整 \(w\): 它对输出作 normalization, 保证输出是标准正态分布,
batch normalization 与 feature scaling 时的 normalization 类似, `the mean and
standard-deviation are calculated per-dimension over the mini-batches`
</p>

<p>
batch normalization 的代码基本上为:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #268bd2;">mu</span> = np.mean<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> axis=-1<span style="color: #757575;">,</span> keepdims=<span style="color: #268bd2; font-weight: bold;">True</span><span style="color: #757575;">)</span>
<span style="color: #268bd2;">std</span> = np.std<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> axis=-1<span style="color: #757575;">,</span> keepdims=<span style="color: #268bd2; font-weight: bold;">True</span><span style="color: #757575;">)</span>
<span style="color: #268bd2;">X_normed</span> = <span style="color: #757575;">(</span>X - mu<span style="color: #757575;">)</span> / <span style="color: #757575;">(</span>std + <span style="color: #859900;">self</span>.epsilon<span style="color: #757575;">)</span>
<span style="color: #268bd2;">out</span> = <span style="color: #859900;">self</span>.gamma * X_normed + <span style="color: #859900;">self</span>.beta
</pre>
</div>

<p>
其中最后一步称为 scale and shift, self.gamma 和 self.beta 是可学习的参数, 为了调整 normalization 的结果.
</p>

<p>
以 relu 为例, 若直接使用 X_normed, 会导致有一半的 x 被丢弃 (X_normed 有 50% 的概率 &lt; 0). 如果通过 scale and shift 能把 X_normed 向正方向移动一下, 就没有问题了.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt
<span style="color: #859900;">import</span> torch

<span style="color: #268bd2;">N</span> = 100
plt.style.use<span style="color: #757575;">(</span><span style="color: #2aa198;">"default"</span><span style="color: #757575;">)</span>

<span style="color: #586e75;"># </span><span style="color: #586e75;">plt.ylim(0, 40)</span>


<span style="color: #859900;">def</span> <span style="color: #268bd2;">init_weight</span><span style="color: #757575;">(</span>in_features<span style="color: #757575;">,</span> out_features<span style="color: #757575;">)</span>:
    <span style="color: #859900;">return</span> torch.nn.Parameter<span style="color: #757575;">(</span>torch.randn<span style="color: #757575;">(</span>in_features<span style="color: #757575;">,</span> out_features<span style="color: #757575;">))</span>


<span style="color: #859900;">class</span> <span style="color: #b58900;">Layer</span><span style="color: #757575;">(</span>torch.nn.Module<span style="color: #757575;">)</span>:
    <span style="color: #859900;">def</span> <span style="color: #268bd2;">__init__</span><span style="color: #757575;">(</span><span style="color: #859900;">self</span><span style="color: #757575;">,</span> in_features<span style="color: #757575;">,</span> out_features<span style="color: #757575;">,</span> n<span style="color: #757575;">)</span>:
        <span style="color: #839496;">super</span><span style="color: #757575;">()</span>.__init__<span style="color: #757575;">()</span>
        <span style="color: #859900;">self</span>.n = n
        <span style="color: #859900;">self</span>.w = init_weight<span style="color: #757575;">(</span>in_features<span style="color: #757575;">,</span> out_features<span style="color: #757575;">)</span>
        <span style="color: #859900;">self</span>.bn = torch.nn.BatchNorm1d<span style="color: #757575;">(</span>in_features<span style="color: #757575;">)</span>

    <span style="color: #859900;">def</span> <span style="color: #268bd2;">forward</span><span style="color: #757575;">(</span><span style="color: #859900;">self</span><span style="color: #757575;">,</span> <span style="color: #839496;">input</span><span style="color: #757575;">)</span>:
        <span style="color: #268bd2;">ret</span> = torch.matmul<span style="color: #757575;">(</span><span style="color: #839496;">input</span><span style="color: #757575;">,</span> <span style="color: #859900;">self</span>.w<span style="color: #757575;">)</span>
        <span style="color: #268bd2;">ret</span> = <span style="color: #859900;">self</span>.bn<span style="color: #757575;">(</span>ret<span style="color: #757575;">)</span>
        <span style="color: #268bd2;">ret</span> = torch.nn.functional.sigmoid<span style="color: #757575;">(</span>ret<span style="color: #757575;">)</span>
        plt.subplot<span style="color: #757575;">(</span>1<span style="color: #757575;">,</span> 10<span style="color: #757575;">,</span> <span style="color: #859900;">self</span>.n<span style="color: #757575;">)</span>
        plt.xlim<span style="color: #757575;">(</span>0<span style="color: #757575;">,</span>1<span style="color: #757575;">)</span>
        plt.hist<span style="color: #757575;">(</span>ret.detach<span style="color: #757575;">()</span>.numpy<span style="color: #757575;">()</span>.reshape<span style="color: #757575;">(</span>-1<span style="color: #757575;">))</span>
        <span style="color: #859900;">return</span> ret


<span style="color: #859900;">def</span> <span style="color: #268bd2;">train</span><span style="color: #757575;">()</span>:
    <span style="color: #268bd2;">net</span> = torch.nn.Sequential<span style="color: #757575;">()</span>

    <span style="color: #859900;">for</span> i <span style="color: #859900;">in</span> <span style="color: #839496;">range</span><span style="color: #757575;">(</span>10<span style="color: #757575;">)</span>:
        net.add_module<span style="color: #757575;">(</span><span style="color: #2aa198;">"linear%d"</span> % <span style="color: #757575;">(</span>i<span style="color: #757575;">),</span> Layer<span style="color: #757575;">(</span>N<span style="color: #757575;">,</span> N<span style="color: #757575;">,</span> i + 1<span style="color: #757575;">))</span>

    <span style="color: #268bd2;">x</span> = torch.rand<span style="color: #757575;">(</span>10<span style="color: #757575;">,</span> N<span style="color: #757575;">)</span>
    net<span style="color: #757575;">(</span>x<span style="color: #757575;">)</span>
    plt.show<span style="color: #757575;">()</span>


train<span style="color: #757575;">()</span>
</pre>
</div>


<div id="org27c5d12" class="figure">
<p><img src="../extra/batch_normal.png" alt="batch_normal.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgd625af2" class="outline-3">
<h3 id="orgd625af2"><span class="section-number-3">1.2</span> DNN 的 batchnorm</h3>
<div class="outline-text-3" id="text-1-2">
<p>
DNN 的 batchnorm 是在 batch 的维度上进行平均, 即:
</p>

<pre class="example" id="orgf888b73">
batch1: a b c d e
batch2: x y z m n

batchnorm 后为:
batch1: a-u1, b-u2, c-u3...
batch2: x-u1, b-u2, c-u3...

u1 是 mean(a,x), u2 是 mean(b-y)...
</pre>

<p>
mean 的个数和 output 的 shape 相等
</p>
</div>
</div>

<div id="outline-container-org36cca9e" class="outline-3">
<h3 id="org36cca9e"><span class="section-number-3">1.3</span> CNN 的 batchnorm</h3>
<div class="outline-text-3" id="text-1-3">
<p>
把一个 channel 看做整体, CNN 的 batchnorm 与 DNN 类似, 即:
</p>

<pre class="example" id="orgb556ced">
batch1: channel1:a b c  channel2: a' b' c'
                 d e f            d' e' f'

batch2: channel1:x y z  channel2: x' y' z'
                 m n p            m' n' p'

batchnorm 后为:
batch1: channel1: a-u1 b-u1 c-u1  channel2: a'-u2 b'-u2...
                  ...                       ....
batch2: channel1: x-u1 y-u1 z-u1  channel2: x'-u2 y'-u2...                  
                  ...                       ...

u1 是  mean(a,b,c,d,e,f,x,y,z,m,n,p)
u2 是  mean(a',b',c',d',e',f',x',y',z',m',n',p')

</pre>

<p>
考虑到同一个 channel 的数据都是用同一个 kernel 计算出来的, mean 的个数和 kernel
的个数 (或 output channel 个数) 相同, 即以 channel 为单位计算 mean
</p>

<p>
tensorflow 的 BatchNormalization 函数接受一个参数 axis, 默认为 -1, 对于 DNN, -1
即 batch 之外那个 axis, 对于 CNN, -1 为 channel.
</p>

<p>
BatchNormalization(axis=-1) 与 sum(axis=-1) 中 axis 的意义不同: sum 的 axis 指去掉这一维, BatchNormalization 的 axis 指保留这一维
</p>
</div>
</div>

<div id="outline-container-orgb3c270f" class="outline-3">
<h3 id="orgb3c270f"><span class="section-number-3">1.4</span> <a href="../tensorflow/tflite_quantization_detail.html#orgf5acdb2">BatchNorm 导致很大的量化误差</a></h3>
</div>

<div id="outline-container-org163f40f" class="outline-3">
<h3 id="org163f40f"><span class="section-number-3">1.5</span> batchnorm-&gt;relu 还是 relu-&gt;batchnorm</h3>
<div class="outline-text-3" id="text-1-5">
</div>
<div id="outline-container-orgd1e9e03" class="outline-4">
<h4 id="orgd1e9e03"><span class="section-number-4">1.5.1</span> batchnorm-&gt;relu 的优点</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
tflite 使用 batchnorm-&gt;relu 时, 可以把 batchnorm 和 relu 融合 (fuse) 到前面一层
cnn 或 dnn, 但使用 relu-&gt;batchnorm 时无法融合, 因为 batchorm 在 inference 时相当于对 input 的线性变换, 如果它的前一层也是一个线性变换, 而两者可以很简单的合并成一个线性变换, 但两者之前加上一个 relu 就没法操作了, 因为 relu 并不是线性的
</p>

<p>
另一方面, relu 放在 batchnorm 之前可能会出来 <a href="../tensorflow/tflite_quantization_detail.html#orgf5acdb2">BatchNorm 导致很大的量化误差</a> 的问题
</p>
</div>
</div>

<div id="outline-container-org8caf8e7" class="outline-4">
<h4 id="org8caf8e7"><span class="section-number-4">1.5.2</span> relu-&gt;batchnorm 优点</h4>
<div class="outline-text-4" id="text-1-5-2">
<p>
从实践上看, relu-&gt;batchnorm 训练出来的模型的性能更好一点
</p>
</div>
</div>
</div>

<div id="outline-container-org69975b9" class="outline-3">
<h3 id="org69975b9"><span class="section-number-3">1.6</span> Layer Normalization</h3>
<div class="outline-text-3" id="text-1-6">
<ul class="org-ul">
<li>batchnorm 是使用 axis(-1) (即 channel) 做 normalization</li>
<li>layernorm 使用 axis(0) (即 batch) 做 normalization</li>
</ul>


<div id="orgbe4926f" class="figure">
<p><img src="../extra/norm.png" alt="norm.png" />
</p>
</div>

<p>
使用 layernorm 的原因有两个:
</p>

<ul class="org-ul">
<li>batchnorm 需要较在的 batch size 才能工作</li>
<li>rnn 里同一个 batch 里样本的大小可能不同</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-08-31 二 00:00<br />
Last updated: 2022-01-15 六 11:03</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

           <div id="disqus_thread"></div>
           <script>

           (function() { // DON'T EDIT BELOW THIS LINE
                    var d = document, s = d.createElement('script');
                    s.src = '//sunwayforever-github-io.disqus.com/embed.js';
                    s.setAttribute('data-timestamp', +new Date());
                    (d.head || d.body).appendChild(s);
                    })();
           </script>
</div>
</body>
</html>
