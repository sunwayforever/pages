<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!--  -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Batch Normalization</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Batch Normalization</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000019">1. Batch Normalization</a>
<ul>
<li><a href="#org0000001">1.1. Overview</a></li>
<li><a href="#org0000005">1.2. DNN 的 batchnorm</a></li>
<li><a href="#org0000009">1.3. CNN 的 batchnorm</a></li>
<li><a href="#org0000012">1.4. batchnorm-&gt;relu vs. relu-&gt;batchnorm</a>
<ul>
<li><a href="#org000000c">1.4.1. batchnorm-&gt;relu 的优点</a></li>
<li><a href="#org000000f">1.4.2. relu-&gt;batchnorm 优点</a></li>
</ul>
</li>
<li><a href="#ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4">1.5. Layer Normalization</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000019" class="outline-2">
<h2 id="org0000019"><span class="section-number-2">1</span> Batch Normalization</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000001" class="outline-3">
<h3 id="org0000001"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
我们调节 \(w\) 的目的是为了保证输出在一定的范围内, 例如符合标准正态分布.
</p>

<p>
batch normalization 不调整 \(w\): 它对输出作 normalization, 保证输出是标准正态分布,
batch normalization 与 feature scaling 时的 normalization 类似, `the mean and
standard-deviation are calculated per-dimension over the mini-batches`
</p>

<p>
batch normalization 的代码基本上为:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">mu</span> = np.mean(X, axis=-1, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
<span style="font-weight: bold; font-style: italic;">std</span> = np.std(X, axis=-1, keepdims=<span style="font-weight: bold; text-decoration: underline;">True</span>)
<span style="font-weight: bold; font-style: italic;">X_normed</span> = (X - mu) / (std + <span style="font-weight: bold;">self</span>.epsilon)
<span style="font-weight: bold; font-style: italic;">out</span> = <span style="font-weight: bold;">self</span>.gamma * X_normed + <span style="font-weight: bold;">self</span>.beta
</pre>
</div>

<p>
其中最后一步称为 scale and shift, self.gamma 和 self.beta 是可学习的参数, 为了调
整 normalization 的结果.
</p>

<p>
以 relu 为例, 若直接使用 X_normed, 会导致有一半的 x 被丢弃 (X_normed 有 50% 的概
率 &lt; 0). 如果通过 scale and shift 能把 X_normed 向正方向移动一下, 就没有问题了.
</p>

<div class="org-src-container">
<pre class="src src-ipython">import matplotlib.pyplot as plt
import torch

N = 100
plt.style.use("default")

# plt.ylim(0, 40)


def init_weight(in_features, out_features):
    return torch.nn.Parameter(torch.randn(in_features, out_features))


class Layer(torch.nn.Module):
    def __init__(self, in_features, out_features, n):
        super().__init__()
        self.n = n
        self.w = init_weight(in_features, out_features)
        self.bn = torch.nn.BatchNorm1d(in_features)

    def forward(self, input):
        ret = torch.matmul(input, self.w)
        ret = self.bn(ret)
        ret = torch.nn.functional.sigmoid(ret)
        plt.subplot(1, 10, self.n)
        plt.xlim(0,1)
        plt.hist(ret.detach().numpy().reshape(-1))
        return ret


def train():
    net = torch.nn.Sequential()

    for i in range(10):
        net.add_module("linear%d" % (i), Layer(N, N, i + 1))

    x = torch.rand(10, N)
    net(x)
    plt.show()


train()
</pre>
</div>


<div id="org0000000" class="figure">
<p><img src="../extra/batch_normal.png" alt="batch_normal.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000005" class="outline-3">
<h3 id="org0000005"><span class="section-number-3">1.2</span> DNN 的 batchnorm</h3>
<div class="outline-text-3" id="text-1-2">
<p>
DNN 的 batchnorm 是在 batch 的维度上进行平均, 即:
</p>

<pre class="example" id="org0000004">
batch1: a b c d e
batch2: x y z m n

batchnorm 后为:
batch1: a-u1, b-u2, c-u3...
batch2: x-u1, b-u2, c-u3...

u1 是 mean(a,x), u2 是 mean(b-y)...
</pre>

<p>
mean 的个数和 output 的 shape 相等
</p>
</div>
</div>

<div id="outline-container-org0000009" class="outline-3">
<h3 id="org0000009"><span class="section-number-3">1.3</span> CNN 的 batchnorm</h3>
<div class="outline-text-3" id="text-1-3">
<p>
把一个 channel 看做整体, CNN 的 batchnorm 与 DNN 类似, 即:
</p>

<pre class="example" id="org0000008">
batch1: channel1:a b c  channel2: a' b' c'
                 d e f            d' e' f'

batch2: channel1:x y z  channel2: x' y' z'
                 m n p            m' n' p'

batchnorm 后为:
batch1: channel1: a-u1 b-u1 c-u1  channel2: a'-u2 b'-u2...
                  ...                       ....
batch2: channel1: x-u1 y-u1 z-u1  channel2: x'-u2 y'-u2...                  
                  ...                       ...

u1 是  mean(a,b,c,d,e,f,x,y,z,m,n,p)
u2 是  mean(a',b',c',d',e',f',x',y',z',m',n',p')

</pre>

<p>
考虑到同一个 channel 的数据都是用同一个 kernel 计算出来的, mean 的个数和 kernel
的个数 (或 output channel 个数) 相同, 即以 channel 为单位计算 mean
</p>

<p>
tensorflow 的 BatchNormalization 函数接受一个参数 axis, 默认为 -1, 对于 DNN, -1
即 batch 之外那个 axis, 对于 CNN, -1 为 channel.
</p>

<p>
BatchNormalization(axis=-1) 与 sum(axis=-1) 中 axis 的意义不同: sum 的 axis 指去
掉这一维, BatchNormalization 的 axis 指保留这一维
</p>
</div>
</div>

<div id="outline-container-org0000012" class="outline-3">
<h3 id="org0000012"><span class="section-number-3">1.4</span> batchnorm-&gt;relu vs. relu-&gt;batchnorm</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org000000c" class="outline-4">
<h4 id="org000000c"><span class="section-number-4">1.4.1</span> batchnorm-&gt;relu 的优点</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
tflite 使用 batchnorm-&gt;relu 时, 可以把 batchnorm 和 relu 融合 (fuse) 到前面一层
cnn 或 dnn, 但使用 relu-&gt;batchnorm 时无法融合, 因为 batchorm 在 inference 时相当
于对 input 的线性变换, 如果它的前一层也是一个线性变换, 而两者可以很简单的合并成
一个线性变换, 但两者之前加上一个 relu 就没法操作了, 因为 relu 并不是线性的
</p>

<p>
另一方面, relu 放在 batchnorm 之前可能会出来 <a href="../tensorflow/tflite_quantization_detail.html#ID-6470583f-2076-4f0e-96e4-aef98fff8a01">BatchNorm 导致很大的量化误差</a> 的问题
</p>
</div>
</div>

<div id="outline-container-org000000f" class="outline-4">
<h4 id="org000000f"><span class="section-number-4">1.4.2</span> relu-&gt;batchnorm 优点</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
从实践上看, relu-&gt;batchnorm 训练出来的模型的性能更好一点
</p>
</div>
</div>
</div>

<div id="outline-container-ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4" class="outline-3">
<h3 id="ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4"><span class="section-number-3">1.5</span> Layer Normalization</h3>
<div class="outline-text-3" id="text-1-5">
<ul class="org-ul">
<li>batchnorm 是使用 axis(-1) (即 channel) 做 normalization</li>
<li>layernorm 使用 axis(0) (即 batch) 做 normalization</li>
</ul>


<div id="org0000015" class="figure">
<p><img src="../extra/norm.png" alt="norm.png" />
</p>
</div>

<p>
使用 layernorm 的原因有两个:
</p>

<ul class="org-ul">
<li>batchnorm 需要较在的 batch size 才能工作</li>
<li>rnn 里同一个 batch 里样本的大小可能不同</li>
</ul>
</div>


<div id="outline-container-org0000016" class="outline-4 references">
<h4 id="org0000016">Backlinks</h4>
<div class="outline-text-4" id="text-org0000016">
<p>
<a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">Attention</a>
(<i>Attention &gt; Transformer &gt; Add&amp;Norm</i>):  - Add 是 <a href="resnet.html#ID-0c626f98-0eab-4638-b296-268c65207120">ResNet</a> 中的 skip connection - Norm 是 <a href="#ID-ce62a8c7-44b1-4b12-9edf-e05e14a15ed4">Layer Normalization</a>
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org000001c" class="outline-2 references">
<h2 id="org000001c">Backlinks</h2>
<div class="outline-text-2" id="text-org000001c">
<p>
<a href="weight_initialization.html#ID-95efdffd-212c-48e1-a3b1-4908885f9846">Weight Initialization</a>
(<i>Weight Initialization &gt; Batch Normalization</i>):   <a href="batchnorm.html#ID-ffa885c6-f5a3-4939-8e61-4ebd2bbbe836">Batch Normalization</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: <br />
Last updated: </p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
