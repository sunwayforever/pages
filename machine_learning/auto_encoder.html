<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Auto Encoder</title>


           <link rel="stylesheet" type="text/css" href="/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="./htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="../htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="/readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="./readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="../readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Auto Encoder</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0000010">1. Auto Encoder</a>
<ul>
<li><a href="#org0000001">1.1. auto-encoder</a></li>
<li><a href="#org0000004">1.2. 提取简单的特征</a></li>
<li><a href="#org0000007">1.3. 复杂一点的特征</a></li>
<li><a href="#org000000a">1.4. 无法压缩的特征</a></li>
<li><a href="#org000000d">1.5. sparse auto-encoder</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000010" class="outline-2">
<h2 id="org0000010"><span class="section-number-2">1.</span> Auto Encoder</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000001" class="outline-3">
<h3 id="org0000001"><span class="section-number-3">1.1.</span> auto-encoder</h3>
<div class="outline-text-3" id="text-1-1">
<p>
auto-encoder (自编码器) 是一种无监督的机器学习方法, 目的是拟合 identity 函数, 即
\(f(x)=x\).
</p>


<div id="org0000000" class="figure">
<p><img src="../extra/auto_encoder.png" alt="auto_encoder.png" />
</p>
</div>

<p>
上面显示了一个简单的 auto-encoder:
</p>

<ul class="org-ul">
<li>输入 feature 大小为 6,</li>
<li>input layer 与 hidden layer 之间的权重可以看作是 encoder, 可以把 input 大小压缩为 3</li>
<li>hidden layer 与 output layer 之间的权重看作是 decoder, 可以把压缩的数据还原为原始数据</li>
</ul>

<p>
数据可以压缩, 是因为数据通常具有相关性, 例如: 假设输入数据 x 为
\([[1,2,3,4,5],[2,4,6,8,10],[3,6,9,12,15]\ldots]\), 其中隐含着一个规律是
\(x_i^j=x_i^0*(j+1)\)
</p>

<p>
所以 x 实际可以压缩为 \([1,2,3...]^T\).
</p>

<p>
auto-encoder 可以学习到这个隐含的规律, 从而对原数据进行压缩. 压缩的结果体现在
hidden layer, 而规律则体现在 encoder 和 decoder 的权重.
</p>

<p>
实际上, auto-encoder 这个特征也可以看作是某种`特征提取`, 和 PCA 降维有类似的效果.但
PCA 属于线性降维, 而 auto-encoder 属于非线性降维 (与 t-SNE) 类似.
</p>
</div>
</div>

<div id="outline-container-org0000004" class="outline-3">
<h3 id="org0000004"><span class="section-number-3">1.2.</span> 提取简单的特征</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt

<span class="org-keyword">import</span> torch
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> nn
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> optim
<span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader, Dataset
<span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F


<span class="org-comment-delimiter"># </span><span class="org-comment">---------- data ----------</span>
<span class="org-keyword">class</span> <span class="org-type">PlainDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.<span class="org-builtin">round</span>(torch.rand(1000) <span class="org-operator">*</span> 200)
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> x.unsqueeze(1)
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.cat((x, x <span class="org-operator">*</span> 2, x <span class="org-operator">*</span> 3, x <span class="org-operator">*</span> 4, x <span class="org-operator">*</span> 5, x <span class="org-operator">*</span> 6, x <span class="org-operator">*</span> 7, x <span class="org-operator">*</span> 8,
                       x <span class="org-operator">*</span> 9, x <span class="org-operator">*</span> 10), 1)
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> <span class="org-operator">=</span> x
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.X

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)


<span class="org-variable-name">training_set</span> <span class="org-operator">=</span> PlainDataset()
<span class="org-variable-name">training_loader</span> <span class="org-operator">=</span> DataLoader(training_set, batch_size<span class="org-operator">=</span>100, shuffle<span class="org-operator">=</span><span class="org-constant">True</span>)


<span class="org-comment-delimiter"># </span><span class="org-comment">---------- helper ----------</span>
<span class="org-keyword">def</span> <span class="org-function-name">test</span>():
    <span class="org-variable-name">m</span> <span class="org-operator">=</span> model[0]

    <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.tensor([[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]]).<span class="org-builtin">float</span>()
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> model(x)
    <span class="org-builtin">print</span>(<span class="org-string">"orig: "</span>, x, <span class="org-string">" new: "</span>, y_hat, <span class="org-string">"a:"</span>, m(x))

    <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).<span class="org-builtin">float</span>()
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> model(x)
    <span class="org-builtin">print</span>(<span class="org-string">"orig: "</span>, x, <span class="org-string">" new: "</span>, y_hat, <span class="org-string">"a:"</span>, m(x))

    <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.tensor([[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]]).<span class="org-builtin">float</span>()
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> model(x)
    <span class="org-builtin">print</span>(<span class="org-string">"orig: "</span>, x, <span class="org-string">" new: "</span>, y_hat, <span class="org-string">"a:"</span>, m(x))

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1500):
        <span class="org-keyword">for</span> x, y <span class="org-keyword">in</span> training_loader:
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> criterion(model(x), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        <span class="org-keyword">if</span> i <span class="org-operator">%</span> 200 <span class="org-operator">==</span> 0:
            <span class="org-builtin">print</span>(<span class="org-string">"epoch #%d: loss: %f"</span> <span class="org-operator">%</span> (i, loss.item()))


<span class="org-comment-delimiter"># </span><span class="org-comment">---------- model ----------</span>

<span class="org-variable-name">model</span> <span class="org-operator">=</span> nn.Sequential(nn.Linear(10, 1), nn.ReLU(), nn.Linear(1, 10))
<span class="org-variable-name">criterion</span> <span class="org-operator">=</span> nn.MSELoss()
<span class="org-variable-name">optimizer</span> <span class="org-operator">=</span> optim.Adam(model.parameters(), weight_decay<span class="org-operator">=</span>0.001)

train()
test()
</pre>
</div>

<p>
epoch #0: loss: 497518.718750
epoch #200: loss: 0.224333
epoch #400: loss: 0.175876
epoch #600: loss: 0.167690
epoch #800: loss: 0.147397
epoch #1000: loss: 0.062419
epoch #1200: loss: 0.034183
epoch #1400: loss: 0.084953
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
</p>
</div>
</div>

<div id="outline-container-org0000007" class="outline-3">
<h3 id="org0000007"><span class="section-number-3">1.3.</span> 复杂一点的特征</h3>
<div class="outline-text-3" id="text-1-3">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">PlainDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.<span class="org-builtin">round</span>(torch.rand(1000) <span class="org-operator">*</span> 200)
        <span class="org-variable-name">z</span> <span class="org-operator">=</span> torch.<span class="org-builtin">round</span>(torch.rand(1000) <span class="org-operator">*</span> 200)
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> x.unsqueeze(1)
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.cat((x, x <span class="org-operator">*</span> 2, x <span class="org-operator">*</span> 3, x <span class="org-operator">*</span> 4, x <span class="org-operator">*</span> 5, x <span class="org-operator">*</span> 6, x <span class="org-operator">*</span> 7, x <span class="org-operator">*</span> 8,
                       x <span class="org-operator">*</span> 9, x <span class="org-operator">*</span> 10), 1)
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> <span class="org-operator">=</span> x <span class="org-operator">+</span> z.unsqueeze(1)
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.X

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)
</pre>
</div>

<p>
x 的规律变为 \(x_i^j=x_i^0*(j+1)+k_i\), 其中 \(k_i\) 是一个随机数.
</p>

<p>
如果 hidden layer 大小为 1, 则无论怎么训练都无法收敛, 因为 x 不可能压缩为一个数,
直观上感觉至少需要两个数: \(x_i^0\) 和 \(k_i\). 实现测试时, 设置 hidden layer 大小为
3 时可以收敛
</p>
</div>
</div>

<div id="outline-container-org000000a" class="outline-3">
<h3 id="org000000a"><span class="section-number-3">1.4.</span> 无法压缩的特征</h3>
<div class="outline-text-3" id="text-1-4">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">class</span> <span class="org-type">PlainDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> <span class="org-operator">=</span> torch.randn(1000, 10)
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.X

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)
</pre>
</div>

<p>
若 x 为随机数, 则 hidden layer 的大小小于 feature 大小时都无法收敛, 因为不同的
feature 之间完全没有相关性, 无法压缩.
</p>
</div>
</div>

<div id="outline-container-org000000d" class="outline-3">
<h3 id="org000000d"><span class="section-number-3">1.5.</span> sparse auto-encoder<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup></h3>
<div class="outline-text-3" id="text-1-5">
<p>
autoencoder hidden layer 神经元的个数并不一定需要小于 feature 的大小, 例如手写数字识别的例子中, 针对 10*10 大小的图片, 我们期望 autoencoder 能提取出更多的特征
(超过 100 个), 以检测各种不同的边缘.
</p>

<p>
若直接使用 autoencoder, 由于 hidden layer 大小 &gt;= feature 大小, 所以 autoencoder
有极大的自由度来选择权重: 无论 hidden layer 为多少, 通过调用权重总是可以收敛. 通过实验可以发现, 每次测试时都会收敛, 但每次收敛时的权重都不同.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> torch
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> nn
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> optim
<span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader, Dataset

<span class="org-comment-delimiter"># </span><span class="org-comment">---------- data ----------</span>
<span class="org-keyword">class</span> <span class="org-type">PlainDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.randn(1000, 10)
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> <span class="org-operator">=</span> x
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> <span class="org-operator">=</span> <span class="org-keyword">self</span>.X

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)


<span class="org-variable-name">training_set</span> <span class="org-operator">=</span> PlainDataset()
<span class="org-variable-name">training_loader</span> <span class="org-operator">=</span> DataLoader(training_set, batch_size<span class="org-operator">=</span>100, shuffle<span class="org-operator">=</span><span class="org-constant">True</span>)


<span class="org-comment-delimiter"># </span><span class="org-comment">---------- helper ----------</span>
<span class="org-keyword">def</span> <span class="org-function-name">test</span>():
    <span class="org-variable-name">m</span> <span class="org-operator">=</span> model[0]
    torch.manual_seed(1000)
    <span class="org-variable-name">x</span> <span class="org-operator">=</span> torch.randn(1, 10)
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> model(x)
    <span class="org-builtin">print</span>(<span class="org-string">"orig: "</span>, x, <span class="org-string">" new: "</span>, y_hat, <span class="org-string">"a:"</span>, m(x))


<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(300):
        <span class="org-keyword">for</span> x, y <span class="org-keyword">in</span> training_loader:
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> criterion(model(x), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        <span class="org-comment-delimiter"># </span><span class="org-comment">if i % 20 == 0:</span>
        <span class="org-comment-delimiter">#     </span><span class="org-comment">print("epoch #%d: loss: %f" % (i, loss.item()))</span>


<span class="org-comment-delimiter"># </span><span class="org-comment">---------- model ----------</span>
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(2):
    <span class="org-variable-name">model</span> <span class="org-operator">=</span> nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))
    <span class="org-variable-name">criterion</span> <span class="org-operator">=</span> nn.MSELoss()
    <span class="org-variable-name">optimizer</span> <span class="org-operator">=</span> optim.Adam(model.parameters())
    train()
    test()
</pre>
</div>

<p>
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
</p>

<p>
所以这种情况下 autoencoder 无法提取到有用的特征.
</p>

<p>
sparse autoencoder 的做法是在 autoencoder 的基础上, 给损失函数加上了一个sparsity
penalty, 这个 penalty 会限制不能有太多的非零的权重.
</p>

<p>
具体实现的时候, 可以在训练时只保留最大的 K 个权重, 将剩余的权重清零 (类似于
dropout), 或者根据计算 hidden layer 中 activation 的值, 通过 KL divergence (KL
divergence 与 cross entropy 类似, 都是用来度量两个概率分布的相似性) penalty 限制每个 activation 的针对所有样本的均值不能过大
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf">cs294a: sparse autoencoder</a> <a href="http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity">UFLDL: Autoencoders and Sparsity</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2018-08-14 Tue 00:00<br />
Last updated: 2021-11-01 Mon 00:51</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>