<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-26 Wed 00:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Auto Encoder</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Auto Encoder</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org757906d">1. Auto Encoder</a>
<ul>
<li><a href="#org6fd8016">1.1. auto-encoder</a></li>
<li><a href="#orgbe095fc">1.2. 提取简单的特征</a></li>
<li><a href="#org1b62120">1.3. 复杂一点的特征</a></li>
<li><a href="#org56fbed0">1.4. 无法压缩的特征</a></li>
<li><a href="#orgb2d1bf8">1.5. sparse auto-encoder</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org757906d" class="outline-2">
<h2 id="org757906d"><span class="section-number-2">1</span> Auto Encoder</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org6fd8016" class="outline-3">
<h3 id="org6fd8016"><span class="section-number-3">1.1</span> auto-encoder</h3>
<div class="outline-text-3" id="text-1-1">
<p>
auto-encoder (自编码器) 是一种无监督的机器学习方法, 目的是拟合 identity 函数, 即
\(f(x)=x\).
</p>


<div id="orgd44ca22" class="figure">
<p><img src="../extra/auto_encoder.png" alt="auto_encoder.png" />
</p>
</div>

<p>
上面显示了一个简单的 auto-encoder:
</p>

<ul class="org-ul">
<li>输入 feature 大小为 6,</li>
<li>input layer 与 hidden layer 之间的权重可以看作是 encoder, 可以把 input 大小压缩为 3</li>
<li>hidden layer 与 output layer 之间的权重看作是 decoder, 可以把压缩的数据还原为
原始数据</li>
</ul>

<p>
数据可以压缩, 是因为数据通常具有相关性, 例如: 假设输入数据 x 为
\([[1,2,3,4,5],[2,4,6,8,10],[3,6,9,12,15]\ldots]\), 其中隐含着一个规律是
\(x_i^j=x_i^0*(j+1)\)
</p>

<p>
所以 x 实际可以压缩为 \([1,2,3...]^T\).
</p>

<p>
auto-encoder 可以学习到这个隐含的规律, 从而对原数据进行压缩. 压缩的结果体现在
hidden layer, 而规律则体现在 encoder 和 decoder 的权重.
</p>

<p>
实际上, auto-encoder 这个特征也可以看作是某种`特征提取`, 和 PCA 降维有类似的效果.但
PCA 属于线性降维, 而 auto-encoder 属于非线性降维 (与 t-SNE) 类似.
</p>
</div>
</div>

<div id="outline-container-orgbe095fc" class="outline-3">
<h3 id="orgbe095fc"><span class="section-number-3">1.2</span> 提取简单的特征</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
import matplotlib.pyplot as plt

import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, Dataset
import torch.nn.functional as F


# ---------- data ----------
class PlainDataset(Dataset):
    def __init__(self):
        x = torch.round(torch.rand(1000) * 200)
        x = x.unsqueeze(1)
        x = torch.cat((x, x * 2, x * 3, x * 4, x * 5, x * 6, x * 7, x * 8,
                       x * 9, x * 10), 1)
        self.X = x
        self.Y = self.X

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)


training_set = PlainDataset()
training_loader = DataLoader(training_set, batch_size=100, shuffle=True)


# ---------- helper ----------
def test():
    m = model[0]

    x = torch.tensor([[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]]).float()
    y_hat = model(x)
    print("orig: ", x, " new: ", y_hat, "a:", m(x))

    x = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).float()
    y_hat = model(x)
    print("orig: ", x, " new: ", y_hat, "a:", m(x))

    x = torch.tensor([[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]]).float()
    y_hat = model(x)
    print("orig: ", x, " new: ", y_hat, "a:", m(x))

def train():
    for i in range(1500):
        for x, y in training_loader:
            loss = criterion(model(x), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        if i % 200 == 0:
            print("epoch #%d: loss: %f" % (i, loss.item()))


# ---------- model ----------

model = nn.Sequential(nn.Linear(10, 1), nn.ReLU(), nn.Linear(1, 10))
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), weight_decay=0.001)

train()
test()
</pre>
</div>

<p>
epoch #0: loss: 497518.718750
epoch #200: loss: 0.224333
epoch #400: loss: 0.175876
epoch #600: loss: 0.167690
epoch #800: loss: 0.147397
epoch #1000: loss: 0.062419
epoch #1200: loss: 0.034183
epoch #1400: loss: 0.084953
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
</p>
</div>
</div>

<div id="outline-container-org1b62120" class="outline-3">
<h3 id="org1b62120"><span class="section-number-3">1.3</span> 复杂一点的特征</h3>
<div class="outline-text-3" id="text-1-3">
<div class="org-src-container">
<pre class="src src-ipython">class PlainDataset(Dataset):
    def __init__(self):
        x = torch.round(torch.rand(1000) * 200)
        z = torch.round(torch.rand(1000) * 200)
        x = x.unsqueeze(1)
        x = torch.cat((x, x * 2, x * 3, x * 4, x * 5, x * 6, x * 7, x * 8,
                       x * 9, x * 10), 1)
        self.X = x + z.unsqueeze(1)
        self.Y = self.X

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)
</pre>
</div>

<p>
x 的规律变为 \(x_i^j=x_i^0*(j+1)+k_i\), 其中 \(k_i\) 是一个随机数.
</p>

<p>
如果 hidden layer 大小为 1, 则无论怎么训练都无法收敛, 因为 x 不可能压缩为一个数,
直观上感觉至少需要两个数: \(x_i^0\) 和 \(k_i\). 实现测试时, 设置 hidden layer 大小为
3 时可以收敛
</p>
</div>
</div>

<div id="outline-container-org56fbed0" class="outline-3">
<h3 id="org56fbed0"><span class="section-number-3">1.4</span> 无法压缩的特征</h3>
<div class="outline-text-3" id="text-1-4">
<div class="org-src-container">
<pre class="src src-ipython">class PlainDataset(Dataset):
    def __init__(self):
        self.X = torch.randn(1000, 10)
        self.Y = self.X

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)
</pre>
</div>

<p>
若 x 为随机数, 则 hidden layer 的大小小于 feature 大小时都无法收敛, 因为不同的
feature 之间完全没有相关性, 无法压缩.
</p>
</div>
</div>

<div id="outline-container-orgb2d1bf8" class="outline-3">
<h3 id="orgb2d1bf8"><span class="section-number-3">1.5</span> sparse auto-encoder<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup></h3>
<div class="outline-text-3" id="text-1-5">
<p>
autoencoder hidden layer 神经元的个数并不一定需要小于 feature 的大小, 例如手写数
字识别的例子中, 针对 10*10 大小的图片, 我们期望 autoencoder 能提取出更多的特征
(超过 100 个), 以检测各种不同的边缘.
</p>

<p>
若直接使用 autoencoder, 由于 hidden layer 大小 &gt;= feature 大小, 所以 autoencoder
有极大的自由度来选择权重: 无论 hidden layer 为多少, 通过调用权重总是可以收敛. 通
过实验可以发现, 每次测试时都会收敛, 但每次收敛时的权重都不同.
</p>

<div class="org-src-container">
<pre class="src src-ipython">import torch
from torch import nn
from torch import optim
from torch.utils.data import DataLoader, Dataset

# ---------- data ----------
class PlainDataset(Dataset):
    def __init__(self):
        x = torch.randn(1000, 10)
        self.X = x
        self.Y = self.X

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)


training_set = PlainDataset()
training_loader = DataLoader(training_set, batch_size=100, shuffle=True)


# ---------- helper ----------
def test():
    m = model[0]
    torch.manual_seed(1000)
    x = torch.randn(1, 10)
    y_hat = model(x)
    print("orig: ", x, " new: ", y_hat, "a:", m(x))


def train():
    for i in range(300):
        for x, y in training_loader:
            loss = criterion(model(x), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        # if i % 20 == 0:
        #     print("epoch #%d: loss: %f" % (i, loss.item()))


# ---------- model ----------
for i in range(2):
    model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 10))
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters())
    train()
    test()
</pre>
</div>

<p>
orig:  tensor()  new:  tensor() a: tensor()
orig:  tensor()  new:  tensor() a: tensor()
</p>

<p>
所以这种情况下 autoencoder 无法提取到有用的特征.
</p>

<p>
sparse autoencoder 的做法是在 autoencoder 的基础上, 给损失函数加上了一个sparsity
penalty, 这个 penalty 会限制不能有太多的非零的权重.
</p>

<p>
具体实现的时候, 可以在训练时只保留最大的 K 个权重, 将剩余的权重清零 (类似于
dropout), 或者根据计算 hidden layer 中 activation 的值, 通过 KL divergence (KL
divergence 与 cross entropy 类似, 都是用来度量两个概率分布的相似性) penalty 限制
每个 activation 的针对所有样本的均值不能过大
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf">cs294a: sparse autoencoder</a> <a href="http://ufldl.stanford.edu/wiki/index.php/Autoencoders_and_Sparsity">UFLDL: Autoencoders and Sparsity</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2018-08-14 Tue 00:00<br />
Last updated: 2021-10-31 Sun 23:40</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
