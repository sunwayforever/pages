<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Point Pillars</title>


           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Point Pillars</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org000004c">1. Point Pillars</a>
<ul>
<li><a href="#org0000008">1.1. kitti dataset</a>
<ul>
<li><a href="#org0000000">1.1.1. velodyne</a></li>
<li><a href="#org0000005">1.1.2. label_2</a></li>
</ul>
</li>
<li><a href="#ID-04bc7df2-c5e4-4fa4-acb6-5cd32a0d7d1e">1.2. pillar</a></li>
<li><a href="#org0000017">1.3. label</a>
<ul>
<li><a href="#org000000e">1.3.1. pillars</a></li>
<li><a href="#org0000011">1.3.2. indices</a></li>
<li><a href="#org0000014">1.3.3. rpn ground truth</a></li>
</ul>
</li>
<li><a href="#org000003a">1.4. network</a>
<ul>
<li><a href="#org0000028">1.4.1. pfe</a></li>
<li><a href="#org000002c">1.4.2. scatter</a></li>
<li><a href="#ID-45a9c02c-77b0-48be-b21a-c60acb5e63e2">1.4.3. rpn</a></li>
</ul>
</li>
<li><a href="#org000003d">1.5. inference</a></li>
<li><a href="#org0000049">1.6. loss</a>
<ul>
<li><a href="#org0000046">1.6.1. focal loss</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org000004c" class="outline-2">
<h2 id="org000004c"><span class="section-number-2">1.</span> Point Pillars</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://arxiv.org/pdf/1812.05784.pdf">PointPillars: Fast Encoders for Object Detection from Point Clouds</a> 2019/5
</p>

<p>
<a href="https://github.com/nutonomy/second.pytorch">https://github.com/nutonomy/second.pytorch</a>
</p>

<p>
<a href="https://github.com/tyagi-iiitv/PointPillars">https://github.com/tyagi-iiitv/PointPillars</a>
</p>
</div>

<div id="outline-container-org0000008" class="outline-3">
<h3 id="org0000008"><span class="section-number-3">1.1.</span> kitti dataset</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<a href="http://www.cvlibs.net/datasets/kitti/">http://www.cvlibs.net/datasets/kitti/</a>
</p>

<p>
<a href="https://github.com/kuixu/kitti_object_vis">https://github.com/kuixu/kitti_object_vis</a>
</p>

<p>
kitti 包含了立体图像 (stereo), 光流 (flow), 视觉测距 (odometry) 和 3d 物体检测
(3d object) 等多种数据. 其中 3d object 主要包含了:
</p>

<ol class="org-ol">
<li>velodyne: velodyne 点云数据</li>
<li>image_2: 对应的图片</li>
<li>calib_2: lidar, camera 和 imu 的 calib 数据</li>
<li>label_2: 标签</li>
</ol>
</div>


<div id="outline-container-org0000000" class="outline-4">
<h4 id="org0000000"><span class="section-number-4">1.1.1.</span> velodyne</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
velody 数据放在 velodyne/xxx.bin 中, 由 N*4 个浮点数组成, 表示 N 个点的(x, y, z,
r), 其中 r 指反射强度 (reflectance/intensity)
</p>
</div>
</div>

<div id="outline-container-org0000005" class="outline-4">
<h4 id="org0000005"><span class="section-number-4">1.1.2.</span> label_2</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
label_2/000000.txt:
</p>

<pre class="example" id="org0000003">
#    class   truncated   遮挡   alpha                 2d bbox(像素)       长宽高(米)      物体的位置   rotation_y
Pedestrian        0.00      0   -0.20   712.40 143.00 810.73 307.92   1.89 0.48 1.20  1.84 1.47 8.41         0.01
</pre>

<p>
其中 alpha 和 rotation_y 如下图所示:
</p>


<div id="org0000004" class="figure">
<p><img src="../extra/kitti.png" alt="kitti.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-ID-04bc7df2-c5e4-4fa4-acb6-5cd32a0d7d1e" class="outline-3">
<h3 id="ID-04bc7df2-c5e4-4fa4-acb6-5cd32a0d7d1e"><span class="section-number-3">1.2.</span> pillar</h3>
<div class="outline-text-3" id="text-1-2">
<p>
在 PointPillars 之前, 存在多种 3d object detection 网络, 主要分为两类:
</p>

<ol class="org-ol">
<li>直接使用 conv3d, 但这种方法的计算量很大</li>

<li>把 point cloud 映射到 2d 空间 (例如通过 bev (bird eye view, 俯视图) 投影到
x-y 平面), 然后进行人工的特征提取 (例如统计 z 方向上 z 或 r 的最大值), 最后用
conv2d 当做图片来处理. 例如: <a href="https://arxiv.org/pdf/1611.07759.pdf">Multi-View 3D Object Detection Network</a>, apollo
中的 cnnseg 用的就是和 mv3d 类似的思想</li>
</ol>

<p>
为了避免手工特征提取, 有两种新的网络:
</p>

<ol class="org-ol">
<li>PointNet (<a href="https://arxiv.org/pdf/1612.00593.pdf">https://arxiv.org/pdf/1612.00593.pdf</a> 2017/4)</li>

<li>VoxelNet (<a href="https://arxiv.org/pdf/1711.06396.pdf">https://arxiv.org/pdf/1711.06396.pdf</a> 2017/11)</li>
</ol>

<p>
其中 PointNet 的思想是:
</p>

<p>
假设投影到 x-y 平面后 z 方向上有 N 个 feature (N 通常较小), 直接用 max 函数会导致较多的信息丢失, 那不如先对这 N 个 feature 做一个变换 (例如通过一个 mlp), 变成
M 个 feature (M 会很大), 再做 max 则可以损失较少的信息
</p>

<p>
PointPillars 和 VoxelNet 都参考了 PointNet 的思想, 不同的是:
</p>

<ul class="org-ul">
<li>VoxelNet 使用 <a href="https://en.wikipedia.org/wiki/Voxel">voxel</a> 来聚合点云, 它的 feature extractor 得到 3d 的 feature: (H,
W, D, C), 后续需要用 conv3d 来处理</li>

<li>PointPillars 使用 pillar 来聚合点云, 它的 feature extractor 得到 2d 的
feature: (H, W, C), 后续用 conv2d 来处理</li>
</ul>

<p>
PointPillars 会先把输入的点云数据转换为 (P, N, D), 其中:
</p>

<ol class="org-ol">
<li>P 表示把 x-y 平面分成多少个 pillar (柱子)</li>

<li><p>
N 表示每个 pillar 有多少个点
</p>

<p>
如果某个 pillar 实际的点数超过 N, 则随机 sample N 个, 如果不足 N 个, 则填充
0, 目的是变成定长, 以便后续处理
</p></li>

<li><p>
D 表示每个点的 feature 个数
</p>

<p>
这里的 feature 是原始的 feature, 例如 z, r 等, 不需要做人工的特征提取
</p></li>
</ol>

<p>
在后续的 pfe (pillar feature extractor) 中:
</p>

<ol class="org-ol">
<li>(P, N, D) 先通过一个 conv2d 变成 (P ,N, C) (D 相当于 PointNet 中那个较小的 N,
C 相当于那个较大的 M),</li>

<li>再针对 N 这一维进行 max 操作, 得到 (P, C)</li>

<li>最后通过一个 scatter 操作把 P 还原成二维的 (H, W), 得到 (H, W, C), 做为伪图像交给后续的 2d object detector 去处理</li>
</ol>
</div>


<div id="outline-container-org000000b" class="outline-4 references">
<h4 id="org000000b">Backlinks</h4>
<div class="outline-text-4" id="text-org000000b">
<p>
<a href="mobilenet.html#ID-95b517db-933c-4029-ad48-c11ae0dd0719">MobileNetV2</a>
(<i>MobileNet &gt; MobileNetV2 &gt; Linear Bottleneck</i>):  这个做法和 <a href="#ID-04bc7df2-c5e4-4fa4-acb6-5cd32a0d7d1e">PointNet</a> 处理 pooling 的作法类似?
</p>
</div>
</div>
</div>


<div id="outline-container-org0000017" class="outline-3">
<h3 id="org0000017"><span class="section-number-3">1.3.</span> label</h3>
<div class="outline-text-3" id="text-1-3">
<p>
label 分为三部分:
</p>

<ol class="org-ol">
<li>pillars</li>

<li>indices</li>

<li>rpn groud truth</li>
</ol>
</div>

<div id="outline-container-org000000e" class="outline-4">
<h4 id="org000000e"><span class="section-number-4">1.3.1.</span> pillars</h4>
<div class="outline-text-4" id="text-1-3-1">
<p>
pillars 是根据原始点云数据生成的 pillars, shape 为 (P, N, D)
</p>

<p>
生成 pillars 的过程大致是:
</p>

<ol class="org-ol">
<li>先确定要处理的点云的范围, 例如, x 属于 (0, 80m), y 属于 (-40m, 40m), &#x2026;, 过滤掉不需要关注的点</li>

<li>点云量化为最终需要生成的伪图像的尺寸, 例如 (504*504)</li>

<li>统计 504*504 的 grid 上每个像素对应的 voxel</li>

<li>从 504*504 个点中随机选择 P 个做为 pillar (例如 12000) 个, 同时记录下 P 和伪图像上 (H,W) 的对应关系, 做为后面需要用到的 indices</li>

<li>针对每个 pillar 随机选择 N 个 voxel</li>
</ol>
</div>
</div>

<div id="outline-container-org0000011" class="outline-4">
<h4 id="org0000011"><span class="section-number-4">1.3.2.</span> indices</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
indices 是生成 pillars 同时生成的 indice, shape 为 (P, 2), 表示 scatter_nd 时 P
与 (H, W) 的对应关系
</p>
</div>
</div>

<div id="outline-container-org0000014" class="outline-4">
<h4 id="org0000014"><span class="section-number-4">1.3.3.</span> rpn ground truth</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
与 ssd 的 assign label 过程类似: 需要生成 anchor box (例如 252*252*4 个) , 每个
anchor box 通过与 gt_box 的 iou 对比决定是否需要把 gt_box 的真值赋予 anchor box:
occupancy, position, size, angle, heading
</p>
</div>
</div>
</div>

<div id="outline-container-org000003a" class="outline-3">
<h3 id="org000003a"><span class="section-number-3">1.4.</span> network</h3>
<div class="outline-text-3" id="text-1-4">

<div id="org000001a" class="figure">
<p><img src="../extra/point_pillars.png" alt="point_pillars.png" />
</p>
</div>

<p>
PointPillars 分成三部分:
</p>

<ol class="org-ol">
<li>pillar feature extractor (pfe)</li>

<li>scatter</li>

<li>rpn

<ol class="org-ol">
<li>detection backbone</li>

<li>detection head</li>
</ol></li>
</ol>
</div>

<div id="outline-container-org0000028" class="outline-4">
<h4 id="org0000028"><span class="section-number-4">1.4.1.</span> pfe</h4>
<div class="outline-text-4" id="text-1-4-1">
<pre class="example" id="org000001b">
pillars/input (InputLayer)      [(1, 12000, 100, 7)] 0                                            
__________________________________________________________________________________________________
pillars/conv2d (Conv2D)         (1, 12000, 100, 64)  448         pillars/input[0][0]              
__________________________________________________________________________________________________
pillars/batchnorm (BatchNormali (1, 12000, 100, 64)  256         pillars/conv2d[0][0]             
__________________________________________________________________________________________________
pillars/relu (Activation)       (1, 12000, 100, 64)  0           pillars/batchnorm[0][0]          
__________________________________________________________________________________________________
pillars/maxpooling2d (MaxPoolin (1, 12000, 1, 64)    0           pillars/relu[0][0]               
__________________________________________________________________________________________________
pillars/reshape (Reshape)       (1, 12000, 64)       0           pillars/maxpooling2d[0][0]       
</pre>
</div>

<div id="outline-container-org000001c" class="outline-5">
<h5 id="org000001c"><span class="section-number-5">1.4.1.1.</span> input</h5>
<div class="outline-text-5" id="text-1-4-1-1">
<p>
input 是 (12000, 100, 7), 表示:
</p>

<ol class="org-ol">
<li>12000 个 pillar, 相当于论文中的 P</li>

<li>每个 pillar 有 100 个 voxel, 相当于论文中的 N</li>

<li>每个 voxel 有 7 个 feature (x,y,z,reflect&#x2026;), 相当于论文中的 D</li>
</ol>
</div>
</div>

<div id="outline-container-org000001f" class="outline-5">
<h5 id="org000001f"><span class="section-number-5">1.4.1.2.</span> conv2d</h5>
<div class="outline-text-5" id="text-1-4-1-2">
<p>
把 7 变成 64, 相当于论文中的 D 变成 C
</p>
</div>
</div>

<div id="outline-container-org0000022" class="outline-5">
<h5 id="org0000022"><span class="section-number-5">1.4.1.3.</span> maxpooling2d</h5>
<div class="outline-text-5" id="text-1-4-1-3">
<p>
这里的 maxpooling2d 实际上相当于 max(axis=2), 去掉了 N 这一维
</p>
</div>
</div>

<div id="outline-container-org0000025" class="outline-5">
<h5 id="org0000025"><span class="section-number-5">1.4.1.4.</span> reshape</h5>
<div class="outline-text-5" id="text-1-4-1-4">
<p>
reshape 后数据变成 (12000, 64), 即 (P, C), 相当于论文中的 Learned Features
</p>
</div>
</div>
</div>


<div id="outline-container-org000002c" class="outline-4">
<h4 id="org000002c"><span class="section-number-4">1.4.2.</span> scatter</h4>
<div class="outline-text-4" id="text-1-4-2">
<pre class="example" id="org000002b">
pillars/scatter_nd (Lambda)     (1, 504, 504, 64)    0           
</pre>

<p>
scatter 是为了把 (P, C) 变成 (H, W, C), 它的输出对应论文中的 Pseudo Image
</p>

<p>
scatter/gather 的概念在各个不同领域都有应用:
</p>

<ol class="org-ol">
<li>gather 是指从不同的位置获得数据</li>

<li>scatter 是指把数据分散到不同的位置</li>
</ol>

<p>
例如:
</p>

<ol class="org-ol">
<li><p>
linux 的 scather/gather IO
</p>

<p>
writev 相当于 gather
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">struct</span> <span class="org-type">iovec</span> <span class="org-variable-name">iov</span>[2];

<span class="org-type">iov</span>[0].iov_base = x;
<span class="org-type">iov</span>[0].iov_len = lx;
<span class="org-type">iov</span>[1].iov_base = y;
<span class="org-type">iov</span>[1].iov_len = ly;

<span class="org-function-name">writev</span>(fd, <span class="org-type">iov</span>, 2);
</pre>
</div>

<p>
readv 相当于 scatter
</p>

<div class="org-src-container">
<pre class="src src-c"><span class="org-keyword">struct</span> <span class="org-type">iovec</span> <span class="org-variable-name">iov</span>[2];

<span class="org-type">iov</span>[0].iov_base = x;
<span class="org-type">iov</span>[0].iov_len = lx;
<span class="org-type">iov</span>[1].iov_base = y;
<span class="org-type">iov</span>[1].iov_len = ly;

<span class="org-function-name">readv</span>(fd, <span class="org-type">iov</span>, 2);
</pre>
</div></li>

<li><p>
dma
</p>

<p>
scatter/gather dma 与 linux scather/gather IO 原理上是一样的, 都是为了解决
source/dest 不连续时如何高效的复制的问题
</p></li>

<li><p>
numpy
</p>

<p>
b=a[(x,y)] 相当于 gather
</p>

<p>
a[(x,y)]=b 相当于 scatter
</p></li>
</ol>
</div>
</div>


<div id="outline-container-ID-45a9c02c-77b0-48be-b21a-c60acb5e63e2" class="outline-4">
<h4 id="ID-45a9c02c-77b0-48be-b21a-c60acb5e63e2"><span class="section-number-4">1.4.3.</span> rpn</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
rpn 用来在 pseudo image 上检测物体, 具体对可以分为两部分:
</p>

<ol class="org-ol">
<li>backbone</li>

<li>detection head</li>
</ol>

<p>
标准 <a href="ssd.html#ID-5b8ef506-3d82-4b1e-a869-84555f911555">ssd</a> 可以看做 backbone 和 detection head 是放在一起的, 每个 level 的 feature
直接 encode 了 (class, coord).
</p>

<p>
这里的 rpn 使用了 backbone + detection head 的结构:
</p>

<ol class="org-ol">
<li>backbone 是一个 <a href="yolo.html#ID-75596db9-449d-4004-9f46-971270c48159">Feature Pyramid Networks</a></li>

<li>所有 level 的 feature concat 在一起后再通过多个单独的 conv 生成不同的数据
(class, coord, &#x2026;), 每个数据对应一个 detection head, 这里的 concat 相当于
`neck`.</li>
</ol>

<p>
通过 backbone + neck + head 的形式, 可以更加灵活的添加新的 detection head
</p>
</div>

<div id="outline-container-org0000030" class="outline-5">
<h5 id="org0000030"><span class="section-number-5">1.4.3.1.</span> backbone</h5>
<div class="outline-text-5" id="text-1-4-3-1">
<pre class="example" id="org000002f">
cnn/block1/conv2d0 (Conv2D)     (1, 252, 252, 64)    36928       pillars/scatter_nd[0][0]         
__________________________________________________________________________________________________
cnn/block1/conv2d1 (Conv2D)     (1, 252, 252, 64)    36928       cnn/block1/bn0[0][0]             
...

__________________________________________________________________________________________________
cnn/block2/conv2d0 (Conv2D)     (1, 126, 126, 128)   73856       cnn/block1/bn3[0][0]             
__________________________________________________________________________________________________
cnn/block2/conv2d1 (Conv2D)     (1, 126, 126, 128)   147584      cnn/block2/bn0[0][0]             
...

__________________________________________________________________________________________________
cnn/block3/conv2d0 (Conv2D)     (1, 63, 63, 128)     147584      cnn/block2/bn5[0][0]             
__________________________________________________________________________________________________
cnn/block3/conv2d1 (Conv2D)     (1, 63, 63, 128)     147584      cnn/block3/bn0[0][0]             
...

__________________________________________________________________________________________________
cnn/up1/conv2dt (Conv2DTranspos (1, 252, 252, 128)   73856       cnn/block1/bn3[0][0]             
__________________________________________________________________________________________________
cnn/up2/conv2dt (Conv2DTranspos (1, 252, 252, 128)   147584      cnn/block2/bn5[0][0]             
__________________________________________________________________________________________________
cnn/up3/conv2dt (Conv2DTranspos (1, 252, 252, 128)   147584      cnn/block3/bn5[0][0]             
__________________________________________________________________________________________________
cnn/concatenate (Concatenate)   (1, 252, 252, 384)   0           cnn/up1/bn0[0][0]
                                                                 cnn/up2/bn0[0][0]
                                                                 cnn/up3/bn0[0][0]

</pre>
</div>
</div>

<div id="outline-container-org0000034" class="outline-5">
<h5 id="org0000034"><span class="section-number-5">1.4.3.2.</span> detection head</h5>
<div class="outline-text-5" id="text-1-4-3-2">
<pre class="example" id="org0000033">
loc/conv2d (Conv2D)             (1, 252, 252, 12)    4620        cnn/concatenate[0][0]            
__________________________________________________________________________________________________
size/conv2d (Conv2D)            (1, 252, 252, 12)    4620        cnn/concatenate[0][0]            
__________________________________________________________________________________________________
clf/conv2d (Conv2D)             (1, 252, 252, 16)    6160        cnn/concatenate[0][0]            
__________________________________________________________________________________________________
occupancy/conv2d (Conv2D)       (1, 252, 252, 4)     1540        cnn/concatenate[0][0]            
__________________________________________________________________________________________________
angle/conv2d (Conv2D)           (1, 252, 252, 4)     1540        cnn/concatenate[0][0]            
__________________________________________________________________________________________________
heading/conv2d (Conv2D)         (1, 252, 252, 4)     1540        cnn/concatenate[0][0]            
__________________________________________________________________________________________________

loc/reshape (Reshape)           (1, 252, 252, 4, 3)  0           loc/conv2d[0][0]                 
__________________________________________________________________________________________________
size/reshape (Reshape)          (1, 252, 252, 4, 3)  0           size/conv2d[0][0]                
__________________________________________________________________________________________________
clf/reshape (Reshape)           (1, 252, 252, 4, 4)  0           clf/conv2d[0][0]                 
</pre>

<p>
共有 6 个 detection head:
</p>

<ol class="org-ol">
<li><p>
loc
</p>

<p>
(252, 252, 4, 3) 表示 (252, 252, 4) 个 anchor box, 每个 box (3) 个中心点坐标
(x, y, z)
</p></li>

<li><p>
size
</p>

<p>
(252, 252, 4, 3) 表示 (252, 252, 4) 个 anchor box, 每个 box 的 (lenght,
width, height)
</p></li>

<li><p>
clf
</p>

<p>
(252, 252, 4, 4) 表示 (252, 252, 4) 个 anchor box, 每个 box 的 class
confidence (一个 4 个 class: car, pedestrian, cyclist, misc)
</p></li>

<li><p>
occupancy
</p>

<p>
(252, 252, 4) 表示 (252, 252, 4) 个 anchor box 的 objectness confidence
</p></li>

<li><p>
angle
</p>

<p>
(252, 252, 4) 表示 (252, 252, 4) 个 anchor box 的 yaw angle
</p></li>

<li><p>
heading
</p>

<p>
(252, 252, 4) 表示 (252, 252, 4) 个 anchor box 的 heading: 物体是面向你还是背向你?
</p></li>
</ol>
</div>
</div>


<div id="outline-container-org0000037" class="outline-5 references">
<h5 id="org0000037">Backlinks</h5>
<div class="outline-text-5" id="text-org0000037">
<p>
<a href="ssd.html#ID-5b8ef506-3d82-4b1e-a869-84555f911555">SSD</a>
(<i>SSD &gt; network</i>):  标准的 ssd 的网络没有区别 backbone 和 detection head, 和 <a href="#ID-45a9c02c-77b0-48be-b21a-c60acb5e63e2">rpn</a> 相比显得不太灵活
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org000003d" class="outline-3">
<h3 id="org000003d"><span class="section-number-3">1.5.</span> inference</h3>
<div class="outline-text-3" id="text-1-5">
<p>
inference 时需要先转换为 pillar, inference 结束后需要 <a href="yolo.html#ID-02cce045-e3eb-4143-aa71-d86ce44347cb">NMS</a>, 两个都比较耗时.
</p>
</div>
</div>

<div id="outline-container-org0000049" class="outline-3">
<h3 id="org0000049"><span class="section-number-3">1.6.</span> loss</h3>
<div class="outline-text-3" id="text-1-6">
<p>
与 ssd 类似, PointPillars 的 loss 也是各个 detection head 的输出的 loss 之和,
且大部分 loss 计算时需要考虑 objectness 的 ground truth 是否为真, 因为只有
objectness 为真的 anchor box 才需要考虑它的 loc loss, class loss&#x2026;
</p>

<p>
但是在处理 objectness 的 loss (这里是 occupancy 的 loss) 时, 需要考虑正负样本的平衡问题. ssd 使用的是 hard negative mining, 而 PointPillars 使用了 focal loss
</p>
</div>

<div id="outline-container-org0000046" class="outline-4">
<h4 id="org0000046"><span class="section-number-4">1.6.1.</span> focal loss</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
focal loss (<a href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a>) 是 RetinaNet 中用来解决正负样本不平衡的方法
</p>

<p>
正常的 bce loss 为:
</p>

<p>
\(\mathcal{L}=\begin{cases}
-log(p)&  y=1\\
-log(1-p)&  y=0 \\
\end{cases}\)
</p>

<p>
简单起见, 定义:
</p>

<p>
\(p_t=\begin{cases}
p&  y=1\\
1-p&  y=0 \\
\end{cases}\)
</p>

<p>
\(\alpha_t=\begin{cases}
\alpha&  y=1\\
1-\alpha&  y=0 \\
\end{cases}\)
</p>

<p>
正常的 bce loss 为:
</p>

<p>
\(\mathcal{L}=-log(p_t)\)
</p>

<p>
focal loss 为:
</p>

<p>
\(\mathcal{L}=-\alpha_t(1-p_t)^{\gamma}log(p_t)\)
</p>

<p>
其中 \(\alpha, \gamma\) 是预定义的常量, \(\alpha\) 默认为 0.25, \(\gamma\) 默认为 2
</p>
</div>

<div id="outline-container-org0000040" class="outline-5">
<h5 id="org0000040"><span class="section-number-5">1.6.1.1.</span> \(\gamma\) 调节 容易/困难 样本的权重</h5>
<div class="outline-text-5" id="text-1-6-1-1">
<p>
当 \(p_t\) 较大(接近 1)时, 说明处理的是容易的样本, loss 会乘以一个非常小的
\((1-p_t)^\gamma\),导致 loss 变得很小
</p>

<p>
当 \(p_t\) 较小(接近 0)时, 说明处理的是困难的样本, loss 会乘以一个大一些的的
\((1-p_t)^\gamma\), 导致 loss 不会变得很小
</p>
</div>
</div>

<div id="outline-container-org0000043" class="outline-5">
<h5 id="org0000043"><span class="section-number-5">1.6.1.2.</span> \(\alpha\) 调用正负样本的权重</h5>
<div class="outline-text-5" id="text-1-6-1-2">
<p>
按照直觉, \(\alpha\) 是用来调节正负样本比例的, 例如 \(\alpha = 0.75\) 表示正样本的
loss 会有一个 0.75 的权重, 而负样本 loss 权重为 0.25, 但实际实验的结果表明
\(\alpha\) 在调节正负样比例方面用处不大&#x2026;可能是由于大量负样本通常也是容易的样本,
所以通过 \(\gamma\) 就可以达到目的
</p>
</div>
</div>
</div>
</div>
</div>



<div id="outline-container-org000004f" class="outline-2 references">
<h2 id="org000004f">Backlinks</h2>
<div class="outline-text-2" id="text-org000004f">
<p>
<a href="object_detection.html#ID-5a090a6c-c6bf-44b4-88f1-5fb9d1cef73c">Object Detection</a>
(<i>Object Detection &gt; Point Pillars</i>):   <a href="point_pillars.html#ID-57f3effe-1370-44a6-87c7-b5d2ea7da9e6">Point Pillars</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2022-01-13 Thu 00:00<br />
Last updated: 2022-07-17 Sun 11:39</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
