<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Word2vec</title>


<link rel="stylesheet" type="text/css" href="/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="./htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/readtheorg.css"/>
<link rel="stylesheet" type="text/css" href="./readtheorg.css"/>
<link rel="stylesheet" type="text/css" href="../readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
</head>
<body>
<div id="content" class="content">
<h1 class="title">Word2vec</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0000019">1. Word2vec</a>
<ul>
<li><a href="#org000000c">1.1. 训练过程</a>
<ul>
<li><a href="#org0000001">1.1.1. n-gram</a></li>
<li><a href="#ID-8cf28ef6-4d2d-45aa-9bba-f8b65de82b8f">1.1.2. cbow</a></li>
<li><a href="#ID-9f70d896-fc3e-4135-a4f6-03fe25d36cfc">1.1.3. skip-gram</a></li>
</ul>
</li>
<li><a href="#org0000010">1.2. gensim</a></li>
<li><a href="#org0000013">1.3. pytorch.nn.Embedding</a></li>
<li><a href="#org0000016">1.4. 使用 word2vec 的例子</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000019" class="outline-2">
<h2 id="org0000019"><span class="section-number-2">1.</span> Word2vec</h2>
<div class="outline-text-2" id="text-1">
<p>
在 的例子中, 使用了 word2vec
</p>

<p>
普通的 rnn 对单词的索引使用 one-hot 编码, 所以每个 feature 变成一个大小为 N (N
为单词的个数) 的 vector. 这种做法有两个问题:
</p>

<ol class="org-ol">
<li>vector 太大且没有必要, 因为这个 vector 绝大部分都是 0</li>

<li>这个 vector 无法反映不同单词之间的相似性, 例如, apple 和 orange
很相似 (都是水果, 都可以吃,&#x2026;), 相似表示两者可以互相替代,
机器学习时学习到的参数 (权重) 可以共享等</li>
</ol>

<p>
因此, 我们会使用一种称为 word2vec 或者叫 word embedding 的技术代替
one-hot 编码
</p>

<p>
word2vec 其实就是一个反映各个单词特征的 matrix, 例如:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">king</th>
<th scope="col" class="org-right">queen</th>
<th scope="col" class="org-right">apple</th>
<th scope="col" class="org-right">orange</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">human?</td>
<td class="org-right">0.99</td>
<td class="org-right">0.98</td>
<td class="org-right">0.01</td>
<td class="org-right">0.01</td>
</tr>

<tr>
<td class="org-left">fruit?</td>
<td class="org-right">0.01</td>
<td class="org-right">0.02</td>
<td class="org-right">0.99</td>
<td class="org-right">0.98</td>
</tr>

<tr>
<td class="org-left">food?</td>
<td class="org-right">0.01</td>
<td class="org-right">0.01</td>
<td class="org-right">0.99</td>
<td class="org-right">0.99</td>
</tr>

<tr>
<td class="org-left">size</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
</table>

<p>
以上面特征 (human?, fruit?&#x2026;) 是我们事先选定的, 但这个 matrix
实际上可以通过训练让机器自己学习到.
</p>

<p>
word embedding 和 CNN 中的卷积很像: 都是反映了数据的特征,
都可以通过训练学习到
</p>
</div>

<div id="outline-container-org000000c" class="outline-3">
<h3 id="org000000c"><span class="section-number-3">1.1.</span> 训练过程</h3>
<div class="outline-text-3" id="text-1-1">
<p>
word2vec 获得的 embedding 是通过训练得到的, 它是一个简单的使用非监督学习的网络:
</p>

<p>
从训练样本中选取一些词做为输入, 让它去预测另一个词, hidden layer 的输出即为
embedding
</p>

<p>
以 `the quick brown fox jumps over the lazy dog` 为例
</p>
</div>

<div id="outline-container-org0000001" class="outline-4">
<h4 id="org0000001"><span class="section-number-4">1.1.1.</span> n-gram</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
输入为连续 n 个词, 预测标签是第 n+1 个词:
</p>

<p>
当 n=3 时:
</p>

<pre class="example" id="org0000000">
the quick brown -&gt; fox
quick brown fox -&gt; jumps
brown fox jumps -&gt; over
...
</pre>

<p>
由于训练时使用 3 个词做为输入, 但用来获得 embedding 时只使用 1 个词做为输入, 所以网络的设计也许可以这样:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">get_training_model</span><span class="org-parenthesis">()</span>:
    <span class="org-variable-name">input_1</span> <span class="org-operator">=</span> Input<span class="org-parenthesis">(</span>N<span class="org-parenthesis">)</span>
    <span class="org-variable-name">input_2</span> <span class="org-operator">=</span> Input<span class="org-parenthesis">(</span>N<span class="org-parenthesis">)</span>
    <span class="org-variable-name">input_3</span> <span class="org-operator">=</span> Input<span class="org-parenthesis">(</span>N<span class="org-parenthesis">)</span>
    <span class="org-variable-name">embedding</span> <span class="org-operator">=</span> Dense<span class="org-parenthesis">(</span>EMBEDDING_SIZE<span class="org-parenthesis">)</span>
    <span class="org-variable-name">output_1</span> <span class="org-operator">=</span> embedding<span class="org-parenthesis">(</span>input_1<span class="org-parenthesis">,</span> name<span class="org-operator">=</span><span class="org-string">"output"</span><span class="org-parenthesis">)</span>
    <span class="org-variable-name">output_2</span> <span class="org-operator">=</span> embedding<span class="org-parenthesis">(</span>input_2<span class="org-parenthesis">)</span>
    <span class="org-variable-name">output_3</span> <span class="org-operator">=</span> embedding<span class="org-parenthesis">(</span>input_3<span class="org-parenthesis">)</span>
    <span class="org-variable-name">output</span> <span class="org-operator">=</span> Concatenate<span class="org-parenthesis">()</span>[output_1<span class="org-parenthesis">,</span> output_2<span class="org-parenthesis">,</span> output_3]
    <span class="org-variable-name">output</span> <span class="org-operator">=</span> Dense<span class="org-parenthesis">(</span>N<span class="org-parenthesis">)(</span>output<span class="org-parenthesis">)</span>
    <span class="org-variable-name">output</span> <span class="org-operator">=</span> Softmax<span class="org-parenthesis">()(</span>output<span class="org-parenthesis">)</span>
    <span class="org-keyword">return</span> Model<span class="org-parenthesis">(</span><span class="org-builtin">input</span><span class="org-operator">=</span>[input_1<span class="org-parenthesis">,</span> input_2<span class="org-parenthesis">,</span> input_3]<span class="org-parenthesis">,</span> output<span class="org-operator">=</span>output<span class="org-parenthesis">)</span>


<span class="org-keyword">def</span> <span class="org-function-name">get_eval_model</span><span class="org-parenthesis">()</span>:
    <span class="org-variable-name">model</span> <span class="org-operator">=</span> get_training_model<span class="org-parenthesis">()</span>
    model.load_weights<span class="org-parenthesis">(</span><span class="org-string">"save.h5"</span><span class="org-parenthesis">)</span>
    <span class="org-keyword">return</span> Model<span class="org-parenthesis">(</span><span class="org-builtin">input</span><span class="org-operator">=</span>model.inputs[0]<span class="org-parenthesis">,</span> output<span class="org-operator">=</span>model.get_layer<span class="org-parenthesis">(</span><span class="org-string">"output"</span><span class="org-parenthesis">)</span>.output<span class="org-parenthesis">)</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-ID-8cf28ef6-4d2d-45aa-9bba-f8b65de82b8f" class="outline-4">
<h4 id="ID-8cf28ef6-4d2d-45aa-9bba-f8b65de82b8f"><span class="section-number-4">1.1.2.</span> cbow</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
用两边的词预测中间的词
</p>

<pre class="example" id="org0000004">
the brown -&gt; quick
quick fox -&gt; brown
brown jumps -&gt; fox
...
</pre>
</div>


<div id="outline-container-org0000005" class="outline-5 references">
<h5 id="org0000005">Backlinks</h5>
<div class="outline-text-5" id="text-org0000005">
<p>
<a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">Attention</a>
(<i>Attention &gt; BERT &gt; Train BERT</i>):  BERT 实际上类似于 word2vec, 随机 mask 的作法和 <a href="#ID-8cf28ef6-4d2d-45aa-9bba-f8b65de82b8f">cbow</a> 也有些类似, 但它用起来与 word2vec 并不完全相同:
</p>
</div>
</div>
</div>


<div id="outline-container-ID-9f70d896-fc3e-4135-a4f6-03fe25d36cfc" class="outline-4">
<h4 id="ID-9f70d896-fc3e-4135-a4f6-03fe25d36cfc"><span class="section-number-4">1.1.3.</span> skip-gram</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
使用中的词预测两边的词   
</p>

<pre class="example" id="org0000008">
quick -&gt; the brown
brown -&gt; quick fox
fox -&gt; brown jumps
...
</pre>
</div>


<div id="outline-container-org0000009" class="outline-5 references">
<h5 id="org0000009">Backlinks</h5>
<div class="outline-text-5" id="text-org0000009">
<p>
<a href="gnn.html#ID-626264dd-6611-4a74-8609-74946f47a3c4">GNN</a>
(<i>GNN &gt; Embedding &gt; DeepWalk</i>):      2. 在 node 序列上使用 <a href="#ID-9f70d896-fc3e-4135-a4f6-03fe25d36cfc">skip-gram</a> 得到 embedding
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org0000010" class="outline-3">
<h3 id="org0000010"><span class="section-number-3">1.2.</span> gensim</h3>
<div class="outline-text-3" id="text-1-2">
<p>
gensim 工具可以帮助我们进行上面的训练过程
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> gensim.models <span class="org-keyword">import</span> word2vec
<span class="org-variable-name">sentences</span> <span class="org-operator">=</span> word2vec.Text8Corpus<span class="org-parenthesis">(</span><span class="org-string">"../extra/xwz.txt"</span><span class="org-parenthesis">)</span>
<span class="org-variable-name">vec</span> <span class="org-operator">=</span> word2vec.Word2Vec<span class="org-parenthesis">(</span>sentences<span class="org-parenthesis">,</span> size<span class="org-operator">=</span>50<span class="org-parenthesis">,</span> <span class="org-builtin">iter</span><span class="org-operator">=</span>5<span class="org-parenthesis">,</span> min_count<span class="org-operator">=</span>0<span class="org-parenthesis">)</span>
<span class="org-builtin">print</span><span class="org-parenthesis">(</span>vec[<span class="org-string">"&#25105;"</span>]<span class="org-parenthesis">)</span>
<span class="org-builtin">print</span><span class="org-parenthesis">(</span>vec[<span class="org-string">"&#25105;"</span>].shape<span class="org-parenthesis">)</span>
<span class="org-builtin">print</span><span class="org-parenthesis">(</span>vec.wv.vocab[<span class="org-string">"&#25105;"</span>].index<span class="org-parenthesis">)</span>
<span class="org-builtin">print</span><span class="org-parenthesis">(</span>vec.wv.vectors[5]<span class="org-parenthesis">)</span>
<span class="org-variable-name">vocabulary</span> <span class="org-operator">=</span> vec.wv.vocab.keys<span class="org-parenthesis">()</span>
<span class="org-comment-delimiter">#</span><span class="org-comment">print(vocabulary)</span>
</pre>
</div>

<pre class="example" id="org000000f">
[ 2.5379989e-01 -7.9855943e-01  2.1004373e-01  1.4060766e-01
  1.5365046e+00 -1.2147485e-02 -1.1567525e+00  1.4223902e-01
 -1.1346768e+00  6.0506749e-01 -8.8734490e-01 -5.5807270e-02
  7.3605202e-02 -5.1732904e-01  8.1780031e-02  4.4866718e-02
 -3.8633597e-01 -5.1058096e-01 -5.1415718e-01 -8.0242729e-01
 -7.0677415e-02  2.1593498e-01  9.4420961e-05  1.5452372e-01
 -4.0667549e-01  2.0172779e-01 -3.7259430e-01 -8.9318126e-01
  1.7669131e-01 -6.7675710e-01  3.1780142e-01 -1.6200885e+00
  6.8548656e-01 -2.7231857e-01  7.0585549e-01  3.9321697e-01
 -8.2931936e-01  9.1785020e-01 -4.0526435e-01  1.2794651e-01
  3.5713235e-01  6.9550622e-01 -3.1667909e-01 -4.8001361e-01
  4.3599684e-02 -1.1073679e+00  9.8762310e-01 -4.1203447e-02
  7.0740056e-01  5.6313020e-01]
(50,)
5
[ 2.5379989e-01 -7.9855943e-01  2.1004373e-01  1.4060766e-01
  1.5365046e+00 -1.2147485e-02 -1.1567525e+00  1.4223902e-01
 -1.1346768e+00  6.0506749e-01 -8.8734490e-01 -5.5807270e-02
  7.3605202e-02 -5.1732904e-01  8.1780031e-02  4.4866718e-02
 -3.8633597e-01 -5.1058096e-01 -5.1415718e-01 -8.0242729e-01
 -7.0677415e-02  2.1593498e-01  9.4420961e-05  1.5452372e-01
 -4.0667549e-01  2.0172779e-01 -3.7259430e-01 -8.9318126e-01
  1.7669131e-01 -6.7675710e-01  3.1780142e-01 -1.6200885e+00
  6.8548656e-01 -2.7231857e-01  7.0585549e-01  3.9321697e-01
 -8.2931936e-01  9.1785020e-01 -4.0526435e-01  1.2794651e-01
  3.5713235e-01  6.9550622e-01 -3.1667909e-01 -4.8001361e-01
  4.3599684e-02 -1.1073679e+00  9.8762310e-01 -4.1203447e-02
  7.0740056e-01  5.6313020e-01]


/usr/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
  after removing the cwd from sys.path.
/usr/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).
  """
</pre>

<p>
more example about gensim and word2vec:
</p>

<p>
<a href="https://textminingonline.com/training-a-chinese-wikipedia-word2vec-model-by-gensim-and-jieba">https://textminingonline.com/training-a-chinese-wikipedia-word2vec-model-by-gensim-and-jieba</a>
</p>
</div>
</div>

<div id="outline-container-org0000013" class="outline-3">
<h3 id="org0000013"><span class="section-number-3">1.3.</span> pytorch.nn.Embedding</h3>
<div class="outline-text-3" id="text-1-3">
<p>
pytorch 的 nn.Embedding 是一个简单的 word2vec 的查找表, 可以支持 load
训练好的 word2vec weight:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">self</span>.<span class="org-variable-name">encoder</span> <span class="org-operator">=</span> torch.nn.Embedding<span class="org-parenthesis">(</span>VOCABULARY_SIZE<span class="org-parenthesis">,</span> FEATURE_SIZE<span class="org-parenthesis">)</span>
<span class="org-keyword">self</span>.encoder.weight.data.copy_<span class="org-parenthesis">(</span>torch.from_numpy<span class="org-parenthesis">(</span>vec.wv.vectors<span class="org-parenthesis">))</span>
<span class="org-keyword">self</span>.encoder.<span class="org-variable-name">requires_grad</span> <span class="org-operator">=</span> <span class="org-constant">False</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000016" class="outline-3">
<h3 id="org0000016"><span class="section-number-3">1.4.</span> 使用 word2vec 的例子</h3>
<div class="outline-text-3" id="text-1-4">
<p>
<a href="file:///backup/sunway/Gitbox/code/machine_learning/pytorch/random_chinese/random_chinese.py">file:~/Gitbox/code/machine_learning/pytorch/random_chinese/random_chinese.py</a>
</p>
</div>
</div>
</div>


<div id="outline-container-org000001c" class="outline-2 references">
<h2 id="org000001c">Backlinks</h2>
<div class="outline-text-2" id="text-org000001c">
<p>
<a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">Attention</a>
(<i>Attention &gt; Transformer &gt; InputEmbedding</i>):  InputEmbedding 通常是一个 word embedding (<a href="word2vec.html#ID-164e18f2-14b2-486b-8e39-8c69d9f09bc1">Word2vec</a>), 与 rnn 不同的是, InputEmbedding 可以被并行处理.
</p>

<p>
<a href="gnn.html#ID-626264dd-6611-4a74-8609-74946f47a3c4">GNN</a>
(<i>GNN &gt; Embedding</i>):  embedding 是用无监督的方法获得低维的向量表示, 例如 <a href="word2vec.html#ID-164e18f2-14b2-486b-8e39-8c69d9f09bc1">Word2vec</a>
</p>

<p>
<a href="rnn.html#ID-8f0c1075-5073-4219-b3d9-5088c8729c6a">RNN</a>
(<i>RNN &gt; Word2vec</i>):   <a href="word2vec.html#ID-164e18f2-14b2-486b-8e39-8c69d9f09bc1">Word2vec</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2021-08-27 Fri 00:00<br />
Last updated: 2024-01-31 Wed 17:35</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>