<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>TensorRT</title>


           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">TensorRT</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0000027">1. TensorRT</a>
<ul>
<li><a href="#ID-beebccdd-76dc-4f17-afa4-074611a912d2">1.1. Quantization Aware Training</a>
<ul>
<li><a href="#org0000000">1.1.1. Overview</a></li>
<li><a href="#org000001c">1.1.2. Impl</a></li>
<li><a href="#org0000021">1.1.3. TensorRT Optimizer</a></li>
</ul>
</li>
<li><a href="#org0000024">1.2. Sparsity</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000027" class="outline-2">
<h2 id="org0000027"><span class="section-number-2">1.</span> TensorRT</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-ID-beebccdd-76dc-4f17-afa4-074611a912d2" class="outline-3">
<h3 id="ID-beebccdd-76dc-4f17-afa4-074611a912d2"><span class="section-number-3">1.1.</span> Quantization Aware Training</h3>
<div class="outline-text-3" id="text-1-1">
</div>

<div id="outline-container-org0000000" class="outline-4">
<h4 id="org0000000"><span class="section-number-4">1.1.1.</span> Overview</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
<a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/">https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/</a>
</p>

<p>
<a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization">https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization</a>
</p>

<p>
TensorRT 的 pytorch_quantization 是一个实现 fake quantization 的 pytorch plugin
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-10-29 14:24</span>
<span class="org-keyword">import</span> os
<span class="org-keyword">import</span> torch
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> nn
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> optim
<span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader, Dataset
<span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-keyword">import</span> torch.onnx

<span class="org-keyword">from</span> pytorch_quantization <span class="org-keyword">import</span> nn <span class="org-keyword">as</span> quant_nn
<span class="org-keyword">from</span> pytorch_quantization <span class="org-keyword">import</span> calib
<span class="org-keyword">from</span> pytorch_quantization.tensor_quant <span class="org-keyword">import</span> QuantDescriptor
<span class="org-keyword">from</span> pytorch_quantization <span class="org-keyword">import</span> quant_modules
<span class="org-keyword">from</span> absl <span class="org-keyword">import</span> logging

logging.set_verbosity(logging.FATAL)


<span class="org-keyword">def</span> <span class="org-function-name">export_to_onnx</span>(model):
    model.<span class="org-builtin">eval</span>()
    <span class="org-variable-name">dummy_input</span> = torch.randn(1, 5)

    <span class="org-comment-delimiter"># </span><span class="org-comment">use_fb_fake_quant &#26159; tensorrt &#30340;&#19968;&#20010; hack: &#24403;&#38656;&#35201;&#23548;&#20986;&#20026; onnx &#26102;&#35774;&#20026; true,</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">forward &#26102; fake_tensor_quant &#20250;&#34987;&#26367;&#25442;&#20026; _fb_fake_quant, &#21518;&#32773;&#20250;&#35843;&#29992;torch &#33258;</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">&#24049;&#30340; fake_quantize_per_channel_affine &#20197;&#20415;&#23548;&#20986;&#20026; onnx &#30340;</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">QuantizeLinear/DequantizeLinear</span>
    quant_nn.TensorQuantizer.<span class="org-variable-name">use_fb_fake_quant</span> = <span class="org-constant">True</span>

    torch.onnx.export(
        model,
        dummy_input,
        <span class="org-string">"fake.onnx"</span>,
        opset_version=13,
    )
    quant_nn.TensorQuantizer.<span class="org-variable-name">use_fb_fake_quant</span> = <span class="org-constant">False</span>


<span class="org-keyword">def</span> <span class="org-function-name">get_data</span>():
    <span class="org-keyword">class</span> <span class="org-type">PlainDataset</span>(Dataset):
        <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
            <span class="org-variable-name">x</span> = torch.<span class="org-builtin">round</span>(torch.rand(10000) * 200)
            <span class="org-variable-name">x</span> = x.unsqueeze(1)
            <span class="org-variable-name">x</span> = torch.cat((x, x * 2, x * 3, x * 4, x * 5), 1)
            <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> = x

        <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
            <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index]

        <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
            <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)

    <span class="org-variable-name">training_set</span> = PlainDataset()
    <span class="org-keyword">return</span> DataLoader(training_set, batch_size=100, shuffle=<span class="org-constant">True</span>)


<span class="org-keyword">def</span> <span class="org-function-name">pretrain_model</span>():
    <span class="org-variable-name">model</span> = nn.Sequential(
        nn.Linear(5, 1),
        nn.Linear(1, 5),
    )

    <span class="org-keyword">if</span> os.path.exists(<span class="org-string">"model.pt"</span>):
        <span class="org-keyword">return</span>

    train_model(model)
    torch.save(model.state_dict(), <span class="org-string">"model.pt"</span>)


<span class="org-keyword">def</span> <span class="org-function-name">train_model</span>(model, epoch=500):
    <span class="org-variable-name">criterion</span> = nn.MSELoss()
    <span class="org-variable-name">optimizer</span> = optim.Adam(model.parameters())

    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(epoch):
        <span class="org-keyword">for</span> x <span class="org-keyword">in</span> get_data():
            <span class="org-variable-name">loss</span> = criterion(model(x), x)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        <span class="org-comment-delimiter"># </span><span class="org-comment">if i % 10 == 0:</span>
        <span class="org-comment-delimiter">#     </span><span class="org-comment">print("epoch #%d: loss: %f" % (i, loss.detach().item()))</span>


<span class="org-keyword">def</span> <span class="org-function-name">load_model</span>():
    <span class="org-variable-name">quant_desc_input</span> = QuantDescriptor(calib_method=<span class="org-string">"histogram"</span>)
    quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)
    quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)

    quant_modules.initialize()

    <span class="org-variable-name">model</span> = nn.Sequential(
        nn.Linear(5, 1),
        nn.Linear(1, 5),
    )
    model.load_state_dict(torch.load(<span class="org-string">"model.pt"</span>))
    <span class="org-keyword">return</span> model


<span class="org-keyword">def</span> <span class="org-function-name">fake_quantize</span>(model):
    <span class="org-keyword">def</span> <span class="org-function-name">collect_stats</span>(model, data, num_batches):
        <span class="org-keyword">for</span> name, module <span class="org-keyword">in</span> model.named_modules():
            <span class="org-keyword">if</span> <span class="org-builtin">isinstance</span>(module, quant_nn.TensorQuantizer):
                module.disable_quant()
                module.enable_calib()

        <span class="org-keyword">for</span> i, data <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(data):
            model(data)
            <span class="org-keyword">if</span> i &gt;= num_batches:
                <span class="org-keyword">break</span>
        <span class="org-keyword">for</span> name, module <span class="org-keyword">in</span> model.named_modules():
            <span class="org-keyword">if</span> <span class="org-builtin">isinstance</span>(module, quant_nn.TensorQuantizer):
                module.enable_quant()
                module.disable_calib()

    <span class="org-keyword">def</span> <span class="org-function-name">compute_amax</span>(model, **kwargs):
        <span class="org-keyword">for</span> name, module <span class="org-keyword">in</span> model.named_modules():
            <span class="org-keyword">if</span> <span class="org-builtin">isinstance</span>(module, quant_nn.TensorQuantizer):
                <span class="org-keyword">if</span> <span class="org-builtin">isinstance</span>(module._calibrator, calib.MaxCalibrator):
                    module.load_calib_amax()
                <span class="org-keyword">else</span>:
                    module.load_calib_amax(method=<span class="org-string">"percentile"</span>)

    <span class="org-keyword">with</span> torch.no_grad():
        collect_stats(model, get_data(), num_batches=1000)
        compute_amax(model)

    <span class="org-comment-delimiter"># </span><span class="org-comment">for name, module in model.named_modules():</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">if isinstance(module, quant_nn.TensorQuantizer):</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">print("------")</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">print(name, module)</span>

    <span class="org-keyword">return</span> model


<span class="org-keyword">def</span> <span class="org-function-name">test</span>(model):
    <span class="org-variable-name">x</span> = torch.tensor([[10, 20, 30, 40, 50]]).<span class="org-builtin">float</span>()
    <span class="org-variable-name">y_hat</span> = model(x)
    <span class="org-builtin">print</span>(y_hat)


<span class="org-keyword">if</span> <span class="org-builtin">__name__</span> == <span class="org-string">"__main__"</span>:
    pretrain_model()

    <span class="org-variable-name">model</span> = load_model()
    test(model)

    fake_quantize(model)
    test(model)

    train_model(model, epoch=1)
    fake_quantize(model)
    test(model)
</pre>
</div>
</div>
</div>

<div id="outline-container-org000001c" class="outline-4">
<h4 id="org000001c"><span class="section-number-4">1.1.2.</span> Impl</h4>
<div class="outline-text-4" id="text-1-1-2">
</div>
<div id="outline-container-org0000003" class="outline-5">
<h5 id="org0000003"><span class="section-number-5">1.1.2.1.</span> monkey patching</h5>
<div class="outline-text-5" id="text-1-1-2-1">
<p>
quant_modules.initialize 会把 torch.nn 中的 Linear 等模块替换成 QuantLinear 等模块
</p>
</div>
</div>

<div id="outline-container-org0000013" class="outline-5">
<h5 id="org0000013"><span class="section-number-5">1.1.2.2.</span> foward</h5>
<div class="outline-text-5" id="text-1-1-2-2">
</div>
<div id="outline-container-org0000006" class="outline-6">
<h6 id="org0000006"><span class="section-number-6">1.1.2.2.1.</span> QuantLinear</h6>
<div class="outline-text-6" id="text-1-1-2-2-1">
<p>
forward 会被 propagate 给两个 TensorQuantizer: input_quantizer 和
weight_quantizer
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
    <span class="org-variable-name">quant_input</span> = <span class="org-keyword">self</span>._input_quantizer(<span class="org-builtin">input</span>)
    <span class="org-variable-name">quant_weight</span> = <span class="org-keyword">self</span>._weight_quantizer(<span class="org-keyword">self</span>.weight)
    <span class="org-variable-name">output</span> = F.linear(quant_input, quant_weight, bias=<span class="org-keyword">self</span>.bias)
    <span class="org-keyword">return</span> output
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000009" class="outline-6">
<h6 id="org0000009"><span class="section-number-6">1.1.2.2.2.</span> TensorQuantizer</h6>
<div class="outline-text-6" id="text-1-1-2-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, inputs):
    <span class="org-keyword">if</span> <span class="org-keyword">self</span>._disabled:
        <span class="org-keyword">return</span> inputs

    <span class="org-variable-name">outputs</span> = inputs

    <span class="org-comment-delimiter"># </span><span class="org-comment">&#35843;&#29992; TensorQuantizer.enable_calib() &#20250;&#35774;&#32622; _if_calib, &#34920;&#31034; calibrator &#38656;&#35201;</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">&#35760;&#24405;&#20197;&#33719;&#24471;&#26368;&#22823;&#20540; amax (abs max)</span>
    <span class="org-keyword">if</span> <span class="org-keyword">self</span>._if_calib:
        <span class="org-keyword">self</span>._calibrator.collect(inputs)

    <span class="org-comment-delimiter"># </span><span class="org-comment">&#35843;&#29992; enable_quant() &#20250;&#35774;&#32622; _if_quant, &#34920;&#31034;&#38656;&#35201;&#20351;&#29992; amax &#36827;&#34892; fake quant&#20351;</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">&#29992;&#26102;&#38656;&#35201;&#20808; enable calib, disable quant, calib &#23436;&#25104;&#21518;&#35745;&#31639; amax, &#28982;&#21518;&#20877;</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">enable quant, disable calib &#36827;&#34892;&#24102; fake quant &#30340; evaluation</span>
    <span class="org-keyword">if</span> <span class="org-keyword">self</span>._if_quant:
        <span class="org-variable-name">outputs</span> = <span class="org-keyword">self</span>._quant_forward(inputs)

    <span class="org-keyword">return</span> outputs


<span class="org-keyword">def</span> <span class="org-function-name">_quant_forward</span>(<span class="org-keyword">self</span>, inputs):
    <span class="org-comment-delimiter"># </span><span class="org-comment">amax &#26102;&#36890;&#36807; load_calib_amax &#35745;&#31639;&#30340; amax</span>
    <span class="org-variable-name">amax</span> = <span class="org-keyword">self</span>._get_amax(inputs)
    <span class="org-comment-delimiter"># </span><span class="org-comment">fake_tensor_quant &#26159;&#27880;&#20876;&#30340; autograd function</span>
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> TensorQuantizer.use_fb_fake_quant:
        <span class="org-variable-name">outputs</span> = fake_tensor_quant(
            inputs, amax, <span class="org-keyword">self</span>._num_bits, <span class="org-keyword">self</span>._unsigned, <span class="org-keyword">self</span>._narrow_range
        )
    <span class="org-keyword">else</span>:
        <span class="org-variable-name">outputs</span> = <span class="org-keyword">self</span>._fb_fake_quant(inputs, amax)
    <span class="org-keyword">return</span> outputs
</pre>
</div>
</div>
</div>

<div id="outline-container-org000000c" class="outline-6">
<h6 id="org000000c"><span class="section-number-6">1.1.2.2.3.</span> fake_tensor_quant</h6>
<div class="outline-text-6" id="text-1-1-2-2-3">
<p>
<a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html</a>
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">FakeTensorQuantFunction</span>(Function):
    @<span class="org-builtin">staticmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(ctx, inputs, amax, num_bits=8, unsigned=<span class="org-constant">False</span>, narrow_range=<span class="org-constant">True</span>):
        ctx.save_for_backward(inputs, amax)
        <span class="org-comment-delimiter"># </span><span class="org-comment">fake quant &#21363; outputs=dequant(quant(inputs))</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">1. quant, &#35745;&#31639; scale &#24182;&#35745;&#31639; outputs=inputs*scale</span>
        <span class="org-variable-name">outputs</span>, <span class="org-variable-name">scale</span> = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
        <span class="org-comment-delimiter"># </span><span class="org-comment">2. dequant, outputs=outputs/scale</span>
        <span class="org-keyword">return</span> outputs / scale.to(inputs.dtype)

    @<span class="org-builtin">staticmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">backward</span>(ctx, grad_outputs):
        <span class="org-comment-delimiter"># </span><span class="org-comment">...</span>

<span class="org-variable-name">fake_tensor_quant</span> = FakeTensorQuantFunction.<span class="org-builtin">apply</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000010" class="outline-6">
<h6 id="org0000010"><span class="section-number-6">1.1.2.2.4.</span> _fb_fake_quant</h6>
<div class="outline-text-6" id="text-1-1-2-2-4">
<p>
在导出成 onnx 时需要通过设置 use_fb_fake_quant = True 调用_fb_fake_quant, 后者会调用 torch.fake_quantize_per_channel_affine,以便 export 时能导出成 onnx 标准的
QuantizeLinear / DequantizeLinear
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">_fb_fake_quant</span>(<span class="org-keyword">self</span>, inputs, amax):
    <span class="org-variable-name">bound</span> = (1 &lt;&lt; (<span class="org-keyword">self</span>._num_bits - 1 + <span class="org-builtin">int</span>(<span class="org-keyword">self</span>._unsigned))) - 1
    <span class="org-variable-name">outputs</span> = torch.fake_quantize_per_tensor_affine(
        inputs,
        amax.item() / bound,
        0,
        -bound - 1 <span class="org-keyword">if</span> <span class="org-keyword">not</span> <span class="org-keyword">self</span>._unsigned <span class="org-keyword">else</span> 0,
        bound,
    )
    <span class="org-keyword">return</span> outputs
</pre>
</div>

<p>
导出成 onnx 的结果为:
</p>


<div id="org000000f" class="figure">
<p><img src="../extra/trt_quant.png" alt="trt_quant.png" />
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org0000019" class="outline-5">
<h5 id="org0000019"><span class="section-number-5">1.1.2.3.</span> backward</h5>
<div class="outline-text-5" id="text-1-1-2-3">
</div>
<div id="outline-container-org0000016" class="outline-6">
<h6 id="org0000016"><span class="section-number-6">1.1.2.3.1.</span> FakeTensorQuantFunction</h6>
<div class="outline-text-6" id="text-1-1-2-3-1">
<p>
Straight Through Estimation (STE) with clipping
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">backward</span>(ctx, grad_outputs):
    <span class="org-variable-name">inputs</span>, <span class="org-variable-name">amax</span> = ctx.saved_tensors
    <span class="org-variable-name">zero</span> = grad_outputs.new_zeros(1)
    <span class="org-variable-name">grad_inputs</span> = torch.where(inputs.<span class="org-builtin">abs</span>() &lt;= amax, grad_outputs, zero)
    <span class="org-keyword">return</span> grad_inputs, <span class="org-constant">None</span>, <span class="org-constant">None</span>, <span class="org-constant">None</span>, <span class="org-constant">None</span>
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000021" class="outline-4">
<h4 id="org0000021"><span class="section-number-4">1.1.3.</span> TensorRT Optimizer</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
<a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31653/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31653/</a>
</p>

<p>
QuantLinear 把原来的 \(w*x+b\) 变成为 \(DQ(Q(w))*DQ(Q(x))+b\), 后续需要 tensorrt 的
optimizer 把 DQ 和 Q 前移或后移达到量化计算的目的, 例如:
</p>

<pre class="example" id="org000001f">
x -&gt; Q1 -&gt; DQ1 -&gt; [mul] -&gt;  y
                    ^
w -&gt; Q2 -&gt; DQ2 -----|
</pre>

<p>
可以优化为:
</p>

<pre class="example" id="org0000020">
x -&gt; Q1 -&gt; [mul] -&gt; [DQ1 -&gt; DQ2] -&gt; y
             ^
    [w-&gt;Q2] -|
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000024" class="outline-3">
<h3 id="org0000024"><span class="section-number-3">1.2.</span> Sparsity</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</a>
</p>

<p>
<a href="https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity">https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity</a> 是一个做
prunning 的 pytorch 插件
</p>

<ol class="org-ol">
<li>它只会对 Linear, Conv2D 等的 weight 做 prunning,</li>

<li>它要求 weight 有特定的 shape, 比如对于 [x,y] 大小的 Linear Layer 需要 x%8==0,
y%16==0</li>

<li>在 pruning 时, 默认使用 m4n2_1d 的方式, 即在一维的方向上每 4 个数固定选择两个绝对值最小的数进行 prunning</li>

<li>prunning 之后会在 torch 模型中针对每个被 prune 的参数记录一个 mask buffer, 这个 buffer 有两个作用:

<ol class="org-ol">
<li>GPU 需要根据这个 buffer 进行 sparsity 操作</li>

<li>sparsity 工具 (ASP) 会对 pytorch 的 optimizer 进行 monkey patching, 修改过的 optimizer 会利用这个 buffer 保证对 prunning 后的模型进行训练时会跳过已经被 prune 的数据</li>
</ol></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-11-09 13:38</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-------------------- sparse_masklib.py --------------------</span>
<span class="org-keyword">import</span> sys
<span class="org-keyword">import</span> torch
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> collections
<span class="org-keyword">from</span> itertools <span class="org-keyword">import</span> permutations


<span class="org-keyword">def</span> <span class="org-function-name">fill</span>(x):
    <span class="org-keyword">return</span> <span class="org-builtin">float</span>(x.nonzero().size(0)) / torch.numel(x)


<span class="org-keyword">def</span> <span class="org-function-name">reshape_1d</span>(matrix, m):
    <span class="org-comment-delimiter"># </span><span class="org-comment">If not a nice multiple of m, fill with zeroes.</span>
    <span class="org-keyword">if</span> matrix.shape[1] % m &gt; 0:
        <span class="org-variable-name">mat</span> = torch.cuda.FloatTensor(
            matrix.shape[0], matrix.shape[1] + (m - matrix.shape[1] % m)
        ).fill_(0)
        mat[:, : matrix.shape[1]] = matrix
        <span class="org-variable-name">shape</span> = mat.shape
        <span class="org-keyword">return</span> mat.view(-1, m), shape
    <span class="org-keyword">else</span>:
        <span class="org-keyword">return</span> matrix.view(-1, m), matrix.shape


<span class="org-variable-name">valid_m4n2_1d_patterns</span> = <span class="org-constant">None</span>


<span class="org-keyword">def</span> <span class="org-function-name">compute_valid_1d_patterns</span>(m, n):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Early exit if patterns was already created.</span>
    <span class="org-keyword">global</span> valid_m4n2_1d_patterns

    <span class="org-keyword">if</span> m == 4 <span class="org-keyword">and</span> n == 2 <span class="org-keyword">and</span> valid_m4n2_1d_patterns <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
        <span class="org-keyword">return</span> valid_m4n2_1d_patterns
    <span class="org-variable-name">patterns</span> = torch.zeros(m)
    <span class="org-variable-name">patterns</span>[:n] = 1
    <span class="org-variable-name">valid_patterns</span> = torch.Tensor(<span class="org-builtin">list</span>(<span class="org-builtin">set</span>(permutations(patterns.tolist()))))
    <span class="org-keyword">if</span> m == 4 <span class="org-keyword">and</span> n == 2:
        <span class="org-variable-name">valid_m4n2_1d_patterns</span> = valid_patterns
    <span class="org-keyword">return</span> valid_patterns


<span class="org-string">""" m:n 1d structured best """</span>


<span class="org-keyword">def</span> <span class="org-function-name">mn_1d_best</span>(matrix, m, n):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Find all possible patterns.</span>
    <span class="org-variable-name">patterns</span> = compute_valid_1d_patterns(m, n).cuda()

    <span class="org-comment-delimiter"># </span><span class="org-comment">Find the best m:n pattern (sum of non-masked weights).</span>
    <span class="org-variable-name">mask</span> = torch.cuda.IntTensor(matrix.shape).fill_(1).view(-1, m)
    <span class="org-variable-name">mat</span>, <span class="org-variable-name">shape</span> = reshape_1d(matrix, m)
    <span class="org-variable-name">pmax</span> = torch.argmax(torch.matmul(mat.<span class="org-builtin">abs</span>(), patterns.t()), dim=1)
    <span class="org-variable-name">mask</span>[:] = patterns[pmax[:]]
    <span class="org-variable-name">mask</span> = mask.view(matrix.shape)
    <span class="org-keyword">return</span> mask


<span class="org-keyword">def</span> <span class="org-function-name">m4n2_1d</span>(mat, density):
    <span class="org-keyword">return</span> mn_1d_best(mat, 4, 2)


<span class="org-keyword">def</span> <span class="org-function-name">create_mask</span>(tensor, pattern=<span class="org-string">"m4n2_1d"</span>, density=0.5):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Reshape tensor and mask.</span>
    <span class="org-variable-name">shape</span> = tensor.shape
    <span class="org-variable-name">ttype</span> = tensor.<span class="org-builtin">type</span>()
    <span class="org-variable-name">t</span> = tensor.<span class="org-builtin">float</span>().contiguous()

    <span class="org-comment-delimiter"># </span><span class="org-comment">1d-tensor</span>
    <span class="org-keyword">if</span> <span class="org-builtin">len</span>(shape) == 1:
        <span class="org-variable-name">t</span> = t.view(1, shape[0])
        <span class="org-variable-name">func</span> = <span class="org-builtin">getattr</span>(sys.modules[<span class="org-builtin">__name__</span>], pattern, <span class="org-constant">None</span>)
        <span class="org-variable-name">mask</span> = func(t, density)
        <span class="org-keyword">return</span> mask.view(shape).<span class="org-builtin">type</span>(ttype)
    <span class="org-comment-delimiter"># </span><span class="org-comment">2d-tensor (in, out)</span>
    <span class="org-keyword">elif</span> <span class="org-builtin">len</span>(shape) == 2:
        <span class="org-variable-name">t</span> = t.view(shape[0], shape[1])
        <span class="org-variable-name">func</span> = <span class="org-builtin">getattr</span>(sys.modules[<span class="org-builtin">__name__</span>], pattern, <span class="org-constant">None</span>)
        <span class="org-variable-name">mask</span> = func(t, density)
        <span class="org-keyword">return</span> mask.view(shape).<span class="org-builtin">type</span>(ttype)
    <span class="org-comment-delimiter"># </span><span class="org-comment">3d-tensor (batch, in, out)</span>
    <span class="org-keyword">elif</span> <span class="org-builtin">len</span>(shape) == 3:
        <span class="org-variable-name">t</span> = t.view(shape[0] * shape[1], shape[2])
        <span class="org-variable-name">func</span> = <span class="org-builtin">getattr</span>(sys.modules[<span class="org-builtin">__name__</span>], pattern, <span class="org-constant">None</span>)
        <span class="org-variable-name">mask</span> = func(t, density)
        <span class="org-keyword">return</span> mask.view(shape).<span class="org-builtin">type</span>(ttype)
    <span class="org-comment-delimiter"># </span><span class="org-comment">4d-tensor (in, out, h, w)</span>
    <span class="org-keyword">elif</span> <span class="org-builtin">len</span>(shape) == 4:
        <span class="org-comment-delimiter"># </span><span class="org-comment">convs</span>
        <span class="org-variable-name">t</span> = (
            t.permute(2, 3, 0, 1)
            .contiguous()
            .view(shape[2] * shape[3] * shape[0], shape[1])
        )
        <span class="org-variable-name">func</span> = <span class="org-builtin">getattr</span>(sys.modules[<span class="org-builtin">__name__</span>], pattern, <span class="org-constant">None</span>)
        <span class="org-variable-name">mask</span> = func(t, density)
        <span class="org-variable-name">mask</span> = (
            mask.view(shape[2], shape[3], shape[0], shape[1])
            .permute(2, 3, 0, 1)
            .contiguous()
        )
        <span class="org-keyword">return</span> mask.view(shape).<span class="org-builtin">type</span>(ttype)


<span class="org-comment-delimiter"># </span><span class="org-comment">-------------------- asp.py --------------------</span>
<span class="org-keyword">import</span> types
<span class="org-keyword">import</span> torch

<span class="org-variable-name">torchvision_imported</span> = <span class="org-constant">True</span>
<span class="org-keyword">try</span>:
    <span class="org-keyword">import</span> torchvision
<span class="org-keyword">except</span> <span class="org-type">ImportError</span>:
    <span class="org-builtin">print</span>(<span class="org-string">"[ASP][Warning] torchvision cannot be imported."</span>)
    <span class="org-variable-name">torchvision_imported</span> = <span class="org-constant">False</span>


<span class="org-keyword">def</span> <span class="org-function-name">eligible_modules</span>(
    model, whitelist_layer_types, allowed_layer_names, disallowed_layer_names
):
    <span class="org-variable-name">eligible_modules_list</span> = []
    <span class="org-keyword">for</span> name, mod <span class="org-keyword">in</span> model.named_modules():
        <span class="org-keyword">if</span> (
            <span class="org-builtin">isinstance</span>(mod, whitelist_layer_types)
            <span class="org-keyword">and</span> name <span class="org-keyword">not</span> <span class="org-keyword">in</span> disallowed_layer_names
        ):
            <span class="org-keyword">if</span> allowed_layer_names <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span> <span class="org-keyword">and</span> name <span class="org-keyword">not</span> <span class="org-keyword">in</span> allowed_layer_names:
                <span class="org-keyword">continue</span>
            eligible_modules_list.append((name, mod))
    <span class="org-keyword">return</span> eligible_modules_list


<span class="org-keyword">class</span> <span class="org-type">ASP</span>:
    <span class="org-variable-name">__model</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">__verbosity</span> = 0
    <span class="org-variable-name">__optimizer</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">__sparse_parameters</span> = []
    <span class="org-variable-name">__calculate_mask</span> = <span class="org-constant">None</span>

    @<span class="org-builtin">classmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">init_model_for_pruning</span>(
        cls,
        model,
        mask_calculator=<span class="org-string">"m4n2_1d"</span>,
        verbosity=3,
        whitelist=[torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d],
        allowed_layer_names=<span class="org-constant">None</span>,
        disallowed_layer_names=[],
        allow_recompute_mask=<span class="org-constant">False</span>,
        custom_layer_dict={},
    ):
        <span class="org-keyword">assert</span> cls.__model <span class="org-keyword">is</span> <span class="org-constant">None</span>, <span class="org-string">"ASP has been initialized already."</span>
        cls.<span class="org-variable-name">__model</span> = model
        cls.<span class="org-variable-name">__verbosity</span> = verbosity

        <span class="org-keyword">if</span> <span class="org-builtin">isinstance</span>(mask_calculator, <span class="org-builtin">str</span>):

            <span class="org-keyword">def</span> <span class="org-function-name">create_mask_from_pattern</span>(param):
                <span class="org-keyword">return</span> create_mask(param, mask_calculator).<span class="org-builtin">bool</span>()

            cls.<span class="org-variable-name">__calculate_mask</span> = create_mask_from_pattern
        <span class="org-keyword">else</span>:
            cls.<span class="org-variable-name">__calculate_mask</span> = mask_calculator  <span class="org-comment-delimiter"># </span><span class="org-comment">user defined function</span>

        <span class="org-comment-delimiter"># </span><span class="org-comment">function to extract variables that will be sparsified.</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">idea is that you will add one of these functions for each module type that can be sparsified.</span>
        <span class="org-keyword">if</span> torchvision_imported:
            <span class="org-builtin">print</span>(
                <span class="org-string">"[ASP] torchvision is imported, can work with the MaskRCNN/KeypointRCNN from torchvision."</span>
            )
            <span class="org-variable-name">sparse_parameter_list</span> = {
                torch.nn.Linear: [<span class="org-string">"weight"</span>],
                torch.nn.Conv1d: [<span class="org-string">"weight"</span>],
                torch.nn.Conv2d: [<span class="org-string">"weight"</span>],
                torch.nn.Conv3d: [<span class="org-string">"weight"</span>],
                torchvision.ops.misc.Conv2d: [<span class="org-string">"weight"</span>],
            }
        <span class="org-keyword">else</span>:
            <span class="org-variable-name">sparse_parameter_list</span> = {
                torch.nn.Linear: [<span class="org-string">"weight"</span>],
                torch.nn.Conv1d: [<span class="org-string">"weight"</span>],
                torch.nn.Conv2d: [<span class="org-string">"weight"</span>],
                torch.nn.Conv3d: [<span class="org-string">"weight"</span>],
            }
        <span class="org-keyword">if</span> (
            custom_layer_dict
        ):  <span class="org-comment-delimiter"># </span><span class="org-comment">Update default list to include user supplied custom (layer type : parameter tensor), make sure this tensor type is something ASP knows how to prune</span>
            sparse_parameter_list.update(custom_layer_dict)
            <span class="org-variable-name">whitelist</span> += <span class="org-builtin">list</span>(custom_layer_dict.keys())

        <span class="org-keyword">for</span> module_type <span class="org-keyword">in</span> whitelist:
            <span class="org-keyword">assert</span> module_type <span class="org-keyword">in</span> sparse_parameter_list, (
                <span class="org-string">"Module %s :: Don't know how to sparsify module."</span> % module.dtype()
            )

        <span class="org-comment-delimiter"># </span><span class="org-comment">find all sparse modules, extract sparse parameters and decorate</span>
        <span class="org-keyword">def</span> <span class="org-function-name">add_sparse_attributes</span>(module_name, module):
            <span class="org-variable-name">sparse_parameters</span> = sparse_parameter_list[<span class="org-builtin">type</span>(module)]
            <span class="org-keyword">for</span> p_name, p <span class="org-keyword">in</span> module.named_parameters():
                <span class="org-keyword">if</span> p_name <span class="org-keyword">in</span> sparse_parameters <span class="org-keyword">and</span> p.requires_grad:
                    <span class="org-comment-delimiter"># </span><span class="org-comment">check for NVIDIA's TC compatibility: we check along the horizontal direction</span>
                    <span class="org-keyword">if</span> p.dtype == torch.float32 <span class="org-keyword">and</span> (
                        (p.size()[0] % 8) != 0 <span class="org-keyword">or</span> (p.size()[1] % 16) != 0
                    ):  <span class="org-comment-delimiter"># </span><span class="org-comment">User defines FP32 and APEX internally uses FP16 math</span>
                        <span class="org-builtin">print</span>(
                            <span class="org-string">"[ASP] Auto skipping pruning %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span class="org-builtin">str</span>(p.size()), <span class="org-builtin">str</span>(p.dtype))
                        )
                        <span class="org-keyword">continue</span>
                    <span class="org-keyword">if</span> p.dtype == torch.float16 <span class="org-keyword">and</span> (
                        (p.size()[0] % 8) != 0 <span class="org-keyword">or</span> (p.size()[1] % 16) != 0
                    ):  <span class="org-comment-delimiter"># </span><span class="org-comment">For Conv2d dim= K x CRS; we prune along C</span>
                        <span class="org-builtin">print</span>(
                            <span class="org-string">"[ASP] Auto skipping pruning %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span class="org-builtin">str</span>(p.size()), <span class="org-builtin">str</span>(p.dtype))
                        )
                        <span class="org-keyword">continue</span>

                    <span class="org-keyword">if</span> cls.__verbosity &gt;= 3:
                        <span class="org-builtin">print</span>(
                            <span class="org-string">"[ASP] Sparsifying %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span class="org-builtin">str</span>(p.size()), <span class="org-builtin">str</span>(p.dtype))
                        )

                    <span class="org-variable-name">mask</span> = torch.ones_like(p).<span class="org-builtin">bool</span>()
                    <span class="org-variable-name">buffname</span> = p_name.split(<span class="org-string">"."</span>)[-1]  <span class="org-comment-delimiter"># </span><span class="org-comment">buffer names cannot contain "."</span>
                    module.register_buffer(<span class="org-string">"__%s_mma_mask"</span> % buffname, mask)
                    <span class="org-keyword">if</span> allow_recompute_mask:
                        <span class="org-variable-name">pruned</span> = torch.zeros_like(p).cpu()
                        module.register_buffer(<span class="org-string">"__%s_mma_pruned_p"</span> % buffname, pruned)
                    <span class="org-keyword">else</span>:
                        <span class="org-variable-name">pruned</span> = <span class="org-constant">None</span>
                    cls.__sparse_parameters.append(
                        (module_name, module, p_name, p, mask, pruned)
                    )
                <span class="org-keyword">else</span>:
                    <span class="org-keyword">if</span> cls.__verbosity &gt;= 3:
                        <span class="org-builtin">print</span>(
                            <span class="org-string">"[ASP] Not sparsifying %s::%s of size=%s and type=%s"</span>
                            % (module_name, p_name, <span class="org-builtin">str</span>(p.size()), <span class="org-builtin">str</span>(p.dtype))
                        )

        <span class="org-keyword">for</span> name, sparse_module <span class="org-keyword">in</span> eligible_modules(
            model, <span class="org-builtin">tuple</span>(whitelist), allowed_layer_names, disallowed_layer_names
        ):
            add_sparse_attributes(name, sparse_module)

    @<span class="org-builtin">classmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">init_optimizer_for_pruning</span>(cls, optimizer):
        <span class="org-keyword">assert</span> cls.__optimizer <span class="org-keyword">is</span> <span class="org-constant">None</span>, <span class="org-string">"ASP has initialized optimizer already."</span>
        <span class="org-keyword">assert</span> (
            cls.__calculate_mask <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>
        ), <span class="org-string">"Called ASP.init_optimizer_for_pruning before ASP.init_model_for_pruning."</span>

        <span class="org-comment-delimiter"># </span><span class="org-comment">store pointer to original optimizer step method</span>
        cls.<span class="org-variable-name">__optimizer</span> = optimizer
        cls.__optimizer.<span class="org-variable-name">__step</span> = optimizer.step

        <span class="org-keyword">def</span> <span class="org-function-name">__step</span>(opt_self, *args, **kwargs):
            <span class="org-comment-delimiter"># </span><span class="org-comment">prune gradients before step method</span>
            <span class="org-keyword">with</span> torch.no_grad():
                <span class="org-keyword">for</span> (
                    module_name,
                    module,
                    p_name,
                    p,
                    mask,
                    pruned,
                ) <span class="org-keyword">in</span> cls.__sparse_parameters:
                    <span class="org-keyword">if</span> p.grad <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:  <span class="org-comment-delimiter"># </span><span class="org-comment">thx pjudd</span>
                        p.grad.mul_(mask)
            <span class="org-comment-delimiter"># </span><span class="org-comment">call original optimizer step method</span>
            <span class="org-variable-name">rval</span> = opt_self.__step(*args, **kwargs)
            <span class="org-comment-delimiter"># </span><span class="org-comment">prune parameters after step method</span>
            <span class="org-keyword">with</span> torch.no_grad():
                <span class="org-keyword">for</span> (
                    module_name,
                    module,
                    p_name,
                    p,
                    mask,
                    pruned,
                ) <span class="org-keyword">in</span> cls.__sparse_parameters:
                    p.mul_(mask)
            <span class="org-keyword">return</span> rval

        cls.__optimizer.<span class="org-variable-name">step</span> = types.MethodType(__step, cls.__optimizer)

    @<span class="org-builtin">classmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">compute_sparse_masks</span>(cls):
        <span class="org-keyword">with</span> torch.no_grad():
            <span class="org-keyword">for</span> module_name, module, p_name, p, mask, pruned <span class="org-keyword">in</span> cls.__sparse_parameters:
                <span class="org-keyword">if</span> mask.<span class="org-builtin">sum</span>() &lt; mask.numel():  <span class="org-comment-delimiter"># </span><span class="org-comment">when recalculating masks</span>
                    <span class="org-comment-delimiter"># </span><span class="org-comment">restore dense parameter if allow_recompute_mask is enabled</span>
                    <span class="org-keyword">assert</span> (
                        pruned <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>
                    ), <span class="org-string">"Unable to restore dense parameter because allow_recompute_mask == False"</span>
                    p.add_(pruned.cuda())

                mask.set_(cls.__calculate_mask(p))

                <span class="org-keyword">if</span> pruned <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:  <span class="org-comment-delimiter"># </span><span class="org-comment">stow away pruned weights to cpu</span>
                    pruned.set_((p * (~mask)).cpu())

                p.mul_(
                    mask
                )  <span class="org-comment-delimiter"># </span><span class="org-comment">in-place multiplication, so pruned weights are 0-values, hence checkpoint will have 0s for pruned weights</span>
                <span class="org-keyword">if</span> cls.__verbosity &gt;= 2:
                    <span class="org-builtin">print</span>(
                        <span class="org-string">"[ASP] Enabled %.2f%% sparsity for %s::%s of size=%s and type=%s"</span>
                        % (
                            100.0 * mask.<span class="org-builtin">sum</span>() / mask.numel(),
                            module_name,
                            p_name,
                            <span class="org-builtin">str</span>(p.size()),
                            <span class="org-builtin">str</span>(p.dtype),
                        )
                    )

    @<span class="org-builtin">classmethod</span>
    <span class="org-keyword">def</span> <span class="org-function-name">prune_trained_model</span>(cls, model, optimizer):
        <span class="org-comment-delimiter"># </span><span class="org-comment">add mask buffers to model (init_model_for_pruning), augment optimizer (init_optimizer_for_pruning) and compute masks (compute_sparse_masks)</span>
        cls.init_model_for_pruning(
            model,
            mask_calculator=<span class="org-string">"m4n2_1d"</span>,
            verbosity=2,
            whitelist=[torch.nn.Linear, torch.nn.Conv2d],
            allow_recompute_mask=<span class="org-constant">False</span>,
        )
        cls.init_optimizer_for_pruning(optimizer)
        cls.compute_sparse_masks()


<span class="org-comment-delimiter"># </span><span class="org-comment">-------------------- test.py --------------------</span>
<span class="org-keyword">import</span> os
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> torch
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> nn
<span class="org-keyword">from</span> torch <span class="org-keyword">import</span> optim
<span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> DataLoader, Dataset
<span class="org-keyword">import</span> torch.nn.functional <span class="org-keyword">as</span> F

<span class="org-variable-name">model</span> = <span class="org-constant">None</span>
<span class="org-variable-name">optimizer</span> = <span class="org-constant">None</span>


<span class="org-keyword">class</span> <span class="org-type">ToyDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-variable-name">x</span> = torch.<span class="org-builtin">round</span>(torch.rand(1000) * 200)
        <span class="org-variable-name">x</span> = x.unsqueeze(1)
        <span class="org-variable-name">x</span> = torch.cat((x, x * 2, x * 3, x * 4, x * 5, x * 6, x * 7, x * 8), 1)
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> = x
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> = <span class="org-keyword">self</span>.X

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)


<span class="org-variable-name">training_loader</span> = DataLoader(ToyDataset(), batch_size=100, shuffle=<span class="org-constant">True</span>)


<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-variable-name">criterion</span> = nn.MSELoss()
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(500):
        <span class="org-keyword">for</span> x, y <span class="org-keyword">in</span> training_loader:
            <span class="org-variable-name">loss</span> = criterion(model(x.to(<span class="org-string">"cuda"</span>)), y.to(<span class="org-string">"cuda"</span>))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    <span class="org-builtin">print</span>(<span class="org-string">"epoch #%d: loss: %f"</span> % (i, loss.item()))


<span class="org-keyword">def</span> <span class="org-function-name">test</span>():
    <span class="org-variable-name">x</span> = torch.tensor([[2, 4, 6, 8, 10, 12, 14, 16]]).<span class="org-builtin">float</span>()
    <span class="org-variable-name">y_hat</span> = model(x.to(<span class="org-string">"cuda"</span>))
    <span class="org-builtin">print</span>(<span class="org-string">"orig: "</span>, x, <span class="org-string">" new: "</span>, y_hat)


<span class="org-keyword">def</span> <span class="org-function-name">get_model</span>(f):
    <span class="org-keyword">global</span> model, optimizer
    <span class="org-keyword">if</span> os.path.exists(f):
        <span class="org-variable-name">model</span> = torch.load(f).cuda()
        <span class="org-variable-name">optimizer</span> = optim.Adam(model.parameters(), lr=0.01)
    <span class="org-keyword">else</span>:
        <span class="org-variable-name">model</span> = nn.Sequential(
            nn.Linear(8, 16),
            nn.PReLU(),
            nn.Linear(16, 8),
        ).cuda()
        <span class="org-variable-name">optimizer</span> = optim.Adam(model.parameters(), lr=0.01)
        train()
        torch.save(model, f)


get_model(<span class="org-string">"/tmp/model.pt"</span>)
<span class="org-builtin">print</span>(<span class="org-string">"-------orig---------"</span>)
test()

<span class="org-builtin">print</span>(model[2].state_dict())
ASP.prune_trained_model(model, optimizer)
<span class="org-builtin">print</span>(<span class="org-string">"-------pruned---------"</span>)
test()
<span class="org-builtin">print</span>(model[2].state_dict())
train()
<span class="org-builtin">print</span>(<span class="org-string">"-------retrain---------"</span>)
test()
<span class="org-builtin">print</span>(model[2].state_dict())
torch.save(model, <span class="org-string">"/tmp/model_sparse.pt"</span>)
</pre>
</div>

<p>
--&#x2013;&#x2014;orig----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.1899, -0.1074, -0.0064, -0.0016,  0.0893,  0.2194, -0.1457, -0.1500,
          0.0482,  0.0495, -0.1510,  0.0169, -0.0174,  0.0402,  0.1461, -0.1233],
        [-0.0973, -0.2051,  0.0303, -0.0798,  0.1052, -0.1524, -0.0244,  0.1359,
          0.0051,  0.0985, -0.1482,  0.1417, -0.0118,  0.1361,  0.2233, -0.1164],
        [ 0.1049, -0.1537, -0.1860,  0.1423, -0.1657, -0.0253, -0.0455,  0.1699,
          0.2134,  0.0081, -0.2659,  0.1806,  0.0515,  0.2417, -0.0409,  0.3283],
        [ 0.1426,  0.0729,  0.0950,  0.2379, -0.2145,  0.0646, -0.0936,  0.1097,
          0.0842, -0.2154, -0.0906, -0.0958,  0.0363,  0.2453,  0.1978,  0.3038],
        [ 0.2264, -0.0101,  0.3551, -0.3178,  0.2250, -0.0257,  0.0879, -0.3122,
         -0.1913, -0.0425, -0.0036,  0.1085,  0.1470,  0.0149,  0.0971,  0.3013],
        [ 0.3303,  0.0674, -0.1155, -0.1443, -0.0213, -0.0546,  0.0669, -0.2751,
         -0.0199,  0.0575, -0.2252,  0.3843, -0.1892,  0.4178, -0.0364,  0.0071],
        [ 0.3373, -0.0020,  0.2039, -0.0458,  0.2323, -0.3360,  0.0140, -0.1100,
         -0.1204, -0.0694, -0.0018,  0.1073,  0.2118,  0.3473,  0.0345, -0.0222],
        [ 0.0731, -0.3941,  0.1664,  0.0100,  0.1053, -0.4457,  0.2373, -0.0818,
         -0.0015, -0.0019, -0.4326,  0.0886, -0.2492,  0.2418,  0.2013,  0.0996]],
       device='cuda:0')), ('bias', tensor([0.0658, 0.0500, 0.1469, 0.0165, 0.1377, 0.1143, 0.0687, 0.1848],
       device='cuda:0'))])
[ASP] torchvision is imported, can work with the MaskRCNN/KeypointRCNN from torchvision.
[ASP] Auto skipping pruning 0::weight of size=torch.Size([16, 8]) and type=torch.float32 for sparsity
[ASP] Enabled 50.00% sparsity for 2::weight of size=torch.Size([8, 16]) and type=torch.float32
--&#x2013;&#x2014;pruned----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.1899, -0.1074, -0.0000, -0.0000,  0.0000,  0.2194, -0.0000, -0.1500,
          0.0000,  0.0495, -0.1510,  0.0000, -0.0000,  0.0000,  0.1461, -0.1233],
        [-0.0973, -0.2051,  0.0000, -0.0000,  0.0000, -0.1524, -0.0000,  0.1359,
          0.0000,  0.0000, -0.1482,  0.1417, -0.0000,  0.1361,  0.2233, -0.0000],
        [ 0.0000, -0.1537, -0.1860,  0.0000, -0.1657, -0.0000, -0.0000,  0.1699,
          0.2134,  0.0000, -0.2659,  0.0000,  0.0000,  0.2417, -0.0000,  0.3283],
        [ 0.1426,  0.0000,  0.0000,  0.2379, -0.2145,  0.0000, -0.0000,  0.1097,
          0.0000, -0.2154, -0.0000, -0.0958,  0.0000,  0.2453,  0.0000,  0.3038],
        [ 0.0000, -0.0000,  0.3551, -0.3178,  0.2250, -0.0000,  0.0000, -0.3122,
         -0.1913, -0.0000, -0.0000,  0.1085,  0.1470,  0.0000,  0.0000,  0.3013],
        [ 0.3303,  0.0000, -0.0000, -0.1443, -0.0000, -0.0000,  0.0669, -0.2751,
         -0.0000,  0.0000, -0.2252,  0.3843, -0.1892,  0.4178, -0.0000,  0.0000],
        [ 0.3373, -0.0000,  0.2039, -0.0000,  0.2323, -0.3360,  0.0000, -0.0000,
         -0.1204, -0.0000, -0.0000,  0.1073,  0.2118,  0.3473,  0.0000, -0.0000],
        [ 0.0000, -0.3941,  0.1664,  0.0000,  0.0000, -0.4457,  0.2373, -0.0000,
         -0.0000, -0.0000, -0.4326,  0.0886, -0.2492,  0.2418,  0.0000,  0.0000]],
       device='cuda:0')), ('bias', tensor([0.0658, 0.0500, 0.1469, 0.0165, 0.1377, 0.1143, 0.0687, 0.1848],
       device='cuda:0')), ('__weight_mma_mask', tensor([[ True,  True, False, False, False,  True, False,  True, False,  True,
          True, False, False, False,  True,  True],
        [ True,  True, False, False, False,  True, False,  True, False, False,
          True,  True, False,  True,  True, False],
        [False,  True,  True, False,  True, False, False,  True,  True, False,
          True, False, False,  True, False,  True],
        [ True, False, False,  True,  True, False, False,  True, False,  True,
         False,  True, False,  True, False,  True],
        [False, False,  True,  True,  True, False, False,  True,  True, False,
         False,  True,  True, False, False,  True],
        [ True, False, False,  True, False, False,  True,  True, False, False,
          True,  True,  True,  True, False, False],
        [ True, False,  True, False,  True,  True, False, False,  True, False,
         False,  True,  True,  True, False, False],
        [False,  True,  True, False, False,  True,  True, False, False, False,
          True,  True,  True,  True, False, False]], device='cuda:0'))])
epoch #499: loss: 0.610391
--&#x2013;&#x2014;retrain----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.2176, -0.1109, -0.0000, -0.0000,  0.0000,  0.1861, -0.0000, -0.1460,
          0.0000,  0.0276, -0.1884,  0.0000, -0.0000,  0.0000,  0.0870, -0.0764],
        [-0.1423, -0.1618,  0.0000, -0.0000,  0.0000, -0.1394, -0.0000,  0.1436,
          0.0000,  0.0000, -0.1384, -0.0720, -0.0000,  0.1603,  0.1100, -0.0000],
        [ 0.0000, -0.1045, -0.1682,  0.0000, -0.1040, -0.0000, -0.0000,  0.1698,
          0.1514,  0.0000, -0.2424,  0.0000,  0.0000,  0.2411, -0.0000,  0.2935],
        [ 0.1369,  0.0000,  0.0000,  0.2759, -0.1883,  0.0000, -0.0000,  0.1236,
          0.0000, -0.0164, -0.0000, -0.1456,  0.0000,  0.2672,  0.0000,  0.2691],
        [ 0.0000, -0.0000,  0.4154, -0.2569,  0.1915, -0.0000,  0.0000, -0.3162,
         -0.1192, -0.0000, -0.0000,  0.0208, -0.0131,  0.0000,  0.0000,  0.3485],
        [ 0.2522,  0.0000, -0.0000, -0.0961, -0.0000, -0.0000,  0.0340, -0.2356,
         -0.0000,  0.0000, -0.1943,  0.2541, -0.0421,  0.4076, -0.0000,  0.0000],
        [ 0.2798, -0.0000,  0.2241, -0.0000,  0.1726, -0.3089,  0.0000, -0.0000,
         -0.0214, -0.0000, -0.0000, -0.1063,  0.3034,  0.3635,  0.0000, -0.0000],
        [ 0.0000, -0.3632,  0.2074,  0.0000,  0.0000, -0.4494,  0.2390, -0.0000,
         -0.0000, -0.0000, -0.4418, -0.0210, -0.2601,  0.2938,  0.0000,  0.0000]],
       device='cuda:0')), ('bias', tensor([0.0612, 0.0340, 0.1160, 0.0579, 0.1724, 0.1567, 0.1405, 0.1688],
       device='cuda:0')), ('__weight_mma_mask', tensor([[ True,  True, False, False, False,  True, False,  True, False,  True,
          True, False, False, False,  True,  True],
        [ True,  True, False, False, False,  True, False,  True, False, False,
          True,  True, False,  True,  True, False],
        [False,  True,  True, False,  True, False, False,  True,  True, False,
          True, False, False,  True, False,  True],
        [ True, False, False,  True,  True, False, False,  True, False,  True,
         False,  True, False,  True, False,  True],
        [False, False,  True,  True,  True, False, False,  True,  True, False,
         False,  True,  True, False, False,  True],
        [ True, False, False,  True, False, False,  True,  True, False, False,
          True,  True,  True,  True, False, False],
        [ True, False,  True, False,  True,  True, False, False,  True, False,
         False,  True,  True,  True, False, False],
        [False,  True,  True, False, False,  True,  True, False, False, False,
          True,  True,  True,  True, False, False]], device='cuda:0'))])
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2021-10-26 Tue 00:00<br />
Last updated: 2022-07-17 Sun 11:39</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>
