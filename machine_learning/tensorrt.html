<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-25 Tue 15:41 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>TensorRT</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">TensorRT</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org440f3e9">1. TensorRT</a>
<ul>
<li><a href="#orgd98f78e">1.1. Quantization Aware Training</a>
<ul>
<li><a href="#org09cd3d0">1.1.1. Overview</a></li>
<li><a href="#orgf395158">1.1.2. Impl</a></li>
<li><a href="#org8bd64dc">1.1.3. TensorRT Optimizer</a></li>
</ul>
</li>
<li><a href="#org4613dd8">1.2. Sparsity</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org440f3e9" class="outline-2">
<h2 id="org440f3e9"><span class="section-number-2">1</span> TensorRT</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgd98f78e" class="outline-3">
<h3 id="orgd98f78e"><span class="section-number-3">1.1</span> Quantization Aware Training</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-org09cd3d0" class="outline-4">
<h4 id="org09cd3d0"><span class="section-number-4">1.1.1</span> Overview</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
<a href="https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/">https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/</a>
</p>

<p>
<a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization">https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization</a>
</p>

<p>
TensorRT 的 pytorch_quantization 是一个实现 fake quantization 的 pytorch plugin
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-10-29 14:24</span>
<span style="font-weight: bold;">import</span> os
<span style="font-weight: bold;">import</span> torch
<span style="font-weight: bold;">from</span> torch <span style="font-weight: bold;">import</span> nn
<span style="font-weight: bold;">from</span> torch <span style="font-weight: bold;">import</span> optim
<span style="font-weight: bold;">from</span> torch.utils.data <span style="font-weight: bold;">import</span> DataLoader, Dataset
<span style="font-weight: bold;">import</span> torch.nn.functional <span style="font-weight: bold;">as</span> F
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np

<span style="font-weight: bold;">import</span> torch.onnx

<span style="font-weight: bold;">from</span> pytorch_quantization <span style="font-weight: bold;">import</span> nn <span style="font-weight: bold;">as</span> quant_nn
<span style="font-weight: bold;">from</span> pytorch_quantization <span style="font-weight: bold;">import</span> calib
<span style="font-weight: bold;">from</span> pytorch_quantization.tensor_quant <span style="font-weight: bold;">import</span> QuantDescriptor
<span style="font-weight: bold;">from</span> pytorch_quantization <span style="font-weight: bold;">import</span> quant_modules
<span style="font-weight: bold;">from</span> absl <span style="font-weight: bold;">import</span> logging

logging.set_verbosity(logging.FATAL)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">export_to_onnx</span>(model):
    model.<span style="font-weight: bold;">eval</span>()
    <span style="font-weight: bold; font-style: italic;">dummy_input</span> = torch.randn(1, 5)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">use_fb_fake_quant &#26159; tensorrt &#30340;&#19968;&#20010; hack: &#24403;&#38656;&#35201;&#23548;&#20986;&#20026; onnx &#26102;&#35774;&#20026; true,</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">forward &#26102; fake_tensor_quant &#20250;&#34987;&#26367;&#25442;&#20026; _fb_fake_quant, &#21518;&#32773;&#20250;&#35843;&#29992;torch &#33258;</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&#24049;&#30340; fake_quantize_per_channel_affine &#20197;&#20415;&#23548;&#20986;&#20026; onnx &#30340;</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">QuantizeLinear/DequantizeLinear</span>
    <span style="font-weight: bold; font-style: italic;">quant_nn.TensorQuantizer.use_fb_fake_quant</span> = <span style="font-weight: bold; text-decoration: underline;">True</span>

    torch.onnx.export(
        model,
        dummy_input,
        <span style="font-style: italic;">"fake.onnx"</span>,
        opset_version=13,
    )
    <span style="font-weight: bold; font-style: italic;">quant_nn.TensorQuantizer.use_fb_fake_quant</span> = <span style="font-weight: bold; text-decoration: underline;">False</span>


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">get_data</span>():
    <span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">PlainDataset</span>(Dataset):
        <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>):
            <span style="font-weight: bold; font-style: italic;">x</span> = torch.<span style="font-weight: bold;">round</span>(torch.rand(10000) * 200)
            <span style="font-weight: bold; font-style: italic;">x</span> = x.unsqueeze(1)
            <span style="font-weight: bold; font-style: italic;">x</span> = torch.cat((x, x * 2, x * 3, x * 4, x * 5), 1)
            <span style="font-weight: bold;">self</span>.X = x

        <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__getitem__</span>(<span style="font-weight: bold;">self</span>, index):
            <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">self</span>.X[index]

        <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__len__</span>(<span style="font-weight: bold;">self</span>):
            <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">len</span>(<span style="font-weight: bold;">self</span>.X)

    <span style="font-weight: bold; font-style: italic;">training_set</span> = PlainDataset()
    <span style="font-weight: bold;">return</span> DataLoader(training_set, batch_size=100, shuffle=<span style="font-weight: bold; text-decoration: underline;">True</span>)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">pretrain_model</span>():
    <span style="font-weight: bold; font-style: italic;">model</span> = nn.Sequential(
        nn.Linear(5, 1),
        nn.Linear(1, 5),
    )

    <span style="font-weight: bold;">if</span> os.path.exists(<span style="font-style: italic;">"model.pt"</span>):
        <span style="font-weight: bold;">return</span>

    train_model(model)
    torch.save(model.state_dict(), <span style="font-style: italic;">"model.pt"</span>)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train_model</span>(model, epoch=500):
    <span style="font-weight: bold; font-style: italic;">criterion</span> = nn.MSELoss()
    <span style="font-weight: bold; font-style: italic;">optimizer</span> = optim.Adam(model.parameters())

    <span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(epoch):
        <span style="font-weight: bold;">for</span> x <span style="font-weight: bold;">in</span> get_data():
            <span style="font-weight: bold; font-style: italic;">loss</span> = criterion(model(x), x)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">if i % 10 == 0:</span>
        <span style="font-weight: bold; font-style: italic;">#     </span><span style="font-weight: bold; font-style: italic;">print("epoch #%d: loss: %f" % (i, loss.detach().item()))</span>


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">load_model</span>():
    <span style="font-weight: bold; font-style: italic;">quant_desc_input</span> = QuantDescriptor(calib_method=<span style="font-style: italic;">"histogram"</span>)
    quant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)
    quant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)

    quant_modules.initialize()

    <span style="font-weight: bold; font-style: italic;">model</span> = nn.Sequential(
        nn.Linear(5, 1),
        nn.Linear(1, 5),
    )
    model.load_state_dict(torch.load(<span style="font-style: italic;">"model.pt"</span>))
    <span style="font-weight: bold;">return</span> model


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">fake_quantize</span>(model):
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">collect_stats</span>(model, data, num_batches):
        <span style="font-weight: bold;">for</span> name, module <span style="font-weight: bold;">in</span> model.named_modules():
            <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(module, quant_nn.TensorQuantizer):
                module.disable_quant()
                module.enable_calib()

        <span style="font-weight: bold;">for</span> i, data <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">enumerate</span>(data):
            model(data)
            <span style="font-weight: bold;">if</span> i &gt;= num_batches:
                <span style="font-weight: bold;">break</span>
        <span style="font-weight: bold;">for</span> name, module <span style="font-weight: bold;">in</span> model.named_modules():
            <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(module, quant_nn.TensorQuantizer):
                module.enable_quant()
                module.disable_calib()

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">compute_amax</span>(model, **kwargs):
        <span style="font-weight: bold;">for</span> name, module <span style="font-weight: bold;">in</span> model.named_modules():
            <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(module, quant_nn.TensorQuantizer):
                <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(module._calibrator, calib.MaxCalibrator):
                    module.load_calib_amax()
                <span style="font-weight: bold;">else</span>:
                    module.load_calib_amax(method=<span style="font-style: italic;">"percentile"</span>)

    <span style="font-weight: bold;">with</span> torch.no_grad():
        collect_stats(model, get_data(), num_batches=1000)
        compute_amax(model)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">for name, module in model.named_modules():</span>
    <span style="font-weight: bold; font-style: italic;">#     </span><span style="font-weight: bold; font-style: italic;">if isinstance(module, quant_nn.TensorQuantizer):</span>
    <span style="font-weight: bold; font-style: italic;">#         </span><span style="font-weight: bold; font-style: italic;">print("------")</span>
    <span style="font-weight: bold; font-style: italic;">#         </span><span style="font-weight: bold; font-style: italic;">print(name, module)</span>

    <span style="font-weight: bold;">return</span> model


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test</span>(model):
    <span style="font-weight: bold; font-style: italic;">x</span> = torch.tensor([[10, 20, 30, 40, 50]]).<span style="font-weight: bold;">float</span>()
    <span style="font-weight: bold; font-style: italic;">y_hat</span> = model(x)
    <span style="font-weight: bold;">print</span>(y_hat)


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    pretrain_model()

    <span style="font-weight: bold; font-style: italic;">model</span> = load_model()
    test(model)

    fake_quantize(model)
    test(model)

    train_model(model, epoch=1)
    fake_quantize(model)
    test(model)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf395158" class="outline-4">
<h4 id="orgf395158"><span class="section-number-4">1.1.2</span> Impl</h4>
<div class="outline-text-4" id="text-1-1-2">
</div>
<div id="outline-container-org09c1650" class="outline-5">
<h5 id="org09c1650"><span class="section-number-5">1.1.2.1</span> monkey patching</h5>
<div class="outline-text-5" id="text-1-1-2-1">
<p>
quant_modules.initialize 会把 torch.nn 中的 Linear 等模块替换成 QuantLinear 等模
块
</p>
</div>
</div>

<div id="outline-container-org8c4896b" class="outline-5">
<h5 id="org8c4896b"><span class="section-number-5">1.1.2.2</span> foward</h5>
<div class="outline-text-5" id="text-1-1-2-2">
</div>
<ol class="org-ol">
<li><a id="orgd39e97b"></a>QuantLinear<br />
<div class="outline-text-6" id="text-1-1-2-2-1">
<p>
forward 会被 propagate 给 input_quantizer 和 weight_quantizer
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, <span style="font-weight: bold;">input</span>):
    <span style="font-weight: bold; font-style: italic;">quant_input</span> = <span style="font-weight: bold;">self</span>._input_quantizer(<span style="font-weight: bold;">input</span>)
    <span style="font-weight: bold; font-style: italic;">quant_weight</span> = <span style="font-weight: bold;">self</span>._weight_quantizer(<span style="font-weight: bold;">self</span>.weight)
    <span style="font-weight: bold; font-style: italic;">output</span> = F.linear(quant_input, quant_weight, bias=<span style="font-weight: bold;">self</span>.bias)
    <span style="font-weight: bold;">return</span> output
</pre>
</div>
</div>
</li>

<li><a id="org572fc77"></a>TensorQuantizer<br />
<div class="outline-text-6" id="text-1-1-2-2-2">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(<span style="font-weight: bold;">self</span>, inputs):
    <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">self</span>._disabled:
        <span style="font-weight: bold;">return</span> inputs

    <span style="font-weight: bold; font-style: italic;">outputs</span> = inputs

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&#35843;&#29992; TensorQuantizer.enable_calib() &#20250;&#35774;&#32622; _if_calib, &#34920;&#31034; calibrator &#38656;&#35201;</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&#35760;&#24405;&#20197;&#33719;&#24471;&#26368;&#22823;&#20540; amax (abs max)</span>
    <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">self</span>._if_calib:
        <span style="font-weight: bold;">self</span>._calibrator.collect(inputs)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&#35843;&#29992; enable_quant() &#20250;&#35774;&#32622; _if_quant, &#34920;&#31034;&#38656;&#35201;&#20351;&#29992; amax &#36827;&#34892; fake quant&#20351;</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">&#29992;&#26102;&#38656;&#35201;&#20808; enable calib, disable quant, calib &#23436;&#25104;&#21518;&#35745;&#31639; amax, &#28982;&#21518;&#20877;</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">enable quant, disable calib &#36827;&#34892;&#24102; fake quant &#30340; evaluation</span>
    <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">self</span>._if_quant:
        <span style="font-weight: bold; font-style: italic;">outputs</span> = <span style="font-weight: bold;">self</span>._quant_forward(inputs)

    <span style="font-weight: bold;">return</span> outputs


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">_quant_forward</span>(<span style="font-weight: bold;">self</span>, inputs):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">amax &#26102;&#36890;&#36807; load_calib_amax &#35745;&#31639;&#30340; amax</span>
    <span style="font-weight: bold; font-style: italic;">amax</span> = <span style="font-weight: bold;">self</span>._get_amax(inputs)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fake_tensor_quant &#26159;&#27880;&#20876;&#30340; autograd function</span>
    <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">not</span> TensorQuantizer.use_fb_fake_quant:
        <span style="font-weight: bold; font-style: italic;">outputs</span> = fake_tensor_quant(
            inputs, amax, <span style="font-weight: bold;">self</span>._num_bits, <span style="font-weight: bold;">self</span>._unsigned, <span style="font-weight: bold;">self</span>._narrow_range
        )
    <span style="font-weight: bold;">else</span>:
        <span style="font-weight: bold; font-style: italic;">outputs</span> = <span style="font-weight: bold;">self</span>._fb_fake_quant(inputs, amax)
    <span style="font-weight: bold;">return</span> outputs
</pre>
</div>
</div>
</li>

<li><a id="orgfe58d62"></a>fake_tensor_quant<br />
<div class="outline-text-6" id="text-1-1-2-2-3">
<p>
<a href="https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html">https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html</a>
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">FakeTensorQuantFunction</span>(Function):
    @<span style="font-weight: bold;">staticmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">forward</span>(ctx, inputs, amax, num_bits=8, unsigned=<span style="font-weight: bold; text-decoration: underline;">False</span>, narrow_range=<span style="font-weight: bold; text-decoration: underline;">True</span>):
        ctx.save_for_backward(inputs, amax)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fake quant &#21363; outputs=dequant(quant(inputs))</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">1. quant, &#35745;&#31639; scale &#24182;&#35745;&#31639; outputs=inputs*scale</span>
        <span style="font-weight: bold; font-style: italic;">outputs</span>, <span style="font-weight: bold; font-style: italic;">scale</span> = _tensor_quant(inputs, amax, num_bits, unsigned, narrow_range)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2. dequant, outputs=outputs/scale</span>
        <span style="font-weight: bold;">return</span> outputs / scale.to(inputs.dtype)

    @<span style="font-weight: bold;">staticmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">backward</span>(ctx, grad_outputs):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">...</span>

<span style="font-weight: bold; font-style: italic;">fake_tensor_quant</span> = FakeTensorQuantFunction.<span style="font-weight: bold;">apply</span>
</pre>
</div>
</div>
</li>

<li><a id="org4578a95"></a>_fb_fake_quant<br />
<div class="outline-text-6" id="text-1-1-2-2-4">
<p>
在导出成 onnx 时需要通过设置 use_fb_fake_quant = True 调用_fb_fake_quant, 后者会
调用 torch.fake_quantize_per_channel_affine,以便 export 时能导出成 onnx 标准的
QuantizeLinear / DequantizeLinear
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">_fb_fake_quant</span>(<span style="font-weight: bold;">self</span>, inputs, amax):
    <span style="font-weight: bold; font-style: italic;">bound</span> = (1 &lt;&lt; (<span style="font-weight: bold;">self</span>._num_bits - 1 + <span style="font-weight: bold;">int</span>(<span style="font-weight: bold;">self</span>._unsigned))) - 1
    <span style="font-weight: bold; font-style: italic;">outputs</span> = torch.fake_quantize_per_tensor_affine(
        inputs,
        amax.item() / bound,
        0,
        -bound - 1 <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold;">self</span>._unsigned <span style="font-weight: bold;">else</span> 0,
        bound,
    )
    <span style="font-weight: bold;">return</span> outputs
</pre>
</div>

<p>
导出成 onnx 的结果为:
</p>


<div id="orgb5338e0" class="figure">
<p><img src="../extra/trt_quant.png" alt="trt_quant.png" />
</p>
</div>
</div>
</li>
</ol>
</div>


<div id="outline-container-org649fd3e" class="outline-5">
<h5 id="org649fd3e"><span class="section-number-5">1.1.2.3</span> backward</h5>
<div class="outline-text-5" id="text-1-1-2-3">
</div>
<ol class="org-ol">
<li><a id="orgcc091ac"></a>FakeTensorQuantFunction<br />
<div class="outline-text-6" id="text-1-1-2-3-1">
<p>
Straight Through Estimation (STE) with clipping
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">def</span> <span style="font-weight: bold;">backward</span>(ctx, grad_outputs):
    <span style="font-weight: bold; font-style: italic;">inputs</span>, <span style="font-weight: bold; font-style: italic;">amax</span> = ctx.saved_tensors
    <span style="font-weight: bold; font-style: italic;">zero</span> = grad_outputs.new_zeros(1)
    <span style="font-weight: bold; font-style: italic;">grad_inputs</span> = torch.where(inputs.<span style="font-weight: bold;">abs</span>() &lt;= amax, grad_outputs, zero)
    <span style="font-weight: bold;">return</span> grad_inputs, <span style="font-weight: bold; text-decoration: underline;">None</span>, <span style="font-weight: bold; text-decoration: underline;">None</span>, <span style="font-weight: bold; text-decoration: underline;">None</span>, <span style="font-weight: bold; text-decoration: underline;">None</span>
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org8bd64dc" class="outline-4">
<h4 id="org8bd64dc"><span class="section-number-4">1.1.3</span> TensorRT Optimizer</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
<a href="https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31653/">https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31653/</a>
</p>

<p>
QuantLinear 把原来的 \(w*x+b\) 变成为 \(DQ(Q(w))*DQ(Q(x))+b\), 后续需要 tensorrt 的
optimizer 把 DQ 和 Q 前移或后移达到量化计算的目的, 例如:
</p>

<pre class="example" id="org3c176b3">
x -&gt; Q1 -&gt; DQ1 -&gt; [mul] -&gt;  y
                    ^
w -&gt; Q2 -&gt; DQ2 -----|
</pre>

<p>
可以优化为:
</p>

<pre class="example" id="orgbdfcc73">
x -&gt; Q1 -&gt; [mul] -&gt; [DQ1 -&gt; DQ2] -&gt; y
             ^
    [w-&gt;Q2] -|
</pre>
</div>
</div>
</div>

<div id="outline-container-org4613dd8" class="outline-3">
<h3 id="org4613dd8"><span class="section-number-3">1.2</span> Sparsity</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/">https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/</a>
</p>

<p>
<a href="https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity">https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity</a> 是一个做
prunning 的 pytorch 插件
</p>

<ol class="org-ol">
<li>它只会对 Linear, Conv2D 等的 weight 做 prunning,</li>

<li>它要求 weight 有特定的 shape, 比如对于 [x,y] 大小的 Linear Layer 需要 x%8==0,
y%16==0</li>

<li>在 pruning 时, 默认使用 m4n2_1d 的方式, 即在一维的方向上每 4 个数固定选择两个
绝对值最小的数进行 prunning</li>

<li>prunning 之后会在 torch 模型中针对每个被 prune 的参数记录一个 mask buffer, 这
个 buffer 有两个作用:

<ol class="org-ol">
<li>GPU 需要根据这个 buffer 进行 sparsity 操作</li>

<li>sparsity 工具 (ASP) 会对 pytorch 的 optimizer 进行 monkey patching, 修改过
的 optimizer 会利用这个 buffer 保证对 prunning 后的模型进行训练时会跳过已
经被 prune 的数据</li>
</ol></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-11-09 13:38</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-------------------- sparse_masklib.py --------------------</span>
<span style="font-weight: bold;">import</span> sys
<span style="font-weight: bold;">import</span> torch
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold;">import</span> collections
<span style="font-weight: bold;">from</span> itertools <span style="font-weight: bold;">import</span> permutations


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">fill</span>(x):
    <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">float</span>(x.nonzero().size(0)) / torch.numel(x)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">reshape_1d</span>(matrix, m):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">If not a nice multiple of m, fill with zeroes.</span>
    <span style="font-weight: bold;">if</span> matrix.shape[1] % m &gt; 0:
        <span style="font-weight: bold; font-style: italic;">mat</span> = torch.cuda.FloatTensor(
            matrix.shape[0], matrix.shape[1] + (m - matrix.shape[1] % m)
        ).fill_(0)
        mat[:, : matrix.shape[1]] = matrix
        <span style="font-weight: bold; font-style: italic;">shape</span> = mat.shape
        <span style="font-weight: bold;">return</span> mat.view(-1, m), shape
    <span style="font-weight: bold;">else</span>:
        <span style="font-weight: bold;">return</span> matrix.view(-1, m), matrix.shape


<span style="font-weight: bold; font-style: italic;">valid_m4n2_1d_patterns</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">compute_valid_1d_patterns</span>(m, n):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Early exit if patterns was already created.</span>
    <span style="font-weight: bold;">global</span> valid_m4n2_1d_patterns

    <span style="font-weight: bold;">if</span> m == 4 <span style="font-weight: bold;">and</span> n == 2 <span style="font-weight: bold;">and</span> valid_m4n2_1d_patterns <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span>:
        <span style="font-weight: bold;">return</span> valid_m4n2_1d_patterns
    <span style="font-weight: bold; font-style: italic;">patterns</span> = torch.zeros(m)
    <span style="font-weight: bold; font-style: italic;">patterns</span>[:n] = 1
    <span style="font-weight: bold; font-style: italic;">valid_patterns</span> = torch.Tensor(<span style="font-weight: bold;">list</span>(<span style="font-weight: bold;">set</span>(permutations(patterns.tolist()))))
    <span style="font-weight: bold;">if</span> m == 4 <span style="font-weight: bold;">and</span> n == 2:
        <span style="font-weight: bold; font-style: italic;">valid_m4n2_1d_patterns</span> = valid_patterns
    <span style="font-weight: bold;">return</span> valid_patterns


<span style="font-style: italic;">""" m:n 1d structured best """</span>


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">mn_1d_best</span>(matrix, m, n):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Find all possible patterns.</span>
    <span style="font-weight: bold; font-style: italic;">patterns</span> = compute_valid_1d_patterns(m, n).cuda()

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Find the best m:n pattern (sum of non-masked weights).</span>
    <span style="font-weight: bold; font-style: italic;">mask</span> = torch.cuda.IntTensor(matrix.shape).fill_(1).view(-1, m)
    <span style="font-weight: bold; font-style: italic;">mat</span>, <span style="font-weight: bold; font-style: italic;">shape</span> = reshape_1d(matrix, m)
    <span style="font-weight: bold; font-style: italic;">pmax</span> = torch.argmax(torch.matmul(mat.<span style="font-weight: bold;">abs</span>(), patterns.t()), dim=1)
    <span style="font-weight: bold; font-style: italic;">mask</span>[:] = patterns[pmax[:]]
    <span style="font-weight: bold; font-style: italic;">mask</span> = mask.view(matrix.shape)
    <span style="font-weight: bold;">return</span> mask


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">m4n2_1d</span>(mat, density):
    <span style="font-weight: bold;">return</span> mn_1d_best(mat, 4, 2)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">create_mask</span>(tensor, pattern=<span style="font-style: italic;">"m4n2_1d"</span>, density=0.5):
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Reshape tensor and mask.</span>
    <span style="font-weight: bold; font-style: italic;">shape</span> = tensor.shape
    <span style="font-weight: bold; font-style: italic;">ttype</span> = tensor.<span style="font-weight: bold;">type</span>()
    <span style="font-weight: bold; font-style: italic;">t</span> = tensor.<span style="font-weight: bold;">float</span>().contiguous()

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">1d-tensor</span>
    <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">len</span>(shape) == 1:
        <span style="font-weight: bold; font-style: italic;">t</span> = t.view(1, shape[0])
        <span style="font-weight: bold; font-style: italic;">func</span> = <span style="font-weight: bold;">getattr</span>(sys.modules[<span style="font-weight: bold;">__name__</span>], pattern, <span style="font-weight: bold; text-decoration: underline;">None</span>)
        <span style="font-weight: bold; font-style: italic;">mask</span> = func(t, density)
        <span style="font-weight: bold;">return</span> mask.view(shape).<span style="font-weight: bold;">type</span>(ttype)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2d-tensor (in, out)</span>
    <span style="font-weight: bold;">elif</span> <span style="font-weight: bold;">len</span>(shape) == 2:
        <span style="font-weight: bold; font-style: italic;">t</span> = t.view(shape[0], shape[1])
        <span style="font-weight: bold; font-style: italic;">func</span> = <span style="font-weight: bold;">getattr</span>(sys.modules[<span style="font-weight: bold;">__name__</span>], pattern, <span style="font-weight: bold; text-decoration: underline;">None</span>)
        <span style="font-weight: bold; font-style: italic;">mask</span> = func(t, density)
        <span style="font-weight: bold;">return</span> mask.view(shape).<span style="font-weight: bold;">type</span>(ttype)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">3d-tensor (batch, in, out)</span>
    <span style="font-weight: bold;">elif</span> <span style="font-weight: bold;">len</span>(shape) == 3:
        <span style="font-weight: bold; font-style: italic;">t</span> = t.view(shape[0] * shape[1], shape[2])
        <span style="font-weight: bold; font-style: italic;">func</span> = <span style="font-weight: bold;">getattr</span>(sys.modules[<span style="font-weight: bold;">__name__</span>], pattern, <span style="font-weight: bold; text-decoration: underline;">None</span>)
        <span style="font-weight: bold; font-style: italic;">mask</span> = func(t, density)
        <span style="font-weight: bold;">return</span> mask.view(shape).<span style="font-weight: bold;">type</span>(ttype)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">4d-tensor (in, out, h, w)</span>
    <span style="font-weight: bold;">elif</span> <span style="font-weight: bold;">len</span>(shape) == 4:
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">convs</span>
        <span style="font-weight: bold; font-style: italic;">t</span> = (
            t.permute(2, 3, 0, 1)
            .contiguous()
            .view(shape[2] * shape[3] * shape[0], shape[1])
        )
        <span style="font-weight: bold; font-style: italic;">func</span> = <span style="font-weight: bold;">getattr</span>(sys.modules[<span style="font-weight: bold;">__name__</span>], pattern, <span style="font-weight: bold; text-decoration: underline;">None</span>)
        <span style="font-weight: bold; font-style: italic;">mask</span> = func(t, density)
        <span style="font-weight: bold; font-style: italic;">mask</span> = (
            mask.view(shape[2], shape[3], shape[0], shape[1])
            .permute(2, 3, 0, 1)
            .contiguous()
        )
        <span style="font-weight: bold;">return</span> mask.view(shape).<span style="font-weight: bold;">type</span>(ttype)


<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-------------------- asp.py --------------------</span>
<span style="font-weight: bold;">import</span> types
<span style="font-weight: bold;">import</span> torch

<span style="font-weight: bold; font-style: italic;">torchvision_imported</span> = <span style="font-weight: bold; text-decoration: underline;">True</span>
<span style="font-weight: bold;">try</span>:
    <span style="font-weight: bold;">import</span> torchvision
<span style="font-weight: bold;">except</span> <span style="font-weight: bold; text-decoration: underline;">ImportError</span>:
    <span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"[ASP][Warning] torchvision cannot be imported."</span>)
    <span style="font-weight: bold; font-style: italic;">torchvision_imported</span> = <span style="font-weight: bold; text-decoration: underline;">False</span>


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">eligible_modules</span>(
    model, whitelist_layer_types, allowed_layer_names, disallowed_layer_names
):
    <span style="font-weight: bold; font-style: italic;">eligible_modules_list</span> = []
    <span style="font-weight: bold;">for</span> name, mod <span style="font-weight: bold;">in</span> model.named_modules():
        <span style="font-weight: bold;">if</span> (
            <span style="font-weight: bold;">isinstance</span>(mod, whitelist_layer_types)
            <span style="font-weight: bold;">and</span> name <span style="font-weight: bold;">not</span> <span style="font-weight: bold;">in</span> disallowed_layer_names
        ):
            <span style="font-weight: bold;">if</span> allowed_layer_names <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span> <span style="font-weight: bold;">and</span> name <span style="font-weight: bold;">not</span> <span style="font-weight: bold;">in</span> allowed_layer_names:
                <span style="font-weight: bold;">continue</span>
            eligible_modules_list.append((name, mod))
    <span style="font-weight: bold;">return</span> eligible_modules_list


<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">ASP</span>:
    <span style="font-weight: bold; font-style: italic;">__model</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>
    <span style="font-weight: bold; font-style: italic;">__verbosity</span> = 0
    <span style="font-weight: bold; font-style: italic;">__optimizer</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>
    <span style="font-weight: bold; font-style: italic;">__sparse_parameters</span> = []
    <span style="font-weight: bold; font-style: italic;">__calculate_mask</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>

    @<span style="font-weight: bold;">classmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">init_model_for_pruning</span>(
        cls,
        model,
        mask_calculator=<span style="font-style: italic;">"m4n2_1d"</span>,
        verbosity=3,
        whitelist=[torch.nn.Linear, torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Conv3d],
        allowed_layer_names=<span style="font-weight: bold; text-decoration: underline;">None</span>,
        disallowed_layer_names=[],
        allow_recompute_mask=<span style="font-weight: bold; text-decoration: underline;">False</span>,
        custom_layer_dict={},
    ):
        <span style="font-weight: bold;">assert</span> cls.__model <span style="font-weight: bold;">is</span> <span style="font-weight: bold; text-decoration: underline;">None</span>, <span style="font-style: italic;">"ASP has been initialized already."</span>
        <span style="font-weight: bold; font-style: italic;">cls.__model</span> = model
        <span style="font-weight: bold; font-style: italic;">cls.__verbosity</span> = verbosity

        <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(mask_calculator, <span style="font-weight: bold;">str</span>):

            <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">create_mask_from_pattern</span>(param):
                <span style="font-weight: bold;">return</span> create_mask(param, mask_calculator).<span style="font-weight: bold;">bool</span>()

            <span style="font-weight: bold; font-style: italic;">cls.__calculate_mask</span> = create_mask_from_pattern
        <span style="font-weight: bold;">else</span>:
            <span style="font-weight: bold; font-style: italic;">cls.__calculate_mask</span> = mask_calculator  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">user defined function</span>

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">function to extract variables that will be sparsified.</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">idea is that you will add one of these functions for each module type that can be sparsified.</span>
        <span style="font-weight: bold;">if</span> torchvision_imported:
            <span style="font-weight: bold;">print</span>(
                <span style="font-style: italic;">"[ASP] torchvision is imported, can work with the MaskRCNN/KeypointRCNN from torchvision."</span>
            )
            <span style="font-weight: bold; font-style: italic;">sparse_parameter_list</span> = {
                torch.nn.Linear: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv1d: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv2d: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv3d: [<span style="font-style: italic;">"weight"</span>],
                torchvision.ops.misc.Conv2d: [<span style="font-style: italic;">"weight"</span>],
            }
        <span style="font-weight: bold;">else</span>:
            <span style="font-weight: bold; font-style: italic;">sparse_parameter_list</span> = {
                torch.nn.Linear: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv1d: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv2d: [<span style="font-style: italic;">"weight"</span>],
                torch.nn.Conv3d: [<span style="font-style: italic;">"weight"</span>],
            }
        <span style="font-weight: bold;">if</span> (
            custom_layer_dict
        ):  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">Update default list to include user supplied custom (layer type : parameter tensor), make sure this tensor type is something ASP knows how to prune</span>
            sparse_parameter_list.update(custom_layer_dict)
            <span style="font-weight: bold; font-style: italic;">whitelist</span> += <span style="font-weight: bold;">list</span>(custom_layer_dict.keys())

        <span style="font-weight: bold;">for</span> module_type <span style="font-weight: bold;">in</span> whitelist:
            <span style="font-weight: bold;">assert</span> module_type <span style="font-weight: bold;">in</span> sparse_parameter_list, (
                <span style="font-style: italic;">"Module %s :: Don't know how to sparsify module."</span> % module.dtype()
            )

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">find all sparse modules, extract sparse parameters and decorate</span>
        <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">add_sparse_attributes</span>(module_name, module):
            <span style="font-weight: bold; font-style: italic;">sparse_parameters</span> = sparse_parameter_list[<span style="font-weight: bold;">type</span>(module)]
            <span style="font-weight: bold;">for</span> p_name, p <span style="font-weight: bold;">in</span> module.named_parameters():
                <span style="font-weight: bold;">if</span> p_name <span style="font-weight: bold;">in</span> sparse_parameters <span style="font-weight: bold;">and</span> p.requires_grad:
                    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">check for NVIDIA's TC compatibility: we check along the horizontal direction</span>
                    <span style="font-weight: bold;">if</span> p.dtype == torch.float32 <span style="font-weight: bold;">and</span> (
                        (p.size()[0] % 8) != 0 <span style="font-weight: bold;">or</span> (p.size()[1] % 16) != 0
                    ):  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">User defines FP32 and APEX internally uses FP16 math</span>
                        <span style="font-weight: bold;">print</span>(
                            <span style="font-style: italic;">"[ASP] Auto skipping pruning %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span style="font-weight: bold;">str</span>(p.size()), <span style="font-weight: bold;">str</span>(p.dtype))
                        )
                        <span style="font-weight: bold;">continue</span>
                    <span style="font-weight: bold;">if</span> p.dtype == torch.float16 <span style="font-weight: bold;">and</span> (
                        (p.size()[0] % 8) != 0 <span style="font-weight: bold;">or</span> (p.size()[1] % 16) != 0
                    ):  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">For Conv2d dim= K x CRS; we prune along C</span>
                        <span style="font-weight: bold;">print</span>(
                            <span style="font-style: italic;">"[ASP] Auto skipping pruning %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span style="font-weight: bold;">str</span>(p.size()), <span style="font-weight: bold;">str</span>(p.dtype))
                        )
                        <span style="font-weight: bold;">continue</span>

                    <span style="font-weight: bold;">if</span> cls.__verbosity &gt;= 3:
                        <span style="font-weight: bold;">print</span>(
                            <span style="font-style: italic;">"[ASP] Sparsifying %s::%s of size=%s and type=%s for sparsity"</span>
                            % (module_name, p_name, <span style="font-weight: bold;">str</span>(p.size()), <span style="font-weight: bold;">str</span>(p.dtype))
                        )

                    <span style="font-weight: bold; font-style: italic;">mask</span> = torch.ones_like(p).<span style="font-weight: bold;">bool</span>()
                    <span style="font-weight: bold; font-style: italic;">buffname</span> = p_name.split(<span style="font-style: italic;">"."</span>)[-1]  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">buffer names cannot contain "."</span>
                    module.register_buffer(<span style="font-style: italic;">"__%s_mma_mask"</span> % buffname, mask)
                    <span style="font-weight: bold;">if</span> allow_recompute_mask:
                        <span style="font-weight: bold; font-style: italic;">pruned</span> = torch.zeros_like(p).cpu()
                        module.register_buffer(<span style="font-style: italic;">"__%s_mma_pruned_p"</span> % buffname, pruned)
                    <span style="font-weight: bold;">else</span>:
                        <span style="font-weight: bold; font-style: italic;">pruned</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>
                    cls.__sparse_parameters.append(
                        (module_name, module, p_name, p, mask, pruned)
                    )
                <span style="font-weight: bold;">else</span>:
                    <span style="font-weight: bold;">if</span> cls.__verbosity &gt;= 3:
                        <span style="font-weight: bold;">print</span>(
                            <span style="font-style: italic;">"[ASP] Not sparsifying %s::%s of size=%s and type=%s"</span>
                            % (module_name, p_name, <span style="font-weight: bold;">str</span>(p.size()), <span style="font-weight: bold;">str</span>(p.dtype))
                        )

        <span style="font-weight: bold;">for</span> name, sparse_module <span style="font-weight: bold;">in</span> eligible_modules(
            model, <span style="font-weight: bold;">tuple</span>(whitelist), allowed_layer_names, disallowed_layer_names
        ):
            add_sparse_attributes(name, sparse_module)

    @<span style="font-weight: bold;">classmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">init_optimizer_for_pruning</span>(cls, optimizer):
        <span style="font-weight: bold;">assert</span> cls.__optimizer <span style="font-weight: bold;">is</span> <span style="font-weight: bold; text-decoration: underline;">None</span>, <span style="font-style: italic;">"ASP has initialized optimizer already."</span>
        <span style="font-weight: bold;">assert</span> (
            cls.__calculate_mask <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span>
        ), <span style="font-style: italic;">"Called ASP.init_optimizer_for_pruning before ASP.init_model_for_pruning."</span>

        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">store pointer to original optimizer step method</span>
        <span style="font-weight: bold; font-style: italic;">cls.__optimizer</span> = optimizer
        <span style="font-weight: bold; font-style: italic;">cls.__optimizer.__step</span> = optimizer.step

        <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__step</span>(opt_self, *args, **kwargs):
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">prune gradients before step method</span>
            <span style="font-weight: bold;">with</span> torch.no_grad():
                <span style="font-weight: bold;">for</span> (
                    module_name,
                    module,
                    p_name,
                    p,
                    mask,
                    pruned,
                ) <span style="font-weight: bold;">in</span> cls.__sparse_parameters:
                    <span style="font-weight: bold;">if</span> p.grad <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span>:  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">thx pjudd</span>
                        p.grad.mul_(mask)
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">call original optimizer step method</span>
            <span style="font-weight: bold; font-style: italic;">rval</span> = opt_self.__step(*args, **kwargs)
            <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">prune parameters after step method</span>
            <span style="font-weight: bold;">with</span> torch.no_grad():
                <span style="font-weight: bold;">for</span> (
                    module_name,
                    module,
                    p_name,
                    p,
                    mask,
                    pruned,
                ) <span style="font-weight: bold;">in</span> cls.__sparse_parameters:
                    p.mul_(mask)
            <span style="font-weight: bold;">return</span> rval

        <span style="font-weight: bold; font-style: italic;">cls.__optimizer.step</span> = types.MethodType(__step, cls.__optimizer)

    @<span style="font-weight: bold;">classmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">compute_sparse_masks</span>(cls):
        <span style="font-weight: bold;">with</span> torch.no_grad():
            <span style="font-weight: bold;">for</span> module_name, module, p_name, p, mask, pruned <span style="font-weight: bold;">in</span> cls.__sparse_parameters:
                <span style="font-weight: bold;">if</span> mask.<span style="font-weight: bold;">sum</span>() &lt; mask.numel():  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">when recalculating masks</span>
                    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">restore dense parameter if allow_recompute_mask is enabled</span>
                    <span style="font-weight: bold;">assert</span> (
                        pruned <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span>
                    ), <span style="font-style: italic;">"Unable to restore dense parameter because allow_recompute_mask == False"</span>
                    p.add_(pruned.cuda())

                mask.set_(cls.__calculate_mask(p))

                <span style="font-weight: bold;">if</span> pruned <span style="font-weight: bold;">is</span> <span style="font-weight: bold;">not</span> <span style="font-weight: bold; text-decoration: underline;">None</span>:  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">stow away pruned weights to cpu</span>
                    pruned.set_((p * (~mask)).cpu())

                p.mul_(
                    mask
                )  <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">in-place multiplication, so pruned weights are 0-values, hence checkpoint will have 0s for pruned weights</span>
                <span style="font-weight: bold;">if</span> cls.__verbosity &gt;= 2:
                    <span style="font-weight: bold;">print</span>(
                        <span style="font-style: italic;">"[ASP] Enabled %.2f%% sparsity for %s::%s of size=%s and type=%s"</span>
                        % (
                            100.0 * mask.<span style="font-weight: bold;">sum</span>() / mask.numel(),
                            module_name,
                            p_name,
                            <span style="font-weight: bold;">str</span>(p.size()),
                            <span style="font-weight: bold;">str</span>(p.dtype),
                        )
                    )

    @<span style="font-weight: bold;">classmethod</span>
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">prune_trained_model</span>(cls, model, optimizer):
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">add mask buffers to model (init_model_for_pruning), augment optimizer (init_optimizer_for_pruning) and compute masks (compute_sparse_masks)</span>
        cls.init_model_for_pruning(
            model,
            mask_calculator=<span style="font-style: italic;">"m4n2_1d"</span>,
            verbosity=2,
            whitelist=[torch.nn.Linear, torch.nn.Conv2d],
            allow_recompute_mask=<span style="font-weight: bold; text-decoration: underline;">False</span>,
        )
        cls.init_optimizer_for_pruning(optimizer)
        cls.compute_sparse_masks()


<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-------------------- test.py --------------------</span>
<span style="font-weight: bold;">import</span> os
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold;">import</span> torch
<span style="font-weight: bold;">from</span> torch <span style="font-weight: bold;">import</span> nn
<span style="font-weight: bold;">from</span> torch <span style="font-weight: bold;">import</span> optim
<span style="font-weight: bold;">from</span> torch.utils.data <span style="font-weight: bold;">import</span> DataLoader, Dataset
<span style="font-weight: bold;">import</span> torch.nn.functional <span style="font-weight: bold;">as</span> F

<span style="font-weight: bold; font-style: italic;">model</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>
<span style="font-weight: bold; font-style: italic;">optimizer</span> = <span style="font-weight: bold; text-decoration: underline;">None</span>


<span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">ToyDataset</span>(Dataset):
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__init__</span>(<span style="font-weight: bold;">self</span>):
        <span style="font-weight: bold; font-style: italic;">x</span> = torch.<span style="font-weight: bold;">round</span>(torch.rand(1000) * 200)
        <span style="font-weight: bold; font-style: italic;">x</span> = x.unsqueeze(1)
        <span style="font-weight: bold; font-style: italic;">x</span> = torch.cat((x, x * 2, x * 3, x * 4, x * 5, x * 6, x * 7, x * 8), 1)
        <span style="font-weight: bold;">self</span>.X = x
        <span style="font-weight: bold;">self</span>.Y = <span style="font-weight: bold;">self</span>.X

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__getitem__</span>(<span style="font-weight: bold;">self</span>, index):
        <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">self</span>.X[index], <span style="font-weight: bold;">self</span>.Y[index]

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">__len__</span>(<span style="font-weight: bold;">self</span>):
        <span style="font-weight: bold;">return</span> <span style="font-weight: bold;">len</span>(<span style="font-weight: bold;">self</span>.X)


<span style="font-weight: bold; font-style: italic;">training_loader</span> = DataLoader(ToyDataset(), batch_size=100, shuffle=<span style="font-weight: bold; text-decoration: underline;">True</span>)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">train</span>():
    <span style="font-weight: bold; font-style: italic;">criterion</span> = nn.MSELoss()
    <span style="font-weight: bold;">for</span> i <span style="font-weight: bold;">in</span> <span style="font-weight: bold;">range</span>(500):
        <span style="font-weight: bold;">for</span> x, y <span style="font-weight: bold;">in</span> training_loader:
            <span style="font-weight: bold; font-style: italic;">loss</span> = criterion(model(x.to(<span style="font-style: italic;">"cuda"</span>)), y.to(<span style="font-style: italic;">"cuda"</span>))
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    <span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"epoch #%d: loss: %f"</span> % (i, loss.item()))


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test</span>():
    <span style="font-weight: bold; font-style: italic;">x</span> = torch.tensor([[2, 4, 6, 8, 10, 12, 14, 16]]).<span style="font-weight: bold;">float</span>()
    <span style="font-weight: bold; font-style: italic;">y_hat</span> = model(x.to(<span style="font-style: italic;">"cuda"</span>))
    <span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"orig: "</span>, x, <span style="font-style: italic;">" new: "</span>, y_hat)


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">get_model</span>(f):
    <span style="font-weight: bold;">global</span> model, optimizer
    <span style="font-weight: bold;">if</span> os.path.exists(f):
        <span style="font-weight: bold; font-style: italic;">model</span> = torch.load(f).cuda()
        <span style="font-weight: bold; font-style: italic;">optimizer</span> = optim.Adam(model.parameters(), lr=0.01)
    <span style="font-weight: bold;">else</span>:
        <span style="font-weight: bold; font-style: italic;">model</span> = nn.Sequential(
            nn.Linear(8, 16),
            nn.PReLU(),
            nn.Linear(16, 8),
        ).cuda()
        <span style="font-weight: bold; font-style: italic;">optimizer</span> = optim.Adam(model.parameters(), lr=0.01)
        train()
        torch.save(model, f)


get_model(<span style="font-style: italic;">"/tmp/model.pt"</span>)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"-------orig---------"</span>)
test()

<span style="font-weight: bold;">print</span>(model[2].state_dict())
ASP.prune_trained_model(model, optimizer)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"-------pruned---------"</span>)
test()
<span style="font-weight: bold;">print</span>(model[2].state_dict())
train()
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"-------retrain---------"</span>)
test()
<span style="font-weight: bold;">print</span>(model[2].state_dict())
torch.save(model, <span style="font-style: italic;">"/tmp/model_sparse.pt"</span>)
</pre>
</div>

<p>
--&#x2013;&#x2014;orig----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.1899, -0.1074, -0.0064, -0.0016,  0.0893,  0.2194, -0.1457, -0.1500,
          0.0482,  0.0495, -0.1510,  0.0169, -0.0174,  0.0402,  0.1461, -0.1233],
        [-0.0973, -0.2051,  0.0303, -0.0798,  0.1052, -0.1524, -0.0244,  0.1359,
          0.0051,  0.0985, -0.1482,  0.1417, -0.0118,  0.1361,  0.2233, -0.1164],
        [ 0.1049, -0.1537, -0.1860,  0.1423, -0.1657, -0.0253, -0.0455,  0.1699,
          0.2134,  0.0081, -0.2659,  0.1806,  0.0515,  0.2417, -0.0409,  0.3283],
        [ 0.1426,  0.0729,  0.0950,  0.2379, -0.2145,  0.0646, -0.0936,  0.1097,
          0.0842, -0.2154, -0.0906, -0.0958,  0.0363,  0.2453,  0.1978,  0.3038],
        [ 0.2264, -0.0101,  0.3551, -0.3178,  0.2250, -0.0257,  0.0879, -0.3122,
         -0.1913, -0.0425, -0.0036,  0.1085,  0.1470,  0.0149,  0.0971,  0.3013],
        [ 0.3303,  0.0674, -0.1155, -0.1443, -0.0213, -0.0546,  0.0669, -0.2751,
         -0.0199,  0.0575, -0.2252,  0.3843, -0.1892,  0.4178, -0.0364,  0.0071],
        [ 0.3373, -0.0020,  0.2039, -0.0458,  0.2323, -0.3360,  0.0140, -0.1100,
         -0.1204, -0.0694, -0.0018,  0.1073,  0.2118,  0.3473,  0.0345, -0.0222],
        [ 0.0731, -0.3941,  0.1664,  0.0100,  0.1053, -0.4457,  0.2373, -0.0818,
         -0.0015, -0.0019, -0.4326,  0.0886, -0.2492,  0.2418,  0.2013,  0.0996]],
       device='cuda:0')), ('bias', tensor([0.0658, 0.0500, 0.1469, 0.0165, 0.1377, 0.1143, 0.0687, 0.1848],
       device='cuda:0'))])
[ASP] torchvision is imported, can work with the MaskRCNN/KeypointRCNN from torchvision.
[ASP] Auto skipping pruning 0::weight of size=torch.Size([16, 8]) and type=torch.float32 for sparsity
[ASP] Enabled 50.00% sparsity for 2::weight of size=torch.Size([8, 16]) and type=torch.float32
--&#x2013;&#x2014;pruned----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.1899, -0.1074, -0.0000, -0.0000,  0.0000,  0.2194, -0.0000, -0.1500,
          0.0000,  0.0495, -0.1510,  0.0000, -0.0000,  0.0000,  0.1461, -0.1233],
        [-0.0973, -0.2051,  0.0000, -0.0000,  0.0000, -0.1524, -0.0000,  0.1359,
          0.0000,  0.0000, -0.1482,  0.1417, -0.0000,  0.1361,  0.2233, -0.0000],
        [ 0.0000, -0.1537, -0.1860,  0.0000, -0.1657, -0.0000, -0.0000,  0.1699,
          0.2134,  0.0000, -0.2659,  0.0000,  0.0000,  0.2417, -0.0000,  0.3283],
        [ 0.1426,  0.0000,  0.0000,  0.2379, -0.2145,  0.0000, -0.0000,  0.1097,
          0.0000, -0.2154, -0.0000, -0.0958,  0.0000,  0.2453,  0.0000,  0.3038],
        [ 0.0000, -0.0000,  0.3551, -0.3178,  0.2250, -0.0000,  0.0000, -0.3122,
         -0.1913, -0.0000, -0.0000,  0.1085,  0.1470,  0.0000,  0.0000,  0.3013],
        [ 0.3303,  0.0000, -0.0000, -0.1443, -0.0000, -0.0000,  0.0669, -0.2751,
         -0.0000,  0.0000, -0.2252,  0.3843, -0.1892,  0.4178, -0.0000,  0.0000],
        [ 0.3373, -0.0000,  0.2039, -0.0000,  0.2323, -0.3360,  0.0000, -0.0000,
         -0.1204, -0.0000, -0.0000,  0.1073,  0.2118,  0.3473,  0.0000, -0.0000],
        [ 0.0000, -0.3941,  0.1664,  0.0000,  0.0000, -0.4457,  0.2373, -0.0000,
         -0.0000, -0.0000, -0.4326,  0.0886, -0.2492,  0.2418,  0.0000,  0.0000]],
       device='cuda:0')), ('bias', tensor([0.0658, 0.0500, 0.1469, 0.0165, 0.1377, 0.1143, 0.0687, 0.1848],
       device='cuda:0')), ('__weight_mma_mask', tensor([[ True,  True, False, False, False,  True, False,  True, False,  True,
          True, False, False, False,  True,  True],
        [ True,  True, False, False, False,  True, False,  True, False, False,
          True,  True, False,  True,  True, False],
        [False,  True,  True, False,  True, False, False,  True,  True, False,
          True, False, False,  True, False,  True],
        [ True, False, False,  True,  True, False, False,  True, False,  True,
         False,  True, False,  True, False,  True],
        [False, False,  True,  True,  True, False, False,  True,  True, False,
         False,  True,  True, False, False,  True],
        [ True, False, False,  True, False, False,  True,  True, False, False,
          True,  True,  True,  True, False, False],
        [ True, False,  True, False,  True,  True, False, False,  True, False,
         False,  True,  True,  True, False, False],
        [False,  True,  True, False, False,  True,  True, False, False, False,
          True,  True,  True,  True, False, False]], device='cuda:0'))])
epoch #499: loss: 0.610391
--&#x2013;&#x2014;retrain----&#x2013;&#x2014;
orig:  tensor()  new:  tensor(,
       device='cuda:0', grad_fn=&lt;AddmmBackward&gt;)
OrderedDict([('weight', tensor([[ 0.2176, -0.1109, -0.0000, -0.0000,  0.0000,  0.1861, -0.0000, -0.1460,
          0.0000,  0.0276, -0.1884,  0.0000, -0.0000,  0.0000,  0.0870, -0.0764],
        [-0.1423, -0.1618,  0.0000, -0.0000,  0.0000, -0.1394, -0.0000,  0.1436,
          0.0000,  0.0000, -0.1384, -0.0720, -0.0000,  0.1603,  0.1100, -0.0000],
        [ 0.0000, -0.1045, -0.1682,  0.0000, -0.1040, -0.0000, -0.0000,  0.1698,
          0.1514,  0.0000, -0.2424,  0.0000,  0.0000,  0.2411, -0.0000,  0.2935],
        [ 0.1369,  0.0000,  0.0000,  0.2759, -0.1883,  0.0000, -0.0000,  0.1236,
          0.0000, -0.0164, -0.0000, -0.1456,  0.0000,  0.2672,  0.0000,  0.2691],
        [ 0.0000, -0.0000,  0.4154, -0.2569,  0.1915, -0.0000,  0.0000, -0.3162,
         -0.1192, -0.0000, -0.0000,  0.0208, -0.0131,  0.0000,  0.0000,  0.3485],
        [ 0.2522,  0.0000, -0.0000, -0.0961, -0.0000, -0.0000,  0.0340, -0.2356,
         -0.0000,  0.0000, -0.1943,  0.2541, -0.0421,  0.4076, -0.0000,  0.0000],
        [ 0.2798, -0.0000,  0.2241, -0.0000,  0.1726, -0.3089,  0.0000, -0.0000,
         -0.0214, -0.0000, -0.0000, -0.1063,  0.3034,  0.3635,  0.0000, -0.0000],
        [ 0.0000, -0.3632,  0.2074,  0.0000,  0.0000, -0.4494,  0.2390, -0.0000,
         -0.0000, -0.0000, -0.4418, -0.0210, -0.2601,  0.2938,  0.0000,  0.0000]],
       device='cuda:0')), ('bias', tensor([0.0612, 0.0340, 0.1160, 0.0579, 0.1724, 0.1567, 0.1405, 0.1688],
       device='cuda:0')), ('__weight_mma_mask', tensor([[ True,  True, False, False, False,  True, False,  True, False,  True,
          True, False, False, False,  True,  True],
        [ True,  True, False, False, False,  True, False,  True, False, False,
          True,  True, False,  True,  True, False],
        [False,  True,  True, False,  True, False, False,  True,  True, False,
          True, False, False,  True, False,  True],
        [ True, False, False,  True,  True, False, False,  True, False,  True,
         False,  True, False,  True, False,  True],
        [False, False,  True,  True,  True, False, False,  True,  True, False,
         False,  True,  True, False, False,  True],
        [ True, False, False,  True, False, False,  True,  True, False, False,
          True,  True,  True,  True, False, False],
        [ True, False,  True, False,  True,  True, False, False,  True, False,
         False,  True,  True,  True, False, False],
        [False,  True,  True, False, False,  True,  True, False, False, False,
          True,  True,  True,  True, False, False]], device='cuda:0'))])
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-10-26 Tue 00:00<br />
Last updated: 2022-01-15 Sat 18:13</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
