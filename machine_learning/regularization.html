<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Regularization</title>


           <link rel="stylesheet" type="text/css" href="/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="./htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="../htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="/readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="./readtheorg.css"/>
           <link rel="stylesheet" type="text/css" href="../readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Regularization</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0000028">1. Regularization</a>
<ul>
<li><a href="#org0000000">1.1. 正则化的直观理解</a></li>
<li><a href="#org0000003">1.2. 增加样本</a></li>
<li><a href="#org0000006">1.3. 简化网络</a></li>
<li><a href="#org0000019">1.4. L2 和 L1 正则化</a>
<ul>
<li><a href="#org000000a">1.4.1. 无正则化</a></li>
<li><a href="#org000000e">1.4.2. L2 正则化</a></li>
<li><a href="#org0000012">1.4.3. L1 正则化</a></li>
<li><a href="#org0000016">1.4.4. 另一种解释</a></li>
</ul>
</li>
<li><a href="#org000001c">1.5. Dropout</a></li>
<li><a href="#org000001f">1.6. Batch Normalization</a></li>
<li><a href="#org0000025">1.7. pytorch 中使用正则化</a>
<ul>
<li><a href="#org0000022">1.7.1. weight_decay 和 l2 regularization 是等价的</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000028" class="outline-2">
<h2 id="org0000028"><span class="section-number-2">1.</span> Regularization</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000000" class="outline-3">
<h3 id="org0000000"><span class="section-number-3">1.1.</span> 正则化的直观理解</h3>
<div class="outline-text-3" id="text-1-1">
<p>
以 L2 正则化为例, 假设加上损失函数之前:
</p>

<ul class="org-ul">
<li>\(loss=C0\)</li>

<li>\(\frac{\partial}{\partial{w_i}}loss= d0\)</li>
</ul>

<p>
加上 L2 正则化后:
</p>

<ul class="org-ul">
<li>\(loss=C0+\frac{\alpha}{2}\sum{w^2}\)</li>

<li>\(\frac{\partial}{\partial{w_i}}loss= d0+\alpha w_i\)</li>
</ul>


<ol class="org-ol">
<li>正则化后, 由损失函数的格式可知, w 会变得偏小</li>
<li>w 变小, 会导致拟合曲线更加平滑. 因为在拟合曲线中, w 是 x 的系数, w 偏小即曲线的斜率会普遍变小, 从而更平滑</li>
<li>从 svm 的角度来看, L2 正则化项相当于一个约束, 要求 \(||w||\) 为某个值</li>
<li>w 变小, 导致样本中的少量噪音并不会使损失函数有很大的变化, 从而减小训练时噪音的影响</li>
</ol>
</div>
</div>

<div id="outline-container-org0000003" class="outline-3">
<h3 id="org0000003"><span class="section-number-3">1.2.</span> 增加样本</h3>
<div class="outline-text-3" id="text-1-2">
<p>
正则化是为了解决 overfitting 问题. 模型中的变量与样本的比值越大, 越容易发生
overfitting. 所以在不减少模型中变量的数目的前提下, 增加样本可以减轻 overfitting
</p>
</div>
</div>

<div id="outline-container-org0000006" class="outline-3">
<h3 id="org0000006"><span class="section-number-3">1.3.</span> 简化网络</h3>
<div class="outline-text-3" id="text-1-3">
<ol class="org-ol">
<li>减少 ANN hidden layer 的个数</li>
<li>减少神经元的个数</li>
</ol>
</div>
</div>

<div id="outline-container-org0000019" class="outline-3">
<h3 id="org0000019"><span class="section-number-3">1.4.</span> L2 和 L1 正则化</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org000000a" class="outline-4">
<h4 id="org000000a"><span class="section-number-4">1.4.1.</span> 无正则化</h4>
<div class="outline-text-4" id="text-1-4-1">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_moons
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap
<span class="org-keyword">from</span> torch.utils.data <span class="org-keyword">import</span> Dataset, DataLoader
<span class="org-keyword">import</span> torch

<span class="org-variable-name">model</span> <span class="org-operator">=</span> torch.nn.Sequential(
    torch.nn.Linear(2, 50), torch.nn.ReLU(), torch.nn.Linear(50, 1),
    torch.nn.Sigmoid())


<span class="org-keyword">class</span> <span class="org-type">MoonDataset</span>(Dataset):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>):
        <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> <span class="org-operator">=</span> make_moons(n_samples<span class="org-operator">=</span>1000, noise<span class="org-operator">=</span>0.2)
        <span class="org-keyword">self</span>.<span class="org-variable-name">X</span> <span class="org-operator">=</span> torch.from_numpy(X).<span class="org-builtin">float</span>()
        <span class="org-keyword">self</span>.<span class="org-variable-name">Y</span> <span class="org-operator">=</span> torch.from_numpy(Y).<span class="org-builtin">float</span>().view(<span class="org-operator">-</span>1, 1)

    <span class="org-keyword">def</span> <span class="org-function-name">__getitem__</span>(<span class="org-keyword">self</span>, index):
        <span class="org-keyword">return</span> <span class="org-keyword">self</span>.X[index], <span class="org-keyword">self</span>.Y[index]

    <span class="org-keyword">def</span> <span class="org-function-name">__len__</span>(<span class="org-keyword">self</span>):
        <span class="org-keyword">return</span> <span class="org-builtin">len</span>(<span class="org-keyword">self</span>.X)


<span class="org-variable-name">dataset</span> <span class="org-operator">=</span> MoonDataset()
<span class="org-variable-name">loader</span> <span class="org-operator">=</span> DataLoader(dataset, batch_size<span class="org-operator">=</span>100)

<span class="org-variable-name">criterion</span> <span class="org-operator">=</span> torch.nn.BCELoss()

<span class="org-variable-name">optimizer</span> <span class="org-operator">=</span> torch.optim.Adam(model.parameters())

<span class="org-keyword">def</span> <span class="org-function-name">add_regularization</span>(loss):
    <span class="org-keyword">return</span> loss

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(1000):
        <span class="org-keyword">for</span> x, y <span class="org-keyword">in</span> loader:
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> criterion(model(x), y)
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> add_regularization(loss)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

<span class="org-keyword">def</span> <span class="org-function-name">visualize</span>():
    <span class="org-variable-name">cm</span> <span class="org-operator">=</span> ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])

    plt.subplot(121)
    plt.scatter(x<span class="org-operator">=</span>dataset.X[:, 0], y<span class="org-operator">=</span>dataset.X[:, 1], c<span class="org-operator">=</span>dataset.Y[:, 0], cmap<span class="org-operator">=</span>cm)

    <span class="org-variable-name">xx</span>, <span class="org-variable-name">yy</span> <span class="org-operator">=</span> np.meshgrid(np.arange(<span class="org-operator">-</span>4, 4, 0.02), np.arange(<span class="org-operator">-</span>4, 4, 0.02))
    <span class="org-variable-name">X</span> <span class="org-operator">=</span> np.c_[xx.ravel(), yy.ravel()]

    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> model(torch.from_numpy(X).<span class="org-builtin">float</span>()).detach().numpy()
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> (y_hat <span class="org-operator">&gt;</span> 0.5)[:, 0]
    <span class="org-variable-name">y_hat</span> <span class="org-operator">=</span> y_hat.reshape(xx.shape)

    <span class="org-variable-name">cm</span> <span class="org-operator">=</span> ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
    plt.contour(xx, yy, y_hat, cmap<span class="org-operator">=</span>cm)

    <span class="org-variable-name">x</span> <span class="org-operator">=</span> <span class="org-builtin">next</span>(model.parameters()).view(<span class="org-operator">-</span>1, 1)
    plt.subplot(122)
    plt.hist(x.detach().numpy(), bins<span class="org-operator">=</span>50)
    plt.show()

    <span class="org-keyword">return</span> <span class="org-builtin">sum</span>(x)

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-7.9089])
</pre>


<div id="org0000009" class="figure">
<p><img src="../extra/no_regularization.png" alt="no_regularization.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org000000e" class="outline-4">
<h4 id="org000000e"><span class="section-number-4">1.4.2.</span> L2 正则化</h4>
<div class="outline-text-4" id="text-1-4-2">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">add_regularization</span>(loss):
    <span class="org-keyword">for</span> name, param <span class="org-keyword">in</span> model.named_parameters():
        <span class="org-keyword">if</span> <span class="org-string">'bias'</span> <span class="org-keyword">not</span> <span class="org-keyword">in</span> name:
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> loss <span class="org-operator">+</span> 0.5 <span class="org-operator">*</span> (0.001 <span class="org-operator">*</span> torch.<span class="org-builtin">sum</span>(torch.<span class="org-builtin">pow</span>(param, 2)))
    <span class="org-keyword">return</span> loss

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-5.5809])
</pre>


<div id="org000000d" class="figure">
<p><img src="../extra/l2_regularization.png" alt="l2_regularization.png" />
</p>
</div>

<p>
可以看到 L2 正则化使得 w 的值变得较小
</p>
</div>
</div>

<div id="outline-container-org0000012" class="outline-4">
<h4 id="org0000012"><span class="section-number-4">1.4.3.</span> L1 正则化</h4>
<div class="outline-text-4" id="text-1-4-3">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">add_regularization</span>(loss):
    <span class="org-keyword">for</span> name, param <span class="org-keyword">in</span> model.named_parameters():
        <span class="org-keyword">if</span> <span class="org-string">'bias'</span> <span class="org-keyword">not</span> <span class="org-keyword">in</span> name:
            <span class="org-variable-name">loss</span> <span class="org-operator">=</span> loss <span class="org-operator">+</span> (0.001 <span class="org-operator">*</span> torch.<span class="org-builtin">sum</span>(torch.<span class="org-builtin">abs</span>(param)))
    <span class="org-keyword">return</span> loss

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-7.9170])
</pre>


<div id="org0000011" class="figure">
<p><img src="../extra/l1_regularization.png" alt="l1_regularization.png" />
</p>
</div>

<p>
L1 正则化:
</p>

<ul class="org-ul">
<li>\(loss=C0+\frac{\alpha}{m}\sum{abs(w)}\)</li>

<li><p>
\(\frac{\partial}{\partial{w_i}}loss= d0+\frac{\alpha}{m}*sgn(w_i)\)
</p>

<p>
L1 正则化与 L2 相比, 对 w 的修正是一个定值, 而不像 L2 那样是一个与 w 成比例的值. 这就导致当 w 较小时, L1 修正的范围会比 L2 大, 导致 w 容易减到 0 附近. 当 w
较大时, L1 修正的范围比 L2 小, 导致 w 较大.
</p>

<p>
反映到上面的直方图中, 许多值在零附近, 但值的范围与无正则化时类似.
</p>

<p>
所以 L1 会使得 w 比较稀疏, 适合做特征选择 (选择较大的 w 对应的特征)
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org0000016" class="outline-4">
<h4 id="org0000016"><span class="section-number-4">1.4.4.</span> 另一种解释</h4>
<div class="outline-text-4" id="text-1-4-4">
<p>
从 SVM 的角度来看, 正则化项可以看作是原优化问题的不等式约束, 例如对于 L2 来说,
即是 \(\sum{w_i^2} <= C\)
</p>

<p>
假设 w 为二维, 而 L2 是把 w 约束在一个圆内, 而 L1 是把 w 约束在一个菱形中.
</p>


<div id="org0000015" class="figure">
<p><img src="../extra/l1_l2_regularization.png" alt="l1_l2_regularization.png" />
</p>
</div>

<p>
可以看到 L1 优化时非常容易取得顶点位置, 导致部分权重为 0
</p>
</div>
</div>
</div>

<div id="outline-container-org000001c" class="outline-3">
<h3 id="org000001c"><span class="section-number-3">1.5.</span> Dropout</h3>
</div>

<div id="outline-container-org000001f" class="outline-3">
<h3 id="org000001f"><span class="section-number-3">1.6.</span> Batch Normalization</h3>
</div>

<div id="outline-container-org0000025" class="outline-3">
<h3 id="org0000025"><span class="section-number-3">1.7.</span> pytorch 中使用正则化</h3>
<div class="outline-text-3" id="text-1-7">
<p>
pytorch 通过 optim 的 weight_decay 实现 L2 正则化, 而不是 把 L2 加到 loss 上再进行 backward.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">class</span> <span class="org-type">SGD</span>(Optimizer):
    <span class="org-keyword">def</span> <span class="org-function-name">step</span>():
        <span class="org-comment-delimiter"># </span><span class="org-comment">...</span>
        <span class="org-keyword">for</span> p <span class="org-keyword">in</span> group[<span class="org-string">'params'</span>]:
            <span class="org-variable-name">d_p</span> <span class="org-operator">=</span> p.grad.data
            <span class="org-keyword">if</span> weight_decay <span class="org-operator">!=</span> 0:
                <span class="org-comment-delimiter"># </span><span class="org-comment">add_ &#20250;&#25226; weight_decay*p.data &#21152;&#21040; p.grad.data &#19978;</span>
                d_p.add_(weight_decay, p.data)
        <span class="org-comment-delimiter"># </span><span class="org-comment">...</span>
</pre>
</div>
</div>

<div id="outline-container-org0000022" class="outline-4">
<h4 id="org0000022"><span class="section-number-4">1.7.1.</span> weight_decay 和 l2 regularization 是等价的</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
\(C_0\) 为原始的 loss
</p>

<p>
加入 l2 regularization 后 \(C=C_0+\frac{\lambda}{2}w^2\)
</p>

<p>
梯度为: \(\frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w}+\lambda
w\)
</p>

<p>
梯度更新后 \(w-\eta\frac{\partial C}{\partial w}= (1-\eta \lambda)w-\eta
\frac{\partial C_0}{\partial w}\)
</p>

<p>
可以看到加入 l2 regularization 后相当于把先把 w 乘上一个系数 \(1-\eta \lambda\) 后再用原来的 loss 做梯度更新. 这里的系数 \(1-\eta\lambda\) 就相当于 pytorch 中的
wegith_decay 参数, 这里也能看出来 l2 regularization 对 w 的修正是与 w 的值成例的.
</p>
</div>
</div>
</div>
</div>







<div id="outline-container-org000002b" class="outline-2 references">
<h2 id="org000002b">Backlinks</h2>
<div class="outline-text-2" id="text-org000002b">
<p>
<a href="inception.html#ID-3a113993-aa89-438f-a265-b54930e449dc">Auxiliary Classifier</a>
(<i>Inception &gt; Network &gt; Auxiliary Classifier</i>):  inception 模型会输出两个 softmax, 分别为 F, G, 其中 F 称为 auxiliary classifier. F, G 训练时使用相同的标签, F 人为的增加了训练的难度, 相当于一种 <a href="regularization.html#ID-23eb97b1-0697-4b3d-8948-9120911e1058">regularization</a> 的手段.
</p>

<p>
<a href="inception.html#ID-c8e4ddb9-1372-4a28-99c3-93f7f3e56bdf">Inception</a>
(<i>Inception &gt; Network &gt; Label Smoothing</i>):  类似于知识蒸馏中的 soft label, 可以看做一种 <a href="regularization.html#ID-23eb97b1-0697-4b3d-8948-9120911e1058">regularization</a> 手段.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2018-07-25 Wed 00:00<br />
Last updated: 2022-01-26 Wed 15:19</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>