<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-26 Wed 20:09 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Regularization</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Regularization</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org233a2a6">1. Regularization</a>
<ul>
<li><a href="#orge2c3314">1.1. 正则化的直观理解</a></li>
<li><a href="#org199022a">1.2. 增加样本</a></li>
<li><a href="#org522b64f">1.3. 简化网络</a></li>
<li><a href="#org96a47cd">1.4. L2 和 L1 正则化</a>
<ul>
<li><a href="#org9c0f775">1.4.1. 无正则化</a></li>
<li><a href="#orgaad0480">1.4.2. L2 正则化</a></li>
<li><a href="#orgc70fbfa">1.4.3. L1 正则化</a></li>
<li><a href="#org49fc3a5">1.4.4. 另一种解释</a></li>
</ul>
</li>
<li><a href="#orga01f819">1.5. Dropout</a></li>
<li><a href="#orgdf5f165">1.6. Batch Normalization</a></li>
<li><a href="#org19044df">1.7. pytorch 中使用正则化</a>
<ul>
<li><a href="#orgb2b263c">1.7.1. weight_decay 和 l2 regularization 是等价的</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org233a2a6" class="outline-2">
<h2 id="org233a2a6"><span class="section-number-2">1</span> Regularization</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orge2c3314" class="outline-3">
<h3 id="orge2c3314"><span class="section-number-3">1.1</span> 正则化的直观理解</h3>
<div class="outline-text-3" id="text-1-1">
<p>
以 L2 正则化为例, 假设加上损失函数之前:
</p>

<ul class="org-ul">
<li>\(loss=C0\)</li>

<li>\(\frac{\partial}{\partial{w_i}}loss= d0\)</li>
</ul>

<p>
加上 L2 正则化后:
</p>

<ul class="org-ul">
<li>\(loss=C0+\frac{\alpha}{2}\sum{w^2}\)</li>

<li>\(\frac{\partial}{\partial{w_i}}loss= d0+\alpha w_i\)</li>
</ul>


<ol class="org-ol">
<li>正则化后, 由损失函数的格式可知, w 会变得偏小</li>
<li>w 变小, 会导致拟合曲线更加平滑. 因为在拟合曲线中, w 是 x 的系数, w 偏小即曲线
的斜率会普遍变小, 从而更平滑</li>
<li>从 svm 的角度来看, L2 正则化项相当于一个约束, 要求 \(||w||\) 为某个值</li>
<li>w 变小, 导致样本中的少量噪音并不会使损失函数有很大的变化, 从而减小训练时噪音
的影响</li>
</ol>
</div>
</div>

<div id="outline-container-org199022a" class="outline-3">
<h3 id="org199022a"><span class="section-number-3">1.2</span> 增加样本</h3>
<div class="outline-text-3" id="text-1-2">
<p>
正则化是为了解决 overfitting 问题. 模型中的变量与样本的比值越大, 越容易发生
overfitting. 所以在不减少模型中变量的数目的前提下, 增加样本可以减轻 overfitting
</p>
</div>
</div>

<div id="outline-container-org522b64f" class="outline-3">
<h3 id="org522b64f"><span class="section-number-3">1.3</span> 简化网络</h3>
<div class="outline-text-3" id="text-1-3">
<ol class="org-ol">
<li>减少 ANN hidden layer 的个数</li>
<li>减少神经元的个数</li>
</ol>
</div>
</div>

<div id="outline-container-org96a47cd" class="outline-3">
<h3 id="org96a47cd"><span class="section-number-3">1.4</span> L2 和 L1 正则化</h3>
<div class="outline-text-3" id="text-1-4">
</div>
<div id="outline-container-org9c0f775" class="outline-4">
<h4 id="org9c0f775"><span class="section-number-4">1.4.1</span> 无正则化</h4>
<div class="outline-text-4" id="text-1-4-1">
<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from matplotlib.colors import ListedColormap
from torch.utils.data import Dataset, DataLoader
import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), torch.nn.ReLU(), torch.nn.Linear(50, 1),
    torch.nn.Sigmoid())


class MoonDataset(Dataset):
    def __init__(self):
        X, Y = make_moons(n_samples=1000, noise=0.2)
        self.X = torch.from_numpy(X).float()
        self.Y = torch.from_numpy(Y).float().view(-1, 1)

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)


dataset = MoonDataset()
loader = DataLoader(dataset, batch_size=100)

criterion = torch.nn.BCELoss()

optimizer = torch.optim.Adam(model.parameters())

def add_regularization(loss):
    return loss

def train():
    for i in range(1000):
        for x, y in loader:
            loss = criterion(model(x), y)
            loss = add_regularization(loss)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

def visualize():
    cm = ListedColormap(['#FF0000', '#0000FF'])

    plt.subplot(121)
    plt.scatter(x=dataset.X[:, 0], y=dataset.X[:, 1], c=dataset.Y[:, 0], cmap=cm)

    xx, yy = np.meshgrid(np.arange(-4, 4, 0.02), np.arange(-4, 4, 0.02))
    X = np.c_[xx.ravel(), yy.ravel()]

    y_hat = model(torch.from_numpy(X).float()).detach().numpy()
    y_hat = (y_hat &gt; 0.5)[:, 0]
    y_hat = y_hat.reshape(xx.shape)

    cm = ListedColormap(['#FF0000', '#0000FF'])
    plt.contour(xx, yy, y_hat, cmap=cm)

    x = next(model.parameters()).view(-1, 1)
    plt.subplot(122)
    plt.hist(x.detach().numpy(), bins=50)
    plt.show()

    return sum(x)

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-7.9089])
</pre>


<div id="org03b1fc0" class="figure">
<p><img src="../extra/no_regularization.png" alt="no_regularization.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgaad0480" class="outline-4">
<h4 id="orgaad0480"><span class="section-number-4">1.4.2</span> L2 正则化</h4>
<div class="outline-text-4" id="text-1-4-2">
<div class="org-src-container">
<pre class="src src-ipython">def add_regularization(loss):
    for name, param in model.named_parameters():
        if 'bias' not in name:
            loss = loss + 0.5 * (0.001 * torch.sum(torch.pow(param, 2)))
    return loss

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-5.5809])
</pre>


<div id="orgc22eb70" class="figure">
<p><img src="../extra/l2_regularization.png" alt="l2_regularization.png" />
</p>
</div>

<p>
可以看到 L2 正则化使得 w 的值变得较小
</p>
</div>
</div>

<div id="outline-container-orgc70fbfa" class="outline-4">
<h4 id="orgc70fbfa"><span class="section-number-4">1.4.3</span> L1 正则化</h4>
<div class="outline-text-4" id="text-1-4-3">
<div class="org-src-container">
<pre class="src src-ipython">def add_regularization(loss):
    for name, param in model.named_parameters():
        if 'bias' not in name:
            loss = loss + (0.001 * torch.sum(torch.abs(param)))
    return loss

train()
visualize()
</pre>
</div>

<pre class="example">
tensor([-7.9170])
</pre>


<div id="org31bae87" class="figure">
<p><img src="../extra/l1_regularization.png" alt="l1_regularization.png" />
</p>
</div>

<p>
L1 正则化:
</p>

<ul class="org-ul">
<li>\(loss=C0+\frac{\alpha}{m}\sum{abs(w)}\)</li>

<li><p>
\(\frac{\partial}{\partial{w_i}}loss= d0+\frac{\alpha}{m}*sgn(w_i)\)
</p>

<p>
L1 正则化与 L2 相比, 对 w 的修正是一个定值, 而不像 L2 那样是一个与 w 成比例的
值. 这就导致当 w 较小时, L1 修正的范围会比 L2 大, 导致 w 容易减到 0 附近. 当 w
较大时, L1 修正的范围比 L2 小, 导致 w 较大.
</p>

<p>
反映到上面的直方图中, 许多值在零附近, 但值的范围与无正则化时类似.
</p>

<p>
所以 L1 会使得 w 比较稀疏, 适合做特征选择 (选择较大的 w 对应的特征)
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org49fc3a5" class="outline-4">
<h4 id="org49fc3a5"><span class="section-number-4">1.4.4</span> 另一种解释</h4>
<div class="outline-text-4" id="text-1-4-4">
<p>
从 SVM 的角度来看, 正则化项可以看作是原优化问题的不等式约束, 例如对于 L2 来说,
即是 \(\sum{w_i^2} <= C\)
</p>

<p>
假设 w 为二维, 而 L2 是把 w 约束在一个圆内, 而 L1 是把 w 约束在一个菱形中.
</p>


<div id="org9623174" class="figure">
<p><img src="../extra/l1_l2_regularization.png" alt="l1_l2_regularization.png" />
</p>
</div>

<p>
可以看到 L1 优化时非常容易取得顶点位置, 导致部分权重为 0
</p>
</div>
</div>
</div>

<div id="outline-container-orga01f819" class="outline-3">
<h3 id="orga01f819"><span class="section-number-3">1.5</span> Dropout</h3>
</div>

<div id="outline-container-orgdf5f165" class="outline-3">
<h3 id="orgdf5f165"><span class="section-number-3">1.6</span> Batch Normalization</h3>
</div>

<div id="outline-container-org19044df" class="outline-3">
<h3 id="org19044df"><span class="section-number-3">1.7</span> pytorch 中使用正则化</h3>
<div class="outline-text-3" id="text-1-7">
<p>
pytorch 通过 optim 的 weight_decay 实现 L2 正则化, 而不是 把 L2 加到 loss 上再进
行 backward.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">class</span> <span style="font-weight: bold; text-decoration: underline;">SGD</span>(Optimizer):
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">step</span>():
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">...</span>
        <span style="font-weight: bold;">for</span> p <span style="font-weight: bold;">in</span> group[<span style="font-style: italic;">'params'</span>]:
            <span style="font-weight: bold; font-style: italic;">d_p</span> = p.grad.data
            <span style="font-weight: bold;">if</span> weight_decay != 0:
                <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">add_ &#20250;&#25226; weight_decay*p.data &#21152;&#21040; p.grad.data &#19978;</span>
                d_p.add_(weight_decay, p.data)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">...</span>
</pre>
</div>
</div>

<div id="outline-container-orgb2b263c" class="outline-4">
<h4 id="orgb2b263c"><span class="section-number-4">1.7.1</span> weight_decay 和 l2 regularization 是等价的</h4>
<div class="outline-text-4" id="text-1-7-1">
<p>
\(C_0\) 为原始的 loss
</p>

<p>
加入 l2 regularization 后 \(C=C_0+\frac{\lambda}{2}w^2\)
</p>

<p>
梯度为: \(\frac{\partial C}{\partial w}=\frac{\partial C_0}{\partial w}+\lambda
w\)
</p>

<p>
梯度更新后 \(w-\eta\frac{\partial C}{\partial w}= (1-\eta \lambda)w-\eta
\frac{\partial C_0}{\partial w}\)
</p>

<p>
可以看到加入 l2 regularization 后相当于把先把 w 乘上一个系数 \(1-\eta \lambda\) 后
再用原来的 loss 做梯度更新. 这里的系数 \(1-\eta\lambda\) 就相当于 pytorch 中的
wegith_decay 参数, 这里也能看出来 l2 regularization 对 w 的修正是与 w 的值成例的.
</p>
</div>
</div>
</div>
</div>







<div id="outline-container-org8a1d6fe" class="outline-2 references">
<h2 id="org8a1d6fe">Backlinks</h2>
<div class="outline-text-2" id="text-org8a1d6fe">
<p>
<a href="inception.html#ID-c8e4ddb9-1372-4a28-99c3-93f7f3e56bdf">Inception</a>
(<i>Inception &gt; Network &gt; Auxiliary Classifier</i>):  inception 模型会输出两个 softmax, 分别为 F, G, 其中 F 称为 auxiliary classifier. F, G 训练时使用相同的标签, F 人为的增加了训练的难度, 相当于一种 <a href="regularization.html#ID-23eb97b1-0697-4b3d-8948-9120911e1058">regularization</a> 的手段.
</p>

<p>
<a href="inception.html#ID-c8e4ddb9-1372-4a28-99c3-93f7f3e56bdf">Inception</a>
(<i>Inception &gt; Network &gt; Label Smoothing</i>):  类似于知识蒸馏中的 soft label, 可以看做一种 <a href="regularization.html#ID-23eb97b1-0697-4b3d-8948-9120911e1058">regularization</a> 手段.
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2018-07-25 Wed 00:00<br />
Last updated: 2022-01-26 Wed 15:19</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
