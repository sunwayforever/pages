<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Deep Learning</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org00000bd">1. Deep Learning</a>
<ul>
<li><a href="#org000002f">1.1. linear regression</a>
<ul>
<li><a href="#org0000002">1.1.1. training set</a></li>
<li><a href="#org0000005">1.1.2. feature &amp; label</a></li>
<li><a href="#org0000008">1.1.3. regression</a></li>
<li><a href="#org000000c">1.1.4. hypothesis function</a></li>
<li><a href="#org000000f">1.1.5. cost function</a></li>
<li><a href="#org0000012">1.1.6. mean square errno</a></li>
<li><a href="#org0000016">1.1.7. gradient decent</a></li>
<li><a href="#org0000019">1.1.8. partial derivative</a></li>
<li><a href="#org000001e">1.1.9. simple linear regression example</a></li>
<li><a href="#org0000022">1.1.10. gradient checking</a></li>
<li><a href="#org0000025">1.1.11. learning rate</a></li>
<li><a href="#org000002c">1.1.12. feature scaling</a></li>
</ul>
</li>
<li><a href="#org0000046">1.2. multi-feature linear regression</a>
<ul>
<li><a href="#org0000034">1.2.1. feature &amp; label</a></li>
<li><a href="#org0000043">1.2.2. learning rate</a></li>
</ul>
</li>
<li><a href="#org0000063">1.3. logistic regression</a>
<ul>
<li><a href="#org000004b">1.3.1. feature &amp; label</a></li>
<li><a href="#org000004e">1.3.2. regression</a></li>
<li><a href="#org0000051">1.3.3. hypothesis function</a></li>
<li><a href="#org0000055">1.3.4. sigmoid</a></li>
<li><a href="#org0000058">1.3.5. cost function</a></li>
<li><a href="#org000005d">1.3.6. cross entropy</a></li>
<li><a href="#org0000060">1.3.7. partial derivative</a></li>
</ul>
</li>
<li><a href="#org0000078">1.4. $\frac{\partial}{&part;{W}}J(W,B)</a>
<ul>
<li><a href="#org0000068">1.4.1. logistic regression example</a></li>
<li><a href="#org000006f">1.4.2. polynormial features</a></li>
<li><a href="#org0000072">1.4.3. underfitting &amp; overfitting</a></li>
<li><a href="#org0000075">1.4.4. regularization</a></li>
</ul>
</li>
<li><a href="#org000008e">1.5. multi-class logistic regression</a>
<ul>
<li><a href="#org000007c">1.5.1. training set</a></li>
<li><a href="#org0000080">1.5.2. one_hot</a></li>
<li><a href="#org0000084">1.5.3. softmax</a></li>
<li><a href="#org0000087">1.5.4. partial derivative</a></li>
<li><a href="#org000008b">1.5.5. load_digits logistic regression example</a></li>
</ul>
</li>
<li><a href="#org00000b7">1.6. artificial neural networks</a>
<ul>
<li><a href="#org0000093">1.6.1. activation function</a></li>
<li><a href="#org0000097">1.6.2. relu</a></li>
<li><a href="#org000009a">1.6.3. forward propergation</a></li>
<li><a href="#org000009d">1.6.4. backward propergation</a></li>
<li><a href="#org00000a2">1.6.5. make_moon ANN example</a></li>
<li><a href="#org00000a6">1.6.6. gradient checking</a></li>
<li><a href="#org00000ab">1.6.7. ANN underfitting</a></li>
<li><a href="#org00000b0">1.6.8. ANN overfitting</a></li>
<li><a href="#org00000b4">1.6.9. load_digits ANN example</a></li>
</ul>
</li>
<li><a href="#org00000ba">1.7. What's Next</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org00000bd" class="outline-2">
<h2 id="org00000bd"><span class="section-number-2">1</span> Deep Learning</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org000002f" class="outline-3">
<h3 id="org000002f"><span class="section-number-3">1.1</span> linear regression</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = 0, 0
<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-keyword">global</span> X,Y
    <span class="org-variable-name">data</span> = np.loadtxt(<span class="org-string">"../extra/data.txt"</span>, delimiter=<span class="org-string">","</span>)
    <span class="org-variable-name">X</span> = data[:, 0].reshape(-1, 1)
    <span class="org-variable-name">Y</span> = data[:, 1].reshape(-1, 1)

get_training_set()
plt.scatter(x=X[:, 0], y=Y[:, 0])
plt.show()
</pre>
</div>


<div id="org0000000" class="figure">
<p><img src="machine_learning_files/machine_learning_1_0.png" alt="machine_learning_1_0.png" />
</p>
<p><span class="figure-number">Figure 1: </span>png</p>
</div>
</div>

<div id="outline-container-org0000002" class="outline-4">
<h4 id="org0000002"><span class="section-number-4">1.1.1</span> training set</h4>
<div class="outline-text-4" id="text-1-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(<span class="org-string">"X:"</span>,X[:10])
<span class="org-keyword">print</span>()
<span class="org-keyword">print</span>(<span class="org-string">"Y:"</span>,Y[:10])
</pre>
</div>

<pre class="example" id="org0000001">
X: [[6.1101]
 [5.5277]
 [8.5186]
 [7.0032]
 [5.8598]
 [8.3829]
 [7.4764]
 [8.5781]
 [6.4862]
 [5.0546]]

Y: [[17.592 ]
 [ 9.1302]
 [13.662 ]
 [11.854 ]
 [ 6.8233]
 [11.886 ]
 [ 4.3483]
 [12.    ]
 [ 6.5987]
 [ 3.8166]]
</pre>
</div>
</div>

<div id="outline-container-org0000005" class="outline-4">
<h4 id="org0000005"><span class="section-number-4">1.1.2</span> feature &amp; label</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
X is a matrix of <code>features</code>
</p>

<p>
Y is a matrix of <code>labels</code>
</p>
</div>
</div>

<div id="outline-container-org0000008" class="outline-4">
<h4 id="org0000008"><span class="section-number-4">1.1.3</span> regression</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
给定一个 training set 中不存在的 x, 例如 <code>5.2</code>, 如何预测它对应的 y?
</p>
</div>
</div>

<div id="outline-container-org000000c" class="outline-4">
<h4 id="org000000c"><span class="section-number-4">1.1.4</span> hypothesis function</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>
\(\hat{y}=h(x)=Wx+B\)
</p>

<p>
W ~ <code>weights</code>
</p>

<p>
B ~ <code>Bias</code>
</p>

<div class="org-src-container">
<pre class="src src-python">get_training_set()
plt.scatter(x=X[:, 0], y=Y[:, 0])
<span class="org-variable-name">W</span>=0.2
<span class="org-variable-name">B</span>=8
<span class="org-variable-name">x</span>=np.arange(5,22.5,0.1)
plt.plot(x, np.dot(x,W)+B,label=<span class="org-string">"bad"</span>, )
<span class="org-variable-name">W</span>=1.18
<span class="org-variable-name">B</span>=-3.79
plt.plot(x, np.dot(x,W)+B,label=<span class="org-string">"good"</span>)
plt.legend()
plt.show()
</pre>
</div>


<div id="org000000b" class="figure">
<p><img src="machine_learning_files/machine_learning_10_0.png" alt="machine_learning_10_0.png" />
</p>
<p><span class="figure-number">Figure 2: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org000000f" class="outline-4">
<h4 id="org000000f"><span class="section-number-4">1.1.5</span> cost function</h4>
<div class="outline-text-4" id="text-1-1-5">
<p>
使用 cost function 来衡量 hypothesis function 是否足够好, cost function
值越小， hypothesis function 越好
</p>
</div>
</div>

<div id="outline-container-org0000012" class="outline-4">
<h4 id="org0000012"><span class="section-number-4">1.1.6</span> mean square errno</h4>
<div class="outline-text-4" id="text-1-1-6">
<p>
\(J(W,B)=\frac{1}{2m}\sum\limits_{i=1}^{m}(\hat{y^i}-y^i)^2\)
</p>
</div>
</div>

<div id="outline-container-org0000016" class="outline-4">
<h4 id="org0000016"><span class="section-number-4">1.1.7</span> gradient decent</h4>
<div class="outline-text-4" id="text-1-1-7">
<p>
gradient decent
</p>

<p>
$ W &rarr; W-&alpha;\frac{\partial}{&part;{W}}J(W,B) $
</p>

<p>
$ B &rarr; B-&alpha;\frac{\partial}{&part;{B}}J(W,B) $
</p>


<div id="org0000015" class="figure">
<p><img src="../extra/gradient_decent.png" alt="gradient_decent.png" />
</p>
<p><span class="figure-number">Figure 3: </span>gradient</p>
</div>
</div>
</div>

<div id="outline-container-org0000019" class="outline-4">
<h4 id="org0000019"><span class="section-number-4">1.1.8</span> partial derivative</h4>
<div class="outline-text-4" id="text-1-1-8">
<p>
\(\frac{\partial}{\partial{W}}J(W,B) =
\frac{1}{2m}\sum2/(\hat{y}-y)/\frac{\partial}{\partial{W}}{\hat{y^i}} = \frac{1}{m}\sum(\hat{y}-y)*{x^j}\)
</p>

<p>
\(\frac{\partial}{\partial{B}}J(W,B) =
\frac{1}{2m}\sum2/(\hat{y}-y)/\frac{\partial}{\partial{B}}{\hat{y^i}} = \frac{1}{m}\sum(\hat{y}-y)\)
</p>
</div>
</div>

<div id="outline-container-org000001e" class="outline-4">
<h4 id="org000001e"><span class="section-number-4">1.1.9</span> simple linear regression example</h4>
<div class="outline-text-4" id="text-1-1-9">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> load_boston
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap
<span class="org-keyword">from</span> PIL <span class="org-keyword">import</span> Image
<span class="org-keyword">import</span> sys

<span class="org-variable-name">EPOCH</span> = 50
<span class="org-variable-name">LEARNING_RATE</span> = 0.1
<span class="org-variable-name">WITH_FEATURE_SCALING</span>=<span class="org-constant">True</span>

<span class="org-keyword">def</span> <span class="org-function-name">mse</span>(A, B):
    <span class="org-keyword">return</span> np.square(np.subtract(A, B)).mean()/2

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-keyword">global</span> X, Y
    <span class="org-variable-name">data</span> = np.loadtxt(<span class="org-string">"../extra/data.txt"</span>, delimiter=<span class="org-string">","</span>)

    <span class="org-variable-name">X</span> = data[:, 0].reshape(-1, 1)
    <span class="org-variable-name">Y</span> = data[:, 1].reshape(-1, 1)
    <span class="org-keyword">if</span> WITH_FEATURE_SCALING:
        <span class="org-variable-name">X</span> = preprocessing.scale(X)
        <span class="org-variable-name">Y</span> = preprocessing.scale(Y)

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W, B):
    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-variable-name">J</span> = 0.
    <span class="org-variable-name">dw</span> = np.zeros_like(W)
    <span class="org-variable-name">db</span> = np.zeros_like(B)
    <span class="org-variable-name">y_hat</span> = np.matmul(X, W) + B
    <span class="org-variable-name">J</span> = mse(y_hat, Y)
    <span class="org-variable-name">dw</span> = np.matmul(np.transpose(X), y_hat - Y)/m
    <span class="org-variable-name">db</span> = (y_hat - Y).mean()
    <span class="org-keyword">return</span> J, dw, db

<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W, B):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw</span>, <span class="org-variable-name">db</span> = cost_function(X, Y, W, B)
        <span class="org-keyword">if</span> epoch % (EPOCH//10) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, cost ,W, B)
        <span class="org-variable-name">W</span> = W - alpha * dw
        <span class="org-variable-name">B</span> = B - alpha * db
    <span class="org-keyword">return</span> W, B

<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = 0, 0
<span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = 0, 0

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> X, Y, W, B
    <span class="org-variable-name">W</span> = np.random.randn(X.shape[1], 1)
    <span class="org-variable-name">B</span> = np.random.randn(1, 1)
    <span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = gradient_decent(X, Y, W, B)

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> W, B, X, Y
    plt.scatter(x=X[:, 0], y=Y[:, 0])
    plt.plot(X, np.matmul(X, W) + B, <span class="org-string">'g'</span>)
    plt.show()

get_training_set()
train()
predict()
</pre>
</div>

<pre class="example" id="org000001c">
training: # 0 0.44911036737309545 [[1.57832978]] [[0.22798332]]
training: # 5 0.2536317389235355 [[1.27510542]] [[0.13462187]]
training: # 10 0.1854725556828555 [[1.09605446]] [[0.07949287]]
training: # 15 0.16170691799200512 [[0.99032667]] [[0.04693974]]
training: # 20 0.15342035251397768 [[0.92789546]] [[0.02771745]]
training: # 25 0.15053100578931256 [[0.89103046]] [[0.01636688]]
training: # 30 0.14952355288044825 [[0.86926204]] [[0.00966448]]
training: # 35 0.14917227577171124 [[0.85640801]] [[0.00570678]]
training: # 40 0.149049793017394 [[0.84881783]] [[0.00336979]]
training: # 45 0.14900708592167952 [[0.84433591]] [[0.00198983]]
</pre>


<div id="org000001d" class="figure">
<p><img src="machine_learning_files/machine_learning_21_1.png" alt="machine_learning_21_1.png" />
</p>
<p><span class="figure-number">Figure 4: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org0000022" class="outline-4">
<h4 id="org0000022"><span class="section-number-4">1.1.10</span> gradient checking</h4>
<div class="outline-text-4" id="text-1-1-10">
<p>
make sure that:
</p>

<p>
$\frac{\partial}{&part;{W}}J(W,B)
&asymp; lim\<sub>h-&gt;0</sub>\frac{J(W+h,B)-J(W,B)}{h} $
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">check_gradient</span>():
    <span class="org-keyword">global</span> X,Y
    <span class="org-variable-name">W</span> = np.random.randn(X.shape[1], 1)
    <span class="org-variable-name">B</span> = np.random.randn(1, 1)
    <span class="org-variable-name">cost</span>,<span class="org-variable-name">dw</span>,<span class="org-variable-name">db</span>=cost_function(X,Y,W,B)
    <span class="org-variable-name">W</span>[0]+=1e-3
    <span class="org-variable-name">cost2</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>=cost_function(X,Y,W,B)
    <span class="org-keyword">print</span>((cost2-cost)/1e-3,dw[0,0])
    <span class="org-variable-name">W</span>[0]-=1e-3
    <span class="org-variable-name">B</span>[0]+=1e-3
    <span class="org-variable-name">cost2</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>=cost_function(X,Y,W,B)
    <span class="org-keyword">print</span>((cost2-cost)/1e-3,db)

check_gradient()
</pre>
</div>

<pre class="example" id="org0000021">
-0.8722622453554152 -0.8727622453556267
-1.5264856192187537 -1.5269856192187428
</pre>
</div>
</div>

<div id="outline-container-org0000025" class="outline-4">
<h4 id="org0000025"><span class="section-number-4">1.1.11</span> learning rate</h4>
<div class="outline-text-4" id="text-1-1-11">
<ul class="org-ul">
<li>learning rate 过小会导致 gradient decent 过慢</li>
<li>learning rate 过大会导致 gradient decent 无法收敛

<ul class="org-ul">
<li>在最小值附近反复</li>
<li>变的越来越大</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org000002c" class="outline-4">
<h4 id="org000002c"><span class="section-number-4">1.1.12</span> feature scaling</h4>
<div class="outline-text-4" id="text-1-1-12">
<p>
通过把 feature scale 到一定固定的范围， 例如 (0,1),
可以更容易的选择合适的 learning rate
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">WITH_FEATURE_SCALING</span>=<span class="org-constant">False</span>

get_training_set()
train()
predict()
</pre>
</div>

<pre class="example" id="org0000028">
training: # 0 70.15408361397526 [[-0.40822281]] [[-0.4916959]]
training: # 5 24925634169.17572 [[24500.47499024]] [[2460.57784825]]
training: # 10 9.625926543152364e+18 [[-4.81456725e+08]] [[-48367551.32807031]]
training: # 15 3.717396364137783e+27 [[9.46140039e+12]] [[9.50500295e+11]]
training: # 20 1.4356057742758617e+36 [[-1.85931762e+17]] [[-1.86788622e+16]]
training: # 25 5.544105974322745e+44 [[3.6538587e+21]] [[3.67069739e+20]]
training: # 30 2.1410551284544287e+53 [[-7.18042105e+25]] [[-7.21351179e+24]]
training: # 35 8.268451368556306e+61 [[1.41106843e+30]] [[1.41757129e+29]]
training: # 40 3.193158696643795e+70 [[-2.77297681e+34]] [[-2.78575599e+33]]
training: # 45 1.2331526192107467e+79 [[5.44934622e+38]] [[5.47445936e+37]]
</pre>


<div id="org0000029" class="figure">
<p><img src="machine_learning_files/machine_learning_29_1.png" alt="machine_learning_29_1.png" />
</p>
<p><span class="figure-number">Figure 5: </span>png</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">WITH_FEATURE_SCALING</span>=<span class="org-constant">False</span>
<span class="org-variable-name">LEARNING_RATE</span>=0.01

get_training_set()
train()
predict()
</pre>
</div>

<pre class="example" id="org000002a">
training: # 0 103.38177778309628 [[-0.85004288]] [[1.11590542]]
-125.37942448626893 -125.42012645764883
training: # 5 6.86205229028994 [[0.67856153]] [[1.22261939]]
-0.07423379726745338 -0.11493576867876723
training: # 10 6.81940301336375 [[0.68344985]] [[1.1766803]]
-0.05115933051325072 -0.09186130192399862
training: # 15 6.777519309593261 [[0.68802624]] [[1.13112685]]
-0.05033045343871834 -0.09103242485023677
training: # 20 6.736384504793291 [[0.69256148]] [[1.08598248]]
-0.04951293203792062 -0.09021490344897255
training: # 25 6.6959852083221 [[0.69705599]] [[1.04124353]]
-0.048702753118412545 -0.08940472452908702
training: # 30 6.656308268968583 [[0.70151014]] [[0.99690637]]
-0.04789985004727271 -0.08860182145685279
training: # 35 6.617340770671049 [[0.70592429]] [[0.95296738]]
-0.04710415748121477 -0.08780612889108504
training: # 40 6.579070028312648 [[0.7102988]] [[0.90942298]]
-0.04631561066581469 -0.08701758207749494
training: # 45 6.541483583591981 [[0.71463402]] [[0.86626963]]
-0.045534145431957995 -0.08623611684335007
</pre>


<div id="org000002b" class="figure">
<p><img src="machine_learning_files/machine_learning_30_1.png" alt="machine_learning_30_1.png" />
</p>
<p><span class="figure-number">Figure 6: </span>png</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000046" class="outline-3">
<h3 id="org0000046"><span class="section-number-3">1.2</span> multi-feature linear regression</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org0000034" class="outline-4">
<h4 id="org0000034"><span class="section-number-4">1.2.1</span> feature &amp; label</h4>
<div class="outline-text-4" id="text-1-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span>=0,0,0,0
<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = load_boston(<span class="org-constant">True</span>)
    <span class="org-variable-name">X</span> = preprocessing.scale(X)
    <span class="org-variable-name">Y</span> = Y.reshape(-1, 1)
    [m, features] = X.shape
    <span class="org-comment-delimiter"># </span><span class="org-comment">Z = np.concatenate((X, Y), axis=1)</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">np.random.shuffle(Z)</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">X = Z[:, :features]</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">Y = Z[:, features:]</span>
    <span class="org-keyword">global</span> X_train, Y_train, X_test, Y_test
    <span class="org-variable-name">offset</span> = <span class="org-builtin">int</span>(0.8 * m)
    <span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span> = X[:offset], Y[:offset]
    <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = X[offset:], Y[offset:]

get_training_set()
<span class="org-keyword">print</span>(X_train.shape)
<span class="org-keyword">print</span>(X_train[:5])
<span class="org-keyword">print</span>()
<span class="org-keyword">print</span>(Y_train[:5])
</pre>
</div>

<pre class="example" id="org0000032">
(404, 13)
[[-0.41771335  0.28482986 -1.2879095  -0.27259857 -0.14421743  0.41367189
  -0.12001342  0.1402136  -0.98284286 -0.66660821 -1.45900038  0.44105193
  -1.0755623 ]
 [-0.41526932 -0.48772236 -0.59338101 -0.27259857 -0.74026221  0.19427445
   0.36716642  0.55715988 -0.8678825  -0.98732948 -0.30309415  0.44105193
  -0.49243937]
 [-0.41527165 -0.48772236 -0.59338101 -0.27259857 -0.74026221  1.28271368
  -0.26581176  0.55715988 -0.8678825  -0.98732948 -0.30309415  0.39642699
  -1.2087274 ]
 [-0.41468015 -0.48772236 -1.30687771 -0.27259857 -0.83528384  1.01630251
  -0.80988851  1.07773662 -0.75292215 -1.10611514  0.1130321   0.41616284
  -1.36151682]
 [-0.41040922 -0.48772236 -1.30687771 -0.27259857 -0.83528384  1.22857665
  -0.51117971  1.07773662 -0.75292215 -1.10611514  0.1130321   0.44105193
  -1.02650148]]

[[24. ]
 [21.6]
 [34.7]
 [33.4]
 [36.2]]
</pre>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> load_boston
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap
<span class="org-keyword">from</span> PIL <span class="org-keyword">import</span> Image
<span class="org-keyword">import</span> sys

<span class="org-variable-name">EPOCH</span> = 10000
<span class="org-variable-name">LEARNING_RATE</span> = 0.001

<span class="org-keyword">def</span> <span class="org-function-name">MSE</span>(A, B):
    <span class="org-keyword">return</span> np.square(np.subtract(A, B)).mean()/2

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = load_boston(<span class="org-constant">True</span>)
    <span class="org-variable-name">X</span> = preprocessing.scale(X)
    <span class="org-variable-name">Y</span> = Y.reshape(-1, 1)
    [m, features] = X.shape
    <span class="org-comment-delimiter"># </span><span class="org-comment">Z = np.concatenate((X, Y), axis=1)</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">np.random.shuffle(Z)</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">X = Z[:, :features]</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">Y = Z[:, features:]</span>
    <span class="org-keyword">global</span> X_train, Y_train, X_test, Y_test
    <span class="org-variable-name">offset</span> = <span class="org-builtin">int</span>(0.8 * m)
    <span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span> = X[:offset], Y[:offset]
    <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = X[offset:], Y[offset:]

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W, B):
    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-variable-name">J</span> = 0.
    <span class="org-variable-name">dw</span> = np.zeros_like(W)
    <span class="org-variable-name">db</span> = np.zeros_like(B)
    <span class="org-comment-delimiter"># </span><span class="org-comment">for i in range(m):</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">x = X[i, :].reshape(1, -1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y = Y[i, :].reshape(1, 1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y_hat = np.matmul(x, W) + B</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment"># print(y_hat[0], y[0], MSE(y_hat[0], y[0]))</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">loss = MSE(y_hat, y)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">J += loss</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw += np.matmul(np.transpose(x), y_hat - y)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db += y_hat - y</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">J /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db /= m</span>
    <span class="org-variable-name">y_hat</span> = np.matmul(X, W) + B
    <span class="org-variable-name">J</span> = MSE(y_hat, Y)
    <span class="org-variable-name">dw</span> = np.matmul(np.transpose(X), y_hat - Y) / m
    <span class="org-variable-name">db</span> = (y_hat - Y).mean(axis=0)
    <span class="org-keyword">return</span> J, dw, db

<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W, B):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw</span>, <span class="org-variable-name">db</span> = cost_function(X, Y, W, B)
        <span class="org-variable-name">W</span> = W - alpha * dw
        <span class="org-variable-name">B</span> = B - alpha * db
        <span class="org-keyword">if</span> epoch % (EPOCH // 20) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, cost)
    <span class="org-keyword">return</span> W, B

<span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span>, <span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span> = 0, 0, 0, 0
<span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = 0, 0

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> X_train, Y_train, W, B
    <span class="org-variable-name">W</span> = np.random.randn(X_train.shape[1], 1)
    <span class="org-variable-name">B</span> = np.random.randn(1, 1)
    <span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = gradient_decent(X_train, Y_train, W, B)

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> W, B
    <span class="org-variable-name">predicted</span> = np.matmul(X_test, W) + B
    <span class="org-keyword">print</span>(<span class="org-string">"final mse:"</span>,MSE(predicted, Y_test))

get_training_set()
train()
predict()
<span class="org-keyword">print</span>(W.shape,B.shape)
</pre>
</div>

<pre class="example" id="org0000033">
training: # 0 394.9508088516925
training: # 500 100.00785133938153
training: # 1000 48.72540414517899
training: # 1500 29.563643302467156
training: # 2000 21.405362653669222
training: # 2500 17.580513440523312
training: # 3000 15.634018864772019
training: # 3500 14.561620691509146
training: # 4000 13.919491179775438
training: # 4500 13.50050685782247
training: # 5000 13.2039753832719
training: # 5500 12.979149491604765
training: # 6000 12.799463170021674
training: # 6500 12.650373482061207
training: # 7000 12.523479650727792
training: # 7500 12.413618800743162
training: # 8000 12.317401683858627
training: # 8500 12.232456842224869
training: # 9000 12.15702919159006
training: # 9500 12.0897592813033
final mse: 11.213129305213641
(13, 1) (1, 1)
</pre>
</div>
</div>

<div id="outline-container-org0000043" class="outline-4">
<h4 id="org0000043"><span class="section-number-4">1.2.2</span> learning rate</h4>
<div class="outline-text-4" id="text-1-2-2">
</div>
<div id="outline-container-org0000038" class="outline-5">
<h5 id="org0000038"><span class="section-number-5">1.2.2.1</span> learning rate 偏大</h5>
<div class="outline-text-5" id="text-1-2-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">LEARNING_RATE</span> = 0.1
train()
predict()
</pre>
</div>

<pre class="example" id="org0000037">
training: # 0 323.6508450938955
training: # 500 11.39299545706192
training: # 1000 11.389204091166178
training: # 1500 11.389189815007983
training: # 2000 11.389189761104692
training: # 2500 11.389189760901166
training: # 3000 11.389189760900395
training: # 3500 11.389189760900395
training: # 4000 11.389189760900393
training: # 4500 11.389189760900395
training: # 5000 11.389189760900393
training: # 5500 11.389189760900395
training: # 6000 11.389189760900395
training: # 6500 11.389189760900395
training: # 7000 11.389189760900395
training: # 7500 11.389189760900395
training: # 8000 11.389189760900395
training: # 8500 11.389189760900395
training: # 9000 11.389189760900395
training: # 9500 11.389189760900395
final mse: 16.537813246137475
</pre>
</div>
</div>

<div id="outline-container-org000003c" class="outline-5">
<h5 id="org000003c"><span class="section-number-5">1.2.2.2</span> learning rate 过大</h5>
<div class="outline-text-5" id="text-1-2-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">LEARNING_RATE</span> = 1
train()
predict()
</pre>
</div>

<pre class="example" id="org000003b">
training: # 0 323.85035063163946
training: # 500 nan
training: # 1000 nan
training: # 1500 nan
training: # 2000 nan
training: # 2500 nan

/usr/lib/python3.6/site-packages/numpy/core/_methods.py:70: RuntimeWarning: overflow encountered in reduce
  ret = umr_sum(arr, axis, dtype, out, keepdims)
/usr/lib/python3.6/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in square

training: # 3000 nan
training: # 3500 nan
training: # 4000 nan
training: # 4500 nan
training: # 5000 nan
training: # 5500 nan
training: # 6000 nan
training: # 6500 nan
training: # 7000 nan
training: # 7500 nan
training: # 8000 nan
training: # 8500 nan
training: # 9000 nan
training: # 9500 nan
final mse: nan
</pre>
</div>
</div>

<div id="outline-container-org0000040" class="outline-5">
<h5 id="org0000040"><span class="section-number-5">1.2.2.3</span> learning rate 过小</h5>
<div class="outline-text-5" id="text-1-2-2-3">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">LEARNING_RATE</span> = 0.001
train()
predict()
</pre>
</div>

<pre class="example" id="org000003f">
training: # 0 368.22478200801294
training: # 500 113.60286787202149
training: # 1000 54.16900270646828
training: # 1500 32.30450821323542
training: # 2000 23.043704375375118
training: # 2500 18.67757830837426
training: # 3000 16.42877795251914
training: # 3500 15.169696609178645
training: # 4000 14.40183707813676
training: # 4500 13.891712751211747
training: # 5000 13.52521273665566
training: # 5500 13.244422131600182
training: # 6000 13.018730561961272
training: # 6500 12.83116082771574
training: # 7000 12.671739347524003
training: # 7500 12.534213486546351
training: # 8000 12.414390934896627
training: # 8500 12.30927849859642
training: # 9000 12.216621683323485
training: # 9500 12.134649211355635
final mse: 12.350629498610731
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000063" class="outline-3">
<h3 id="org0000063"><span class="section-number-3">1.3</span> logistic regression</h3>
<div class="outline-text-3" id="text-1-3">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_moons
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap

<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = make_moons(n_samples=1000, noise=0.2)
<span class="org-variable-name">Y</span> = Y.reshape(-1, 1)

<span class="org-variable-name">cm</span> = ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
plt.scatter(x=X[:, 0], y=X[:, 1], c=Y[:, 0], cmap=cm)
plt.show()
</pre>
</div>


<div id="org0000049" class="figure">
<p><img src="machine_learning_files/machine_learning_43_0.png" alt="machine_learning_43_0.png" />
</p>
<p><span class="figure-number">Figure 7: </span>png</p>
</div>
</div>

<div id="outline-container-org000004b" class="outline-4">
<h4 id="org000004b"><span class="section-number-4">1.3.1</span> feature &amp; label</h4>
<div class="outline-text-4" id="text-1-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">print</span>(<span class="org-string">"X:"</span>,X[:10])
<span class="org-keyword">print</span>()
<span class="org-keyword">print</span>(<span class="org-string">"Y:"</span>,Y[:10])
</pre>
</div>

<pre class="example" id="org000004a">
X: [[ 0.8525511  -0.39862199]
 [ 0.1632493  -0.34467561]
 [ 1.15179796  0.77013127]
 [ 1.09955561 -0.42176977]
 [ 2.14572763 -0.19269439]
 [-0.04550188  1.2857288 ]
 [ 1.18711691  0.17124724]
 [ 1.82545617  0.84777188]
 [-0.15896536 -0.1438627 ]
 [ 1.04386354  1.06882075]]

Y: [[1]
 [1]
 [0]
 [1]
 [1]
 [0]
 [0]
 [1]
 [1]
 [0]]
</pre>
</div>
</div>

<div id="outline-container-org000004e" class="outline-4">
<h4 id="org000004e"><span class="section-number-4">1.3.2</span> regression</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
给定一个 x (x0, x1), 预测它所属的类型 （1 或 0， 蓝或红）
</p>
</div>
</div>

<div id="outline-container-org0000051" class="outline-4">
<h4 id="org0000051"><span class="section-number-4">1.3.3</span> hypothesis function</h4>
<div class="outline-text-4" id="text-1-3-3">
<p>
linear regression 的 hypothesis function: \(\hat{y}=h(x)=Wx+B\) 对于
logistic regression 不直接适用， 因为我们需要它输出 0 或 1 的离散值
</p>
</div>
</div>

<div id="outline-container-org0000055" class="outline-4">
<h4 id="org0000055"><span class="section-number-4">1.3.4</span> sigmoid</h4>
<div class="outline-text-4" id="text-1-3-4">
<p>
\(sigmoid(x)=\frac{1}{1+e^{-x}}\)
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">x</span>=np.arange(-10,10,0.1)
<span class="org-variable-name">y</span>=1/(1+np.exp(-x))
plt.plot(x,y)
plt.show()
</pre>
</div>


<div id="org0000054" class="figure">
<p><img src="machine_learning_files/machine_learning_52_0.png" alt="machine_learning_52_0.png" />
</p>
<p><span class="figure-number">Figure 8: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org0000058" class="outline-4">
<h4 id="org0000058"><span class="section-number-4">1.3.5</span> cost function</h4>
<div class="outline-text-4" id="text-1-3-5">
<p>
logistic regression 没有使用 mse 做为 cost function <a href="gradient_descent.html#ID-33e1fdaf-8107-45cb-8cc9-2bdb889e471e">sigmoid 为何需要搭配 BCELoss</a>
</p>

<p>
因此， logistic regression 使用 cross entropy 作为 cost function,
直观上说， 两个概率分布越接近， 其 cross entropy 越小
</p>
</div>
</div>

<div id="outline-container-org000005d" class="outline-4">
<h4 id="org000005d"><span class="section-number-4">1.3.6</span> cross entropy</h4>
<div class="outline-text-4" id="text-1-3-6">
<p>
\(H(p,q)=-\sum _{x}p(x)\,\log q(x).\!\)
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">cross_entropy</span>(predictions, labels):
    <span class="org-variable-name">epsilon</span> = 1e-12
    <span class="org-variable-name">predictions</span> = np.clip(predictions, epsilon, 1 - epsilon)
    <span class="org-variable-name">N</span> = predictions.shape[0]

    <span class="org-variable-name">ce</span> = 0 - np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(labels * np.log(predictions))) / N
    <span class="org-keyword">return</span> ce

<span class="org-keyword">print</span>(cross_entropy(np.array([[0.1,0.9]]),np.array([[1,0]])))
<span class="org-keyword">print</span>(cross_entropy(np.array([[0.9,0.1]]),np.array([[1,0]])))
<span class="org-keyword">print</span>(cross_entropy(np.array([[0.9,0.1]]),np.array([[0,1]])))
<span class="org-keyword">print</span>(cross_entropy(np.array([[0.2,0.8]]),np.array([[0,1]])))
</pre>
</div>

<pre class="example" id="org000005b">
2.3025850929940455
0.10536051565782628
2.3025850929940455
0.2231435513142097
</pre>

<p>
logistic regression 的 cost function 是一个简化版的 cross entropy:
</p>

<p>
即 p 为 \((\hat{y},1-\hat{y})\), q 为 \((y,1-y)\),
</p>

<p>
所以它的 cost function 为
</p>

<p>
\(J(W,B)=-\sum_1^m({y*\log(\hat{y})+(1-y)*\log(1-\hat{y})})\)
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">x</span>=np.arange(0.01,1,0.01)
plt.plot(x,-np.log(x),label=<span class="org-string">"-log(x)"</span>)
plt.plot(x,-np.log(1-x),label=<span class="org-string">"-log(1-x)"</span>)
plt.legend()
plt.show()
</pre>
</div>


<div id="org000005c" class="figure">
<p><img src="machine_learning_files/machine_learning_59_0.png" alt="machine_learning_59_0.png" />
</p>
<p><span class="figure-number">Figure 9: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org0000060" class="outline-4">
<h4 id="org0000060"><span class="section-number-4">1.3.7</span> partial derivative</h4>
<div class="outline-text-4" id="text-1-3-7">
<p>
\(h(x)=Wx+B\)
</p>

<p>
\(\hat{y}=sigmoid(h(x))\)
</p>

<p>
\(J(W,B)=-\sum({y*\log(\hat{y})+(1-y)*\log(1-\hat{y})})\)
</p>
</div>
</div>
</div>

<div id="outline-container-org0000078" class="outline-3">
<h3 id="org0000078"><span class="section-number-3">1.4</span> $\frac{\partial}{&part;{W}}J(W,B)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
-&sum;{\frac{y}{\hat{y}}}\frac{\partial}{&part;{W}}\hat{y}-\frac{1-y}{1-\hat{y}}\frac{\partial}{&part;{W}}\hat{y}
$
</p>

<p>
$\frac{\partial}{&part;{W}}{\hat{y}} = \hat{y}/(1-\hat{y})/x $
</p>

<p>
\(\frac{\partial}{\partial{W}}J(W,B) = \frac{1}{m}{\sum{(\hat{y}-y})*x}\)
</p>

<p>
\(\frac{\partial}{\partial{B}}J(W,B) = \frac{1}{m}{\sum{(\hat{y}-y})}\)
</p>
</div>

<div id="outline-container-org0000068" class="outline-4">
<h4 id="org0000068"><span class="section-number-4">1.4.1</span> logistic regression example</h4>
<div class="outline-text-4" id="text-1-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_moons
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_circles
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap
<span class="org-keyword">from</span> PIL <span class="org-keyword">import</span> Image
<span class="org-keyword">import</span> sys

<span class="org-variable-name">EPOCH</span> = 500
<span class="org-variable-name">LEARNING_RATE</span> = 0.01
<span class="org-variable-name">BATCH_SIZE</span> = 10
<span class="org-variable-name">FEATURE_SIZE</span> = 0
<span class="org-variable-name">POLY_FEATURES</span> = 3

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
    <span class="org-keyword">return</span> 1 / (1 + np.exp(-x))

<span class="org-keyword">def</span> <span class="org-function-name">cross_entropy</span>(predictions, labels):
    <span class="org-variable-name">epsilon</span> = 1e-12
    <span class="org-variable-name">predictions</span> = np.clip(predictions, epsilon, 1 - epsilon)
    <span class="org-variable-name">N</span> = predictions.shape[0]
    <span class="org-variable-name">ce</span> = 0 - np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(labels * np.log(predictions))) / N
    <span class="org-keyword">return</span> ce

<span class="org-keyword">def</span> <span class="org-function-name">draw_decision_boundary</span>(W, B):
    <span class="org-variable-name">xx</span>, <span class="org-variable-name">yy</span> = np.meshgrid(
        np.arange(-1.5, 2.5, 0.02), np.arange(-1.5, 2.5, 0.02))
    <span class="org-variable-name">X</span> = np.c_[xx.ravel(), yy.ravel()]

    <span class="org-variable-name">poly</span> = preprocessing.PolynomialFeatures(POLY_FEATURES, include_bias=<span class="org-constant">False</span>)
    <span class="org-variable-name">X</span> = poly.fit_transform(X)

    <span class="org-variable-name">Y</span> = ((sigmoid(np.matmul(X, W) + B)) &gt; 0.5)[:, 0]
    <span class="org-variable-name">Y</span> = Y.reshape(xx.shape)
    <span class="org-variable-name">cm</span> = ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
    plt.contour(xx, yy, Y, cmap=cm)

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = make_moons(n_samples=1000, noise=0.2)
    <span class="org-comment-delimiter"># </span><span class="org-comment">X, Y = make_circles(n_samples=1000, noise=0.2, factor=0.5)</span>
    <span class="org-variable-name">Y</span> = Y.reshape(1000, 1)
    <span class="org-variable-name">poly</span> = preprocessing.PolynomialFeatures(POLY_FEATURES, include_bias=<span class="org-constant">False</span>)
    <span class="org-variable-name">X</span> = poly.fit_transform(X, Y)
    <span class="org-keyword">global</span> FEATURE_SIZE
    <span class="org-variable-name">FEATURE_SIZE</span> = X.shape[1]
    <span class="org-keyword">return</span> X, Y

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W, B):
    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-keyword">assert</span> (<span class="org-builtin">len</span>(X) == <span class="org-builtin">len</span>(Y))

    <span class="org-variable-name">J</span> = 0.

    <span class="org-variable-name">dw</span> = np.zeros_like(W)
    <span class="org-variable-name">db</span> = np.zeros_like(B)

    <span class="org-variable-name">y_hat</span> = sigmoid(np.matmul(X, W) + B)
    <span class="org-variable-name">J</span> = cross_entropy(np.concatenate((y_hat, 1 - y_hat)), np.concatenate((Y, 1 - Y)))
    <span class="org-variable-name">dw</span> = np.matmul(X.T, y_hat - Y) / m
    <span class="org-variable-name">db</span> = (y_hat - Y).mean(axis=0)
    <span class="org-keyword">return</span> J, dw, db

<span class="org-variable-name">W</span>,<span class="org-variable-name">B</span>=0,0
<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W, B):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">batch</span> = <span class="org-builtin">len</span>(X) // BATCH_SIZE
        <span class="org-variable-name">total_loss</span> = 0
        <span class="org-keyword">for</span> X_batch, Y_batch <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(
                np.split(X[:batch * BATCH_SIZE], batch),
                np.split(Y[:batch * BATCH_SIZE], batch)):
            <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw</span>, <span class="org-variable-name">db</span> = cost_function(X_batch, Y_batch, W, B)
            <span class="org-variable-name">total_loss</span> += cost
            <span class="org-variable-name">W</span> = W - alpha * dw
            <span class="org-variable-name">B</span> = B - alpha * db

        <span class="org-keyword">if</span> epoch % (EPOCH//30) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, total_loss / batch)

    <span class="org-keyword">return</span> W, B

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> W,B
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = get_training_set()

    <span class="org-variable-name">Z</span> = np.concatenate((X, Y), axis=1)
    np.random.shuffle(Z)
    <span class="org-variable-name">X</span> = Z[:, :FEATURE_SIZE]
    <span class="org-variable-name">Y</span> = Z[:, FEATURE_SIZE:]

    <span class="org-variable-name">W</span> = np.random.randn(FEATURE_SIZE, 1)
    <span class="org-variable-name">B</span> = np.random.randn(1, 1)

    <span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = gradient_decent(X, Y, W, B)

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> W,B
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = get_training_set()
    <span class="org-variable-name">cm</span> = ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
    <span class="org-comment-delimiter"># </span><span class="org-comment">plt.plot(X[:, 0], X[:, 1], 'bo', label='Real data')</span>
    <span class="org-variable-name">y_hat</span> = ((sigmoid(np.matmul(X, W) + B)) &gt; 0.5)[:, 0]
    plt.scatter(x=X[:, 0], y=X[:, 1], c=Y[:, 0], cmap=cm)
    <span class="org-comment-delimiter"># </span><span class="org-comment">plt.scatter(x=X[:, 0], y=X[:, 1], c=y_hat, cmap=cm)</span>
    draw_decision_boundary(W, B)
    plt.show()

train()
predict()
</pre>
</div>

<pre class="example" id="org0000066">
training: # 0 0.619742973585997
training: # 16 0.17178658979627992
training: # 32 0.13487915355393248
training: # 48 0.12197846405639046
training: # 64 0.11395684458496719
training: # 80 0.10787208561892815
training: # 96 0.10291655423822696
training: # 112 0.0987579063665693
training: # 128 0.09521177753383403
training: # 144 0.09215291261899158
training: # 160 0.08948702750211028
training: # 176 0.08714053451732778
training: # 192 0.08505555055681267
training: # 208 0.08318653814486694
training: # 224 0.08149765158102548
training: # 240 0.07996058158625095
training: # 256 0.07855283335926001
training: # 272 0.07725638132916592
training: # 288 0.076056636971483
training: # 304 0.07494166614098187
training: # 320 0.07390159911921022
training: # 336 0.07292818633539222
training: # 352 0.07201446275282071
training: # 368 0.0711544927837573
training: # 384 0.0703431748029327
training: # 400 0.06957608988875362
training: # 416 0.06884938357165127
training: # 432 0.06815967240663312
training: # 448 0.06750396938582705
training: # 464 0.06687962379072875
training: # 480 0.06628427222462412
training: # 496 0.0657157983897877
</pre>


<div id="org0000067" class="figure">
<p><img src="machine_learning_files/machine_learning_64_1.png" alt="machine_learning_64_1.png" />
</p>
<p><span class="figure-number">Figure 10: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org000006f" class="outline-4">
<h4 id="org000006f"><span class="section-number-4">1.4.2</span> polynormial features</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
moon 这个例子里， decision boundary
明显不是一条直线，如果我们仅仅用原始的 feature 进行 regression,
最终只能得到一条直线， 例如下面 POLY\_FEATURES 为 1 的情形
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">POLY_FEATURES</span>=1
train()
predict()
</pre>
</div>

<pre class="example" id="org000006b">
training: # 0 0.2854766049060549
training: # 16 0.17277657607109922
training: # 32 0.15635266723112187
training: # 48 0.1493546886034027
training: # 64 0.1455593393582365
training: # 80 0.14329109540467472
training: # 96 0.14185780692665098
training: # 112 0.14091661921781962
training: # 128 0.14028055239864892
training: # 144 0.13984099949762763
training: # 160 0.1395318373157802
training: # 176 0.139311279131328
training: # 192 0.1391521054830019
training: # 208 0.13903613821495753
training: # 224 0.13895098343274292
training: # 240 0.1388880435606131
training: # 256 0.13884126717244613
training: # 272 0.13880634230124886
training: # 288 0.13878016421457948
training: # 304 0.13876047744704142
training: # 320 0.13874563097468282
training: # 336 0.13873440831324527
training: # 352 0.1387259081022462
training: # 368 0.13871945923467058
training: # 384 0.1387145599485834
training: # 400 0.13871083374087895
training: # 416 0.13870799721751376
training: # 432 0.13870583649403773
training: # 448 0.1387041897724283
training: # 464 0.13870293441266346
training: # 480 0.13870197729689518
training: # 496 0.13870124761970357
</pre>


<div id="org000006c" class="figure">
<p><img src="machine_learning_files/machine_learning_67_1.png" alt="machine_learning_67_1.png" />
</p>
<p><span class="figure-number">Figure 11: </span>png</p>
</div>

<p>
为了拟合成一条曲线， 我们把原始的 feature 做了修改， 例如， 原始 feature
为 [\(x1\),\(x2\),\(x3\)], 我们把它变成
[\(x1\),\(x2\),\(x3\),\(x1^2\),\(x2^2\),\(x3^2\),\(x1*x2\),\(x1*x3\),\(x2*x3\)], sklearn
的 PolynormialFeatures 类可以完成这种操作
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">POLY_FEATURES</span>=7
train()
predict()
</pre>
</div>

<pre class="example" id="org000006d">
training: # 0 1.072493450454601
training: # 16 0.12517269994676714
training: # 32 0.10019008105575024
training: # 48 0.08845764537675999
training: # 64 0.08117824825999641
training: # 80 0.07617331085450527
training: # 96 0.0724893405480805
training: # 112 0.06964268516153896
training: # 128 0.06736243690988726
training: # 144 0.06548490220832501
training: # 160 0.06390509140298205
training: # 176 0.06255237236394463
training: # 192 0.06137734432013081
training: # 208 0.0603443289005192
training: # 224 0.05942685621245631
training: # 240 0.05860483518675161
training: # 256 0.057862715087380216
training: # 272 0.05718825356269268
training: # 288 0.05657166868407597
training: # 304 0.056005041406647346
training: # 320 0.055481885676289505
training: # 336 0.05499683340343065
training: # 352 0.05454539978629234
training: # 368 0.05412380589131122
training: # 384 0.05372884272286389
training: # 400 0.05335776581488923
training: # 416 0.053008212586936033
training: # 432 0.052678136893091254
training: # 448 0.05236575670587306
training: # 464 0.05206951194142586
training: # 480 0.05178803019125905
training: # 496 0.051520098673991474

/usr/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: overflow encountered in exp
</pre>


<div id="org000006e" class="figure">
<p><img src="machine_learning_files/machine_learning_69_2.png" alt="machine_learning_69_2.png" />
</p>
<p><span class="figure-number">Figure 12: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org0000072" class="outline-4">
<h4 id="org0000072"><span class="section-number-4">1.4.3</span> underfitting &amp; overfitting</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
在上面的例子中， POLY\_FEATURES 设为 1 时， 拟合的结果是 underfitting,
当 POLY\_FEATURES 为 7 时， 结果有些 overfitting, FEATURES 为 3
时看起来是比较合适的
</p>
</div>
</div>

<div id="outline-container-org0000075" class="outline-4">
<h4 id="org0000075"><span class="section-number-4">1.4.4</span> regularization</h4>
<div class="outline-text-4" id="text-1-4-4">
<ul class="org-ul">
<li>没有 regularization</li>
</ul>

<p>
\(J(W,B)=-\sum({y*\log(\hat{y})+(1-y)*\log(1-\hat{y})})\)
</p>

<p>
\(\frac{\partial}{\partial{W}}J(W,B) = \frac{1}{m}{\sum{(\hat{y}-y})*x}\)
</p>

<ul class="org-ul">
<li>加上 regularization</li>
</ul>

<p>
$J(W,B)=-&sum;({y/log(\hat{y})+(1-y)/log(1-\hat{y})}) +
\frac{\alpha}{2m} &sum;\_i W\_i\^2 $
</p>

<p>
$\frac{\partial}{&part;{W}}J(W,B) = \frac{1}{m}{&sum;{(\hat{y}-y})*x}
+\frac{\alpha}{m}W $
</p>

<p>
通过 \(\alpha\) 可以控制 regularization 的程度， 当 \(\alpha\) 很小时，
倾向于 overfitting, 当 \(\alpha\) 很大时， 倾向于 underfitting
</p>
</div>
</div>
</div>

<div id="outline-container-org000008e" class="outline-3">
<h3 id="org000008e"><span class="section-number-3">1.5</span> multi-class logistic regression</h3>
<div class="outline-text-3" id="text-1-5">
<p>
普通的 logistic regression 用来作二分类， hypothesis function
输出可以看作是预测结果为 1 的概率， label 只有两种结果: 0 或 1
</p>

<p>
在多分类的问题中， label 会有多个值。 mnist 手写数字识别是一个经典的
multi-classification 问题
</p>
</div>

<div id="outline-container-org000007c" class="outline-4">
<h4 id="org000007c"><span class="section-number-4">1.5.1</span> training set</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
mnist training set 中， 每张图片是 28 *28 像素， 处理时把它 flatten
成一个 784 大小的 array.
</p>

<p>
每个 label 的值的范围是 0~9, 表示图片对应的数字
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">from</span> os <span class="org-keyword">import</span> listdir
<span class="org-keyword">from</span> os.path <span class="org-keyword">import</span> isfile, join

<span class="org-keyword">def</span> <span class="org-function-name">get_test_set</span>():
    <span class="org-variable-name">X</span> = np.ndarray(shape=(0, 784))
    <span class="org-variable-name">Y</span> = np.ndarray(shape=(0, 10))
    <span class="org-variable-name">baseDir</span> = <span class="org-string">"/home/sunway/program/mnist/testSet"</span>
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10):
        <span class="org-variable-name">currDir</span> = (baseDir + <span class="org-string">"/"</span> + <span class="org-builtin">str</span>(i))
        <span class="org-variable-name">files</span> = [
            join(currDir, f) <span class="org-keyword">for</span> f <span class="org-keyword">in</span> listdir(currDir)
            <span class="org-keyword">if</span> isfile(join(currDir, f))
        ]
        <span class="org-keyword">for</span> f <span class="org-keyword">in</span> files:
            <span class="org-variable-name">X</span> = np.concatenate((X, np.array(Image.<span class="org-builtin">open</span>(f)).reshape(1, 784)))
            <span class="org-variable-name">Y</span> = np.concatenate((Y, one_hot(np.array([i]), 10)))

    <span class="org-keyword">return</span> X, Y

<span class="org-variable-name">X</span>,<span class="org-variable-name">Y</span>=get_test_set()
<span class="org-keyword">print</span>(<span class="org-string">"X:"</span>,X[0])
<span class="org-keyword">print</span>()
<span class="org-keyword">print</span>(<span class="org-string">"Y:"</span>,Y[0])
</pre>
</div>

<pre class="example" id="org000007b">
X: [  0.   0.   0.   0.   0.   0.   0.   0.   0.  11.   3.   0.   6.   8.
   0.   9.   3.   0.   9.   0.   7.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   9.   0.   0.
  14.   0.   0.   1.   3.   0.   0.   3.   1.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.  12.   0.  27.   0.   0.  10.
  15.   0.   0.   0.   0.  20.   0.   0.   0.   4.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.  28.   0.  14.   0.   0.
   0.   2.   9.   5.   0.   1.   0.   9.   7.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   7.   0.   6.   0.  13.   0.
  10. 108. 168. 224. 231. 145.  58.   0.   0.   6.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   7.   3.   6.   0.  14.  56.
 208. 237. 254. 255. 255. 247. 208.  79.   0.   2.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.  12.   0.   0.   8. 215.
 255. 255. 171. 136. 169. 231. 255. 150.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   6.   0.   5. 160. 247.
 255. 192.  32.   0.   4. 104. 248. 238.  33.   7.   0.   0.   0.   0.
   0.   1.   0.   6.   7.   0.   0.   0.   4.   4.   0.  12. 231. 255.
 178.  11.   3.   0.   7.  19. 246. 255.  55.   0.   0.   0.   0.   0.
   0.   5.   0.   0.   0.   5.  10.   2.   0.   0.   0. 145. 255. 232.
   7.   0.   0.  14.   0.  10. 255. 255.  95.  12.   0.   0.   0.   0.
   0.   0.   4.   0.   0.   4.   5.   0.   0.  14.  97. 232. 255. 138.
   0.   3.   0.   7.   0.  18. 255. 234.  48.   0.   0.   0.   0.   0.
   0.   0.   8.  11.   0.   0.   0.   1.   6.   5. 218. 249. 250. 110.
   0.   6.   0.   0.   8.  43. 250. 252.  25.   7.   0.   0.   0.   0.
   6.   0.   5.   0.   0.   6.   0.   7.   1. 170. 255. 188. 133.  38.
   0.   0.   4.   0.  10.  83. 239. 247.  30.   5.   0.   0.   0.   0.
   0.   0.   7.   0.   0.  15.   0.   0. 118. 255. 252.  54.   0.  14.
   6.   6.   4.   0.   2. 175. 255. 163.  10.   0.   0.   0.   0.   0.
   0.   0.   8.   0.   5.   3.   0.  49. 250. 248.  84.  17.   0.   0.
  31.   0.   0.   0.  66. 255. 255.  79.  10.   6.   0.   0.   0.   0.
   9.   0.   0.   0.   6.   0.  13. 168. 248. 244.   7.   0.   9.   0.
   0.   7.   3.  33. 176. 255. 186.  16.   0.   0.   0.   0.   0.   0.
   2.   3.   0.   5.   0.   3.  61. 237. 255.  68.   0.   8.   2.   0.
  12.   2.   8. 112. 255. 239. 103.   0.   0.   0.   0.   0.   0.   0.
   0.   5.   0.   0.   9.   0. 147. 254. 173.  20.   0.   0.   6.  11.
   0.   0.  76. 250. 233. 133.   0.   8.   4.   5.   0.   0.   0.   0.
   0.   0.   3.   0.  10.  18. 214. 238.  43.   0.   6.   0.  10.   0.
   0. 141. 252. 255. 242.  34.   3.   0.   7.   0.   0.   0.   0.   0.
   7.   0.   5.   3.   0.  95. 233. 205.   9.   0.  18.   0.  15.  98.
 182. 248. 255. 228.  28.   0.   0.   7.   0.  29.   0.   0.   0.   0.
   4.   0.   0.   6.   0. 129. 255. 216.  32.  28.  35.  96. 212. 241.
 252. 255. 203.  76.  31.   0.   1.  14.   0.   0.   0.   0.   0.   0.
   0.  10.   0.   0.   0.  84. 255. 255. 255. 251. 252. 255. 255. 255.
 244. 106.   0.   8.   0.   0.  16.   0.   1.   0.   0.   0.   0.   0.
   0.   1.   0.   0.   3.  21. 190. 254. 252. 248. 245. 250. 251. 159.
  65.   0.  10.   0.   9.  17.   0.   0.   0.  14.   0.   0.   0.   0.
  10.   0.   9.   4.   0.   0.  78. 213. 198. 184. 174. 106.  57.   3.
   0.   7.   0.   5.   0.   0.  14.   0.   0.   2.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.
   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]

Y: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>

<p>
mnist 中， W 的 shape 不再是 [784, 1], 而是变成了 [784, 10], 其中 W[:,0]
用来预测图片为 0 的概率， W[:,1] 预测图片为 1 的概率&#x2026;
</p>

<p>
因此 \(WX+B\) 结果是一个 [m,10] 的 matrix, 但 Y 的值却是 0~9,
所以我们需要把 label 值变为一个 [10,1] 的 matrix, 其中第 n 行代表 label
是 n 的概率。
</p>

<p>
这个转换 label 的过程叫做 one\_hot
</p>
</div>
</div>

<div id="outline-container-org0000080" class="outline-4">
<h4 id="org0000080"><span class="section-number-4">1.5.2</span> one_hot</h4>
<div class="outline-text-4" id="text-1-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">one_hot</span>(Y, C):
    <span class="org-variable-name">one</span> = np.eye(C)[Y.reshape(-1)]
    <span class="org-keyword">return</span> one

<span class="org-variable-name">x</span>=np.array([[0,1,2,3,4,5,6,7,8,9]])
<span class="org-keyword">print</span>(one_hot(x,10))
</pre>
</div>

<pre class="example" id="org000007f">
[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
</pre>

<p>
普通的 logistic regression 通过 sigmoid 把 <code>一个值</code> 转换为
=结果为 1 的概率=， 在 mnist 中 \(XW+B\) 输出 [10,1] 的 matrix，
我们需要一个 hypothesis function 把这个 matrix 变成 10 个概率，
然后再通过某个 cost\_function 与 one\_hot 转换后的 label 比较.
</p>

<p>
把 [10,1] 的 matrix 转换为 10 个概率的方法， 可以是分别进行 sigmoid,
但更常见的是通过 softmax 函数
</p>
</div>
</div>

<div id="outline-container-org0000084" class="outline-4">
<h4 id="org0000084"><span class="section-number-4">1.5.3</span> softmax</h4>
<div class="outline-text-4" id="text-1-5-3">
<p>
\(softmax=\frac{e^{x^i}}{\sum{e^x}}\)
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">softmax</span>(X):
    <span class="org-variable-name">exps</span> = np.exp(X)
    <span class="org-keyword">return</span> exps / np.<span class="org-builtin">sum</span>(exps)

<span class="org-variable-name">x</span>=np.array([[1,2,3,4]])
<span class="org-keyword">print</span>(softmax(x))
</pre>
</div>

<pre class="example" id="org0000083">
[[0.0320586  0.08714432 0.23688282 0.64391426]]
</pre>
</div>
</div>

<div id="outline-container-org0000087" class="outline-4">
<h4 id="org0000087"><span class="section-number-4">1.5.4</span> partial derivative</h4>
<div class="outline-text-4" id="text-1-5-4">
<p>
\(h=WX+B\)
</p>

<p>
\(\hat{y}=softmax(h)\)
</p>

<p>
\(J=cross\_entropy(\hat{y},y)\)
</p>

<p>
\(\frac{\partial}{\partial{W}}J(W,B) = \frac{1}{m}{\sum{(\hat{y}-y})*x}\)
</p>

<p>
\(\frac{\partial}{\partial{B}}J(W,B) = \frac{1}{m}{\sum{(\hat{y}-y})}\)
</p>
</div>
</div>

<div id="outline-container-org000008b" class="outline-4">
<h4 id="org000008b"><span class="section-number-4">1.5.5</span> load_digits logistic regression example</h4>
<div class="outline-text-4" id="text-1-5-5">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">from</span> PIL <span class="org-keyword">import</span> Image
<span class="org-keyword">import</span> sys
<span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> load_digits

<span class="org-variable-name">EPOCH</span> = 100
<span class="org-variable-name">LEARNING_RATE</span> = 0.01
<span class="org-variable-name">BATCH_SIZE</span> = 2
<span class="org-variable-name">REGULARIZATION_FACTOR</span> = 0.01

<span class="org-keyword">def</span> <span class="org-function-name">softmax</span>(z):
    <span class="org-variable-name">s</span> = np.<span class="org-builtin">max</span>(z, axis=1)
    <span class="org-variable-name">s</span> = s[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">necessary step to do broadcasting</span>
    <span class="org-variable-name">e_x</span> = np.exp(z - s)
    <span class="org-variable-name">div</span> = np.<span class="org-builtin">sum</span>(e_x, axis=1)
    <span class="org-variable-name">div</span> = div[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">dito</span>
    <span class="org-keyword">return</span> e_x / div

<span class="org-keyword">def</span> <span class="org-function-name">one_hot</span>(Y, C):
    <span class="org-variable-name">Y</span> = np.eye(C)[Y.reshape(-1)]
    <span class="org-keyword">return</span> Y

<span class="org-keyword">def</span> <span class="org-function-name">cross_entropy</span>(predictions, labels):
    <span class="org-variable-name">epsilon</span> = 1e-12
    <span class="org-variable-name">predictions</span> = np.clip(predictions, epsilon, 1 - epsilon)
    <span class="org-variable-name">N</span> = predictions.shape[0]

    <span class="org-variable-name">ce</span> = 0 - np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(labels * np.log(predictions))) / N
    <span class="org-keyword">return</span> ce

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W, B):
    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-keyword">assert</span> (<span class="org-builtin">len</span>(X) == <span class="org-builtin">len</span>(Y))

    <span class="org-variable-name">J</span> = 0.

    <span class="org-variable-name">dw</span> = np.zeros_like(W)
    <span class="org-variable-name">db</span> = np.zeros_like(B)

    <span class="org-comment-delimiter"># </span><span class="org-comment">for i in range(m):</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">x = X[i, :].reshape(1, 64)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y = Y[i, :].reshape(1, 10)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">f = np.matmul(x, W) + B</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">assert (f.shape == (1, 10))</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y_hat = softmax(f)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">assert (y.shape == y_hat.shape)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">loss = cross_entropy(y_hat, y)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">J += loss</span>

    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw += np.matmul(np.transpose(x), y_hat - y)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db += y_hat - y</span>

    <span class="org-comment-delimiter"># </span><span class="org-comment">J /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db /= m</span>

    <span class="org-variable-name">f</span> = np.matmul(X, W) + B
    <span class="org-variable-name">y_hat</span> = softmax(f)
    <span class="org-variable-name">J</span> = cross_entropy(y_hat, Y)
    <span class="org-variable-name">dw</span> = np.matmul(np.transpose(X), y_hat - Y) / m
    <span class="org-variable-name">db</span> = (y_hat - Y).mean(axis=0)

    <span class="org-comment-delimiter"># </span><span class="org-comment">regularation</span>
    <span class="org-variable-name">J</span> += REGULARIZATION_FACTOR * <span class="org-builtin">sum</span>(<span class="org-builtin">sum</span>(np.square(W))) / (2 * m)
    <span class="org-variable-name">dw</span> += REGULARIZATION_FACTOR * W / m
    <span class="org-keyword">return</span> J, dw, db

<span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = 0, 0, 0, 0
<span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = 0, 0

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = load_digits(10, <span class="org-constant">True</span>)
    <span class="org-variable-name">Y</span> = Y.reshape(-1, 1)
    <span class="org-variable-name">Y</span> = one_hot(Y, 10)
    [m, features] = X.shape
    <span class="org-variable-name">Z</span> = np.concatenate((X, Y), axis=1)
    np.random.shuffle(Z)
    <span class="org-variable-name">X</span> = Z[:, :features]
    <span class="org-variable-name">Y</span> = Z[:, features:]
    <span class="org-keyword">global</span> X_train, Y_train, X_test, Y_test
    <span class="org-variable-name">offset</span> = <span class="org-builtin">int</span>(0.8 * m)
    <span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span> = X[:offset], Y[:offset]
    <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = X[offset:], Y[offset:]

<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W, B):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">batch</span> = <span class="org-builtin">len</span>(X) // BATCH_SIZE
        <span class="org-variable-name">total_loss</span> = 0
        <span class="org-keyword">for</span> X_batch, Y_batch <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(
                np.split(X[:batch * BATCH_SIZE], batch),
                np.split(Y[:batch * BATCH_SIZE], batch)):
            <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw</span>, <span class="org-variable-name">db</span> = cost_function(X_batch, Y_batch, W, B)
            <span class="org-variable-name">total_loss</span> += cost
            <span class="org-variable-name">W</span> = W - alpha * dw
            <span class="org-variable-name">B</span> = B - alpha * db
        <span class="org-keyword">if</span> epoch % (EPOCH // 30) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, total_loss / batch)
    <span class="org-keyword">return</span> W, B

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> X_test, Y_test, W, B

    <span class="org-variable-name">wrong</span> = 0
    <span class="org-variable-name">correct</span> = 0
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-builtin">len</span>(X_test)):
        <span class="org-variable-name">x</span> = X_test[i, :].reshape(1, -1)
        <span class="org-variable-name">y</span> = Y_test[i, :].reshape(1, -1)
        <span class="org-variable-name">c</span> = np.matmul(x, W) + B
        <span class="org-variable-name">y_hat</span> = softmax(c)

        <span class="org-keyword">if</span> np.argmax(y_hat) == np.argmax(y):
            <span class="org-variable-name">correct</span> += 1
        <span class="org-keyword">else</span>:
            <span class="org-variable-name">wrong</span> += 1
    <span class="org-keyword">print</span>(<span class="org-string">"correct: %d, wrong: %d, accuracy: %f"</span> % (correct, wrong, correct /
                                                    (correct + wrong)))

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> W, B, X_train, Y_train
    <span class="org-variable-name">W</span> = np.random.randn(64, 10)
    <span class="org-variable-name">B</span> = np.random.randn(1, 10)
    <span class="org-variable-name">W</span>, <span class="org-variable-name">B</span> = gradient_decent(X_train, Y_train, W, B)

get_training_set()
train()
predict()
</pre>
</div>

<pre class="example" id="org000008a">
training: # 0 7.413641925368053
training: # 3 2.061976320350589
training: # 6 1.5752044059841135
training: # 9 1.25185391564078
training: # 12 1.0334895315893526
training: # 15 0.9154572322047317
training: # 18 0.7912384177526893
training: # 21 0.6569779524881322
training: # 24 0.6154367878411028
training: # 27 0.6250007208991311
training: # 30 0.519215167487594
training: # 33 0.5119700580330538
training: # 36 0.5275788708720639
training: # 39 0.5304955801897778
training: # 42 0.44298460952993773
training: # 45 0.40653637116386976
training: # 48 0.45331106089167467
training: # 51 0.37611751513467656
training: # 54 0.37240293166520994
training: # 57 0.4553268960419496
training: # 60 0.3623002402468144
training: # 63 0.37149394065324326
training: # 66 0.4276031139481319
training: # 69 0.4003649152441614
training: # 72 0.43875428025202756
training: # 75 0.40422299306062504
training: # 78 0.4026933009211833
training: # 81 0.40849225974261905
training: # 84 0.3801959998258428
training: # 87 0.3976900455580225
training: # 90 0.4330326570439522
training: # 93 0.4622412012606176
training: # 96 0.4132252813914691
training: # 99 0.3563033761424611
correct: 339, wrong: 21, accuracy: 0.941667
</pre>
</div>
</div>
</div>

<div id="outline-container-org00000b7" class="outline-3">
<h3 id="org00000b7"><span class="section-number-3">1.6</span> artificial neural networks</h3>
<div class="outline-text-3" id="text-1-6">
<p>
ANN 可以看作是多层的 logistic regression 级联起来， 前一层 logistic
regression 的输出作为后一层 logistic regression 的输入
</p>


<div id="org0000091" class="figure">
<p><img src="../extra/ann1.png" alt="ann1.png" />
</p>
<p><span class="figure-number">Figure 13: </span>ann</p>
</div>

<p>
把 hidden layer 去掉， 就是一个基本的 logistic regression
</p>
</div>

<div id="outline-container-org0000093" class="outline-4">
<h4 id="org0000093"><span class="section-number-4">1.6.1</span> activation function</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
每个 hidden layer 中的 node 可以认为是一个 logistic regression,
即它们会完成简单的 \(sigmoid(WX+B)\) 运算，其中 sigmoid 在 ANN 中称为
activation function.
</p>

<p>
除了 sigmoid, 常用的 activation function 还有 tanh, relu 等
</p>


<div id="org0000092" class="figure">
<p><img src="../extra/ann2.png" alt="ann2.png" />
</p>
<p><span class="figure-number">Figure 14: </span>ann2</p>
</div>
</div>
</div>

<div id="outline-container-org0000097" class="outline-4">
<h4 id="org0000097"><span class="section-number-4">1.6.2</span> relu</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
relu 是现在的推荐的 activation function
</p>

<p>
\(relu(x)=\max(x,0)\)
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">relu</span>(X):
    <span class="org-keyword">return</span> np.maximum(X, 0)

<span class="org-variable-name">x</span>=np.arange(-10,10,0.1)
plt.plot(x,relu(x))
plt.show()
</pre>
</div>


<div id="org0000096" class="figure">
<p><img src="machine_learning_files/machine_learning_97_0.png" alt="machine_learning_97_0.png" />
</p>
<p><span class="figure-number">Figure 15: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org000009a" class="outline-4">
<h4 id="org000009a"><span class="section-number-4">1.6.3</span> forward propergation</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
以 mnist 为例，
</p>

<ol class="org-ol">
<li>假设 ann 有一个 hidden layer, 该 layer 有三个 node</li>

<li>input layer 有 784 个 feature</li>

<li>output layer 有 10 个 node, 对应 0~9 十个数字</li>

<li>hidden layer 使用 sigmoid 作为 activation function</li>

<li>output layer 使用 softmax + cross\_entropy 做为 cost function</li>
</ol>

<p>
input layer 与 hidden layer 之间的 W 记为 W1, 其 shape 为 [784, 3], 其中
W1[:,0] 对应 hidden layer 的第一个节点
</p>

<p>
hidden layer 与 output layer 之间的 W 记为 W2, 其 shape 为 [3,10], 其中
W2[:0] 对应 output layer 第一个节点
</p>

<p>
<i>forward propergation</i>
</p>

<p>
\(c^{1}=W^{1}X+B^{1}\)
</p>

<p>
\(a^{1}=sigmoid(c^{1})\)
</p>

<p>
\(c^{2}=W^{2}*a^{1}+B^{2}\)
</p>

<p>
\(\hat{y}=softmax(c^{2})\)
</p>

<p>
\(cost=cross\_entropy(\hat{y},y)\)
</p>
</div>
</div>

<div id="outline-container-org000009d" class="outline-4">
<h4 id="org000009d"><span class="section-number-4">1.6.4</span> backward propergation</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
需要分别计算:
</p>

<ul class="org-ul">
<li>\(\frac{\partial}{\partial{W1}}J(W1,W2,B1,B2)\)</li>
<li>\(\frac{\partial}{\partial{B1}}J(W1,W2,B1,B2)\)</li>
<li>\(\frac{\partial}{\partial{W2}}J(W1,W2,B1,B2)\)</li>
<li>\(\frac{\partial}{\partial{B2}}J(W1,W2,B1,B2)\)</li>
</ul>

<p>
\(\frac{\partial}{\partial{W2}}J(W1,W2,B1,B2)=\frac{1}{m}\sum(\hat{y}-y)a1\)
</p>

<p>
\(\frac{\partial}{\partial{B2}}J(W1,W2,B1,B2)=\frac{1}{m}\sum(\hat{y}-y)\)
</p>

<p>
<i>backward propergation</i>
</p>

<p>
\(\frac{\partial}{\partial{W1}}J(W1,W2,B1,B2)= \frac{\partial}{\partial{a1}}J(W1,W2,B1,B2) * \frac{\partial}{\partial{W1}}a{1}\)
</p>

<p>
\(\frac{\partial}{\partial{W1}}a1=a1*(1-a1)*x\)
</p>

<p>
\(\frac{\partial}{\partial{B1}}J(W1,W2,B1,B2)= \frac{\partial}{\partial{a1}}J(W1,W2,B1,B2) * \frac{\partial}{\partial{B1}}a{1}\)
</p>

<p>
\(\frac{\partial}{\partial{B1}}a1=a1*(1-a1)\)
</p>
</div>
</div>

<div id="outline-container-org00000a2" class="outline-4">
<h4 id="org00000a2"><span class="section-number-4">1.6.5</span> make_moon ANN example</h4>
<div class="outline-text-4" id="text-1-6-5">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> sys
<span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> matplotlib.colors <span class="org-keyword">import</span> ListedColormap
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_moons

<span class="org-variable-name">HIDDEN_NODES_NUM</span> = 30
<span class="org-variable-name">LEARNING_RATE</span> = 0.01
<span class="org-variable-name">EPOCH</span> = 2000
<span class="org-variable-name">BATCH_SIZE</span> = 100

<span class="org-keyword">def</span> <span class="org-function-name">relu</span>(X):
    <span class="org-keyword">return</span> np.maximum(X, 0)

<span class="org-keyword">def</span> <span class="org-function-name">relu_derivative</span>(X):
    <span class="org-keyword">return</span> 1. * (X &gt; 0)

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
    <span class="org-keyword">return</span> 1 / (1 + np.exp(-x))

<span class="org-keyword">def</span> <span class="org-function-name">softmax</span>(z):
    <span class="org-keyword">assert</span> <span class="org-builtin">len</span>(z.shape) == 2
    <span class="org-variable-name">s</span> = np.<span class="org-builtin">max</span>(z, axis=1)
    <span class="org-variable-name">s</span> = s[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">necessary step to do broadcasting</span>
    <span class="org-variable-name">e_x</span> = np.exp(z - s)
    <span class="org-variable-name">div</span> = np.<span class="org-builtin">sum</span>(e_x, axis=1)
    <span class="org-variable-name">div</span> = div[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">dito</span>
    <span class="org-keyword">return</span> e_x / div

<span class="org-keyword">def</span> <span class="org-function-name">cross_entropy</span>(predictions, labels):
    <span class="org-variable-name">epsilon</span> = 1e-12
    <span class="org-variable-name">predictions</span> = np.clip(predictions, epsilon, 1 - epsilon)
    <span class="org-variable-name">N</span> = predictions.shape[0]

    <span class="org-variable-name">ce</span> = 0 - np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(labels * np.log(predictions)))/N
    <span class="org-keyword">return</span> ce

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W1, B1, W2, B2):
    <span class="org-comment-delimiter"># </span><span class="org-comment">X:   m   *  2</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">Y:   m   *  1</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">W1:  2   *  HIDDEN_NODES_NUM</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">B1:  1   *  HIDDEN_NODES_NUM</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">W2:  HIDDEN_NODES_NUM   *  1</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">B2:  1   *  1</span>

    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-variable-name">J</span> = 0.
    <span class="org-variable-name">dw1</span> = np.zeros_like(W1)
    <span class="org-variable-name">db1</span> = np.zeros_like(B1)
    <span class="org-variable-name">dw2</span> = np.zeros_like(W2)
    <span class="org-variable-name">db2</span> = np.zeros_like(B2)
    <span class="org-comment-delimiter"># </span><span class="org-comment">for i in range(m):</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">x = X[i, :].reshape(1, 2)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y = Y[i, :].reshape(1, 1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">z1 = np.matmul(x, W1) + B1</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">a1 = relu(z1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">z2 = np.matmul(a1, W2) + B2</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y_hat = sigmoid(z2)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">loss = cross_entropy(</span>
    <span class="org-comment-delimiter">#            </span><span class="org-comment">np.concatenate((y_hat, 1 - y_hat)), np.concatenate((y, 1 - y)))</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">J += loss</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment"># bp</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">delta3 = y_hat - y</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw2 += np.matmul(np.transpose(a1), delta3)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db2 += delta3</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment"># 1*HIDDEN_NODES_NUM</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">delta2 = np.matmul(delta3, np.transpose(W2))</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">delta2 = delta2 * relu_derivative(a1)</span>

    <span class="org-comment-delimiter">#     </span><span class="org-comment"># dw1: 784 * HIDDEN_NODES_NUM</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw1 += np.matmul(np.transpose(x), delta2)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment"># db1: 1 * HIDDEN_NODES_NUM</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db1 += delta2</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">J /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw1 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db1 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw2 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db2 /= m</span>

    <span class="org-variable-name">z1</span> = np.matmul(X, W1) + B1
    <span class="org-variable-name">a1</span> = relu(z1)
    <span class="org-variable-name">z2</span> = np.matmul(a1, W2) + B2
    <span class="org-variable-name">y_hat</span> = sigmoid(z2)

    <span class="org-variable-name">J</span> = cross_entropy(
         np.c_[y_hat,1-y_hat],np.c_[Y,1-Y])

    <span class="org-comment-delimiter"># </span><span class="org-comment">backward propergation</span>
    <span class="org-variable-name">delta3</span> = y_hat - Y
    <span class="org-variable-name">dw2</span> = np.matmul(np.transpose(a1), delta3) / m
    <span class="org-variable-name">db2</span> = delta3.mean(axis=0)

    <span class="org-variable-name">delta2</span> = np.matmul(delta3, np.transpose(W2))
    <span class="org-variable-name">delta2</span> = delta2 * relu_derivative(a1)

    <span class="org-variable-name">dw1</span> = np.matmul(np.transpose(X), delta2) / m
    <span class="org-variable-name">db1</span> = delta2.mean(axis=0)
    <span class="org-keyword">return</span> J, dw1, db1, dw2, db2

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = make_moons(n_samples=1000, noise=0.2)
    <span class="org-variable-name">Y</span> = Y.reshape(1000, 1)
    <span class="org-variable-name">X</span> = preprocessing.scale(X)
    <span class="org-keyword">return</span> X, Y

<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W1, B1, W2, B2):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">batch</span> = <span class="org-builtin">len</span>(X) // BATCH_SIZE
        <span class="org-variable-name">total_loss</span> = 0
        <span class="org-keyword">for</span> X_batch, Y_batch <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(
                np.split(X[:batch * BATCH_SIZE], batch),
                np.split(Y[:batch * BATCH_SIZE], batch)):
            <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw1</span>, <span class="org-variable-name">db1</span>, <span class="org-variable-name">dw2</span>, <span class="org-variable-name">db2</span> = cost_function(X_batch, Y_batch, W1, B1,
                                                     W2, B2)
            <span class="org-comment-delimiter"># </span><span class="org-comment">print("training: #", epoch, cost)</span>
            <span class="org-variable-name">total_loss</span> += cost
            <span class="org-variable-name">W1</span> = W1 - alpha * dw1
            <span class="org-variable-name">B1</span> = B1 - alpha * db1
            <span class="org-variable-name">W2</span> = W2 - alpha * dw2
            <span class="org-variable-name">B2</span> = B2 - alpha * db2
        <span class="org-keyword">if</span> epoch % (EPOCH // 30) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, total_loss / batch)
    <span class="org-keyword">return</span> W1, B1, W2, B2

<span class="org-keyword">def</span> <span class="org-function-name">draw_decision_boundary</span>(W1, B1, W2, B2):
    <span class="org-variable-name">xx</span>, <span class="org-variable-name">yy</span> = np.meshgrid(np.arange(-4, 4, 0.02), np.arange(-4, 4, 0.02))
    <span class="org-variable-name">X</span> = np.c_[xx.ravel(), yy.ravel()]

    <span class="org-variable-name">z1</span> = np.dot(X, W1) + B1
    <span class="org-variable-name">a1</span> = relu(z1)
    <span class="org-variable-name">z2</span> = np.dot(a1, W2) + B2
    <span class="org-variable-name">a2</span> = sigmoid(z2)
    <span class="org-variable-name">y_hat</span> = (a2 &gt; 0.5)[:, 0]
    <span class="org-variable-name">y_hat</span> = y_hat.reshape(xx.shape)
    <span class="org-variable-name">cm</span> = ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
    plt.contour(xx, yy, y_hat, cmap=cm)

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> W1, B1, W2, B2
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = get_training_set()
    <span class="org-variable-name">Z</span> = np.concatenate((X, Y), axis=1)
    np.random.shuffle(Z)
    <span class="org-variable-name">X</span> = Z[:, :2]
    <span class="org-variable-name">Y</span> = Z[:, 2:]
    <span class="org-variable-name">W1</span> = np.random.randn(2, HIDDEN_NODES_NUM)
    <span class="org-variable-name">B1</span> = np.random.randn(1, HIDDEN_NODES_NUM)
    <span class="org-variable-name">W2</span> = np.random.randn(HIDDEN_NODES_NUM, 1)
    <span class="org-variable-name">B2</span> = np.random.randn(1, 1)
    <span class="org-variable-name">W1</span>, <span class="org-variable-name">B1</span>, <span class="org-variable-name">W2</span>, <span class="org-variable-name">B2</span> = gradient_decent(X, Y, W1, B1, W2, B2)

<span class="org-variable-name">W1</span>, <span class="org-variable-name">B1</span>, <span class="org-variable-name">W2</span>, <span class="org-variable-name">B2</span> = 0, 0, 0, 0

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> W1, B1, W2, B2
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = get_training_set()
    <span class="org-variable-name">cm</span> = ListedColormap([<span class="org-string">'#FF0000'</span>, <span class="org-string">'#0000FF'</span>])
    plt.scatter(x=X[:, 0], y=X[:, 1], c=Y[:, 0], cmap=cm)
    draw_decision_boundary(W1, B1, W2, B2)
    plt.show()

train()
predict()
</pre>
</div>

<pre class="example" id="org00000a0">
training: # 0 1.1010464305437424
training: # 66 0.26612700400246736
training: # 132 0.2279739563132131
training: # 198 0.205101820126328
training: # 264 0.1884051410770355
training: # 330 0.17477153275449023
training: # 396 0.16369062967529763
training: # 462 0.1545121735063119
training: # 528 0.14688858135821387
training: # 594 0.14037311583041678
training: # 660 0.13481409437384884
training: # 726 0.1299481748662618
training: # 792 0.1256549732023355
training: # 858 0.1218073002961437
training: # 924 0.11833164864213408
training: # 990 0.11520153425399286
training: # 1056 0.11239083604018733
training: # 1122 0.10985753265561604
training: # 1188 0.10755024891442373
training: # 1254 0.10549406656678581
training: # 1320 0.10362753609096016
training: # 1386 0.10193490774925815
training: # 1452 0.10040633693099095
training: # 1518 0.09902660303283958
training: # 1584 0.09776264737376265
training: # 1650 0.09658752873463469
training: # 1716 0.09549979433826702
training: # 1782 0.09442971652084056
training: # 1848 0.09343720322848759
training: # 1914 0.09254972524996466
training: # 1980 0.09171308756241331
</pre>


<div id="org00000a1" class="figure">
<p><img src="machine_learning_files/machine_learning_107_1.png" alt="machine_learning_107_1.png" />
</p>
<p><span class="figure-number">Figure 16: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org00000a6" class="outline-4">
<h4 id="org00000a6"><span class="section-number-4">1.6.6</span> gradient checking</h4>
<div class="outline-text-4" id="text-1-6-6">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">def</span> <span class="org-function-name">check_gradient</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = get_training_set()

    <span class="org-variable-name">W1</span> = np.random.randn(2, HIDDEN_NODES_NUM)
    <span class="org-variable-name">B1</span> = np.random.randn(1, HIDDEN_NODES_NUM)
    <span class="org-variable-name">W2</span> = np.random.randn(HIDDEN_NODES_NUM, 1)
    <span class="org-variable-name">B2</span> = np.random.randn(1, 1)

    <span class="org-variable-name">cost</span>,<span class="org-variable-name">dw1</span>,<span class="org-variable-name">db1</span>,<span class="org-variable-name">dw2</span>,<span class="org-variable-name">db2</span>=cost_function(X,Y,W1,B1,W2,B2)
    <span class="org-variable-name">W1</span>[0,0]+=1e-3
    <span class="org-variable-name">cost2</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>=cost_function(X,Y,W1,B1,W2,B2)
    <span class="org-keyword">print</span>((cost2-cost)/1e-3,dw1[0,0])
    <span class="org-variable-name">W1</span>[0,0]-=1e-3

    <span class="org-variable-name">W2</span>[0,0]+=1e-3
    <span class="org-variable-name">cost2</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>,<span class="org-variable-name">_</span>=cost_function(X,Y,W1,B1,W2,B2)
    <span class="org-keyword">print</span>((cost2-cost)/1e-3,dw2[0,0])
    <span class="org-variable-name">W2</span>[0,0]-=1e-3

check_gradient()
</pre>
</div>

<pre class="example" id="org00000a5">
-0.11087431749534638 -0.11087971896090518
0.2626766888012 0.2626112969246978
</pre>
</div>
</div>

<div id="outline-container-org00000ab" class="outline-4">
<h4 id="org00000ab"><span class="section-number-4">1.6.7</span> ANN underfitting</h4>
<div class="outline-text-4" id="text-1-6-7">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">HIDDEN_NODES_NUM</span> = 2
train()
predict()
</pre>
</div>

<pre class="example" id="org00000a9">
training: # 0 0.2648734052642972
training: # 66 0.19051075781763988
training: # 132 0.16981209881990778
training: # 198 0.1601615756297182
training: # 264 0.15477825811739088
training: # 330 0.15119051277788853
training: # 396 0.14873291511906206
training: # 462 0.1469816534584579
training: # 528 0.14558059343098362
training: # 594 0.14465389497268938
training: # 660 0.14397394615231032
training: # 726 0.14345675216822246
training: # 792 0.1429674772570023
training: # 858 0.14263718647940898
training: # 924 0.14242584431954342
training: # 990 0.14230632971907475
training: # 1056 0.142229241092
training: # 1122 0.1421798949537487
training: # 1188 0.14214490864219884
training: # 1254 0.14210683414275965
training: # 1320 0.14206771306265004
training: # 1386 0.14201645856299278
training: # 1452 0.14195097849497493
training: # 1518 0.1419079606039002
training: # 1584 0.14188212493955574
training: # 1650 0.14184885602018105
training: # 1716 0.14181514251831864
training: # 1782 0.14177638752625493
training: # 1848 0.14173627526074
training: # 1914 0.14171024414399375
training: # 1980 0.1416899151978159
</pre>


<div id="org00000aa" class="figure">
<p><img src="machine_learning_files/machine_learning_111_1.png" alt="machine_learning_111_1.png" />
</p>
<p><span class="figure-number">Figure 17: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org00000b0" class="outline-4">
<h4 id="org00000b0"><span class="section-number-4">1.6.8</span> ANN overfitting</h4>
<div class="outline-text-4" id="text-1-6-8">
<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">HIDDEN_NODES_NUM</span> = 500
train()
predict()
</pre>
</div>

<pre class="example" id="org00000ae">
training: # 0 2.0567161790524358
training: # 66 0.04604867297108463
training: # 132 0.03752036254177539
training: # 198 0.034219530144220814
training: # 264 0.03230772048803781
training: # 330 0.030934205053510792
training: # 396 0.02990669936277099
training: # 462 0.029094081507601677
training: # 528 0.028367062027028496
training: # 594 0.027774583043645072
training: # 660 0.027274793841699875
training: # 726 0.026865490017476307
training: # 792 0.026495695363192913
training: # 858 0.026169749917253328
training: # 924 0.02587170841831545
training: # 990 0.025602493238891168
training: # 1056 0.02536622820636284
training: # 1122 0.025144524174879168
training: # 1188 0.02492772562338179
training: # 1254 0.02471288682522171
training: # 1320 0.024507402054118534
training: # 1386 0.024313735181501994
training: # 1452 0.024136089408418184
training: # 1518 0.02397675666435648
training: # 1584 0.0238277056608206
training: # 1650 0.02367850858998746
training: # 1716 0.023515055913084094
training: # 1782 0.02335790705150862
training: # 1848 0.02323471795470979
training: # 1914 0.023123359844330806
training: # 1980 0.023017238312910197
</pre>


<div id="org00000af" class="figure">
<p><img src="machine_learning_files/machine_learning_113_1.png" alt="machine_learning_113_1.png" />
</p>
<p><span class="figure-number">Figure 18: </span>png</p>
</div>
</div>
</div>

<div id="outline-container-org00000b4" class="outline-4">
<h4 id="org00000b4"><span class="section-number-4">1.6.9</span> load_digits ANN example</h4>
<div class="outline-text-4" id="text-1-6-9">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> sys
<span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> preprocessing
<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> load_digits

<span class="org-variable-name">HIDDEN_NODES_NUM</span> = 30
<span class="org-variable-name">LEARNING_RATE</span> = 0.01
<span class="org-variable-name">EPOCH</span> = 100
<span class="org-variable-name">BATCH_SIZE</span> = 2
<span class="org-variable-name">REGULARIZATION_FACTOR</span> = 0.01
<span class="org-variable-name">USE_RELU</span> = <span class="org-constant">False</span>

<span class="org-keyword">def</span> <span class="org-function-name">relu</span>(X):
    <span class="org-keyword">return</span> np.maximum(X, 0)

<span class="org-keyword">def</span> <span class="org-function-name">relu_derivative</span>(X):
    <span class="org-keyword">return</span> 1. * (X &gt; 0)

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
    <span class="org-keyword">return</span> 1 / (1 + np.exp(-x))

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid_derivative</span>(X):
    <span class="org-keyword">return</span> X * (1 - X)

<span class="org-keyword">def</span> <span class="org-function-name">softmax</span>(z):
    <span class="org-keyword">assert</span> <span class="org-builtin">len</span>(z.shape) == 2
    <span class="org-variable-name">s</span> = np.<span class="org-builtin">max</span>(z, axis=1)
    <span class="org-variable-name">s</span> = s[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">necessary step to do broadcasting</span>
    <span class="org-variable-name">e_x</span> = np.exp(z - s)
    <span class="org-variable-name">div</span> = np.<span class="org-builtin">sum</span>(e_x, axis=1)
    <span class="org-variable-name">div</span> = div[:, np.newaxis]  <span class="org-comment-delimiter"># </span><span class="org-comment">dito</span>
    <span class="org-keyword">return</span> e_x / div

<span class="org-keyword">def</span> <span class="org-function-name">one_hot</span>(Y, C):
    <span class="org-variable-name">Y</span> = np.eye(C)[Y.reshape(-1)]
    <span class="org-keyword">return</span> Y

<span class="org-keyword">def</span> <span class="org-function-name">cross_entropy</span>(predictions, labels):
    <span class="org-variable-name">epsilon</span> = 1e-12
    <span class="org-variable-name">predictions</span> = np.clip(predictions, epsilon, 1 - epsilon)
    <span class="org-variable-name">N</span> = predictions.shape[0]

    <span class="org-variable-name">ce</span> = 0 - np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(labels * np.log(predictions))) / N
    <span class="org-keyword">return</span> ce

<span class="org-keyword">def</span> <span class="org-function-name">cost_function</span>(X, Y, W1, B1, W2, B2):
    <span class="org-variable-name">m</span> = <span class="org-builtin">len</span>(X)
    <span class="org-keyword">assert</span> (<span class="org-builtin">len</span>(X) == <span class="org-builtin">len</span>(Y))

    <span class="org-variable-name">J</span> = 0.

    <span class="org-variable-name">dw1</span> = np.zeros_like(W1)
    <span class="org-variable-name">db1</span> = np.zeros_like(B1)

    <span class="org-variable-name">dw2</span> = np.zeros_like(W2)
    <span class="org-variable-name">db2</span> = np.zeros_like(B2)
    <span class="org-comment-delimiter"># </span><span class="org-comment">normal</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">for i in range(m):</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">x = X[i, :].reshape(1, -1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y = Y[i, :].reshape(1, -1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">z1 = np.matmul(x, W1) + B1</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">if USE_RELU:</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">a1 = relu(z1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">else:</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">a1 = sigmoid(z1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">z2 = np.matmul(a1, W2) + B2</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">y_hat = softmax(z2)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">loss = cross_entropy(y_hat, y)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">J += loss</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment"># bp</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">delta3 = y_hat - y</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw2 += np.matmul(np.transpose(a1), delta3)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db2 += delta3</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">delta2 = np.matmul(delta3, np.transpose(W2))</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">if USE_RELU:</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">delta2 = delta2 * relu_derivative(a1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">else:</span>
    <span class="org-comment-delimiter">#         </span><span class="org-comment">delta2 = delta2 * sigmoid_derivative(a1)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">dw1 += np.matmul(np.transpose(x), delta2)</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">db1 += delta2</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">J += REGULARIZATION_FACTOR * (</span>
    <span class="org-comment-delimiter">#     </span><span class="org-comment">np.sum(np.sum(np.square(W1))) + np.sum(np.sum(np.square(W2)))) / 2</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw1 += REGULARIZATION_FACTOR * W1</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw2 += REGULARIZATION_FACTOR * W2</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">J /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw1 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db1 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">dw2 /= m</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">db2 /= m</span>

    <span class="org-comment-delimiter"># </span><span class="org-comment">vectorization</span>
    <span class="org-variable-name">z1</span> = np.matmul(X, W1) + B1
    <span class="org-keyword">if</span> USE_RELU:
        <span class="org-variable-name">a1</span> = relu(z1)
    <span class="org-keyword">else</span>:
        <span class="org-variable-name">a1</span> = sigmoid(z1)
    <span class="org-variable-name">z2</span> = np.matmul(a1, W2) + B2
    <span class="org-variable-name">y_hat</span> = softmax(z2)
    <span class="org-variable-name">J</span> = cross_entropy(y_hat, Y)

    <span class="org-variable-name">delta3</span> = y_hat - Y
    <span class="org-variable-name">dw2</span> = np.matmul(np.transpose(a1), delta3) / m
    <span class="org-variable-name">db2</span> = delta3.mean(axis=0)

    <span class="org-variable-name">delta2</span> = np.matmul(delta3, np.transpose(W2))
    <span class="org-keyword">if</span> USE_RELU:
        <span class="org-variable-name">delta2</span> = delta2 * relu_derivative(a1)
    <span class="org-keyword">else</span>:
        <span class="org-variable-name">delta2</span> = delta2 * sigmoid_derivative(a1)

    <span class="org-variable-name">dw1</span> = np.matmul(np.transpose(X), delta2) / m
    <span class="org-variable-name">db1</span> = delta2.mean(axis=0)

    <span class="org-comment-delimiter"># </span><span class="org-comment">regulation</span>
    <span class="org-variable-name">J</span> += REGULARIZATION_FACTOR * (
        np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(np.square(W1))) + np.<span class="org-builtin">sum</span>(np.<span class="org-builtin">sum</span>(np.square(W2)))) / (
            2 * m)
    <span class="org-variable-name">dw1</span> += REGULARIZATION_FACTOR * W1 / m
    <span class="org-variable-name">dw2</span> += REGULARIZATION_FACTOR * W2 / m

    <span class="org-keyword">return</span> J, dw1, db1, dw2, db2

<span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span>, <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = 0, 0, 0, 0
<span class="org-variable-name">W1</span>, <span class="org-variable-name">B1</span>, <span class="org-variable-name">W2</span>, <span class="org-variable-name">B2</span> = 0, 0, 0, 0

<span class="org-keyword">def</span> <span class="org-function-name">get_training_set</span>():
    <span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = load_digits(10, <span class="org-constant">True</span>)
    <span class="org-variable-name">Y</span> = Y.reshape(-1, 1)
    <span class="org-variable-name">Y</span> = one_hot(Y, 10)
    [m, features] = X.shape
    <span class="org-variable-name">Z</span> = np.concatenate((X, Y), axis=1)
    np.random.shuffle(Z)
    <span class="org-variable-name">X</span> = Z[:, :features]
    <span class="org-variable-name">Y</span> = Z[:, features:]
    <span class="org-keyword">global</span> X_train, Y_train, X_test, Y_test
    <span class="org-variable-name">offset</span> = <span class="org-builtin">int</span>(0.8 * m)
    <span class="org-variable-name">X_train</span>, <span class="org-variable-name">Y_train</span> = X[:offset], Y[:offset]
    <span class="org-variable-name">X_test</span>, <span class="org-variable-name">Y_test</span> = X[offset:], Y[offset:]

<span class="org-keyword">def</span> <span class="org-function-name">gradient_decent</span>(X, Y, W1, B1, W2, B2):
    <span class="org-variable-name">alpha</span> = LEARNING_RATE
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-variable-name">batch</span> = <span class="org-builtin">len</span>(X) // BATCH_SIZE
        <span class="org-variable-name">total_loss</span> = 0
        <span class="org-keyword">for</span> X_batch, Y_batch <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(
                np.split(X[:batch * BATCH_SIZE], batch),
                np.split(Y[:batch * BATCH_SIZE], batch)):
            <span class="org-variable-name">cost</span>, <span class="org-variable-name">dw1</span>, <span class="org-variable-name">db1</span>, <span class="org-variable-name">dw2</span>, <span class="org-variable-name">db2</span> = cost_function(X_batch, Y_batch, W1, B1,
                                                     W2, B2)
            <span class="org-variable-name">total_loss</span> += cost
            <span class="org-variable-name">W1</span> = W1 - alpha * dw1
            <span class="org-variable-name">B1</span> = B1 - alpha * db1
            <span class="org-variable-name">W2</span> = W2 - alpha * dw2
        <span class="org-keyword">if</span> epoch % (EPOCH // 20) == 0:
            <span class="org-keyword">print</span>(<span class="org-string">"training: #"</span>, epoch, total_loss / batch)

    <span class="org-keyword">return</span> W1, B1, W2, B2

<span class="org-keyword">def</span> <span class="org-function-name">predict</span>():
    <span class="org-keyword">global</span> X_test, Y_test, W1, B1, W2, B2

    <span class="org-variable-name">wrong</span> = 0
    <span class="org-variable-name">correct</span> = 0
    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(<span class="org-builtin">len</span>(X_test)):
        <span class="org-variable-name">x</span> = X_test[i, :].reshape(1, -1)
        <span class="org-variable-name">y</span> = Y_test[i, :].reshape(1, -1)
        <span class="org-variable-name">z1</span> = np.matmul(x, W1) + B1
        <span class="org-keyword">if</span> USE_RELU:
            <span class="org-variable-name">a1</span> = relu(z1)
        <span class="org-keyword">else</span>:
            <span class="org-variable-name">a1</span> = sigmoid(z1)
        <span class="org-variable-name">z2</span> = np.matmul(a1, W2) + B2
        <span class="org-variable-name">a2</span> = softmax(z2)

        <span class="org-keyword">if</span> np.argmax(a2) == np.argmax(y):
            <span class="org-variable-name">correct</span> += 1
        <span class="org-keyword">else</span>:
            <span class="org-variable-name">wrong</span> += 1
    <span class="org-keyword">print</span>(<span class="org-string">"correct: %d, wrong: %d, accuracy: %f"</span> % (correct, wrong, correct /
                                                    (correct + wrong)))

<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-keyword">global</span> W1, B2, W2, B2, X_train, Y_train
    <span class="org-variable-name">W1</span> = np.random.randn(64, HIDDEN_NODES_NUM)
    <span class="org-variable-name">B1</span> = np.random.randn(1, HIDDEN_NODES_NUM)

    <span class="org-variable-name">W2</span> = np.random.randn(HIDDEN_NODES_NUM, 10)
    <span class="org-variable-name">B2</span> = np.random.randn(1, 10)

    <span class="org-variable-name">W1</span>, <span class="org-variable-name">B1</span>, <span class="org-variable-name">W2</span>, <span class="org-variable-name">B2</span> = gradient_decent(X_train, Y_train, W1, B1, W2, B2)

get_training_set()
train()
predict()
</pre>
</div>

<pre class="example" id="org00000b3">
training: # 0 8.179987275868209
training: # 5 4.487616756607857
training: # 10 3.190552810756847
training: # 15 2.3609422313800867
training: # 20 1.7760653062722396
training: # 25 1.3739858353803298
training: # 30 1.0825991991172912
training: # 35 0.8802306595921261
training: # 40 0.7309049938786311
training: # 45 0.6226726072585916
training: # 50 0.5489742276092043
training: # 55 0.49740708896607166
training: # 60 0.46216408821398436
training: # 65 0.43638596801652746
training: # 70 0.4120282356053934
training: # 75 0.3927758648862652
training: # 80 0.3769661143322518
training: # 85 0.36297691382952496
training: # 90 0.3523873696978746
training: # 95 0.34356971806469333
correct: 350, wrong: 10, accuracy: 0.972222
</pre>
</div>
</div>
</div>

<div id="outline-container-org00000ba" class="outline-3">
<h3 id="org00000ba"><span class="section-number-3">1.7</span> What's Next</h3>
<div class="outline-text-3" id="text-1-7">
<ul class="org-ul">
<li>Convolutional neural networks (ConvNets)</li>
<li>Recurrent neural networks</li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: <br />
Last updated: 2022-01-25 Tue 16:14</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
