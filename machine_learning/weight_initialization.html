<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Weight Initialization</title>


           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/htmlize.css"/>
           <link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/css/readtheorg.css"/>
           <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
           <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
           <script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
           <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
           <link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
           <link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
           <link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Weight Initialization</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org000001c">1. Weight Initialization</a>
<ul>
<li><a href="#org0000000">1.1. 初始化为相同的值</a></li>
<li><a href="#org0000004">1.2. 初始化为随机值</a></li>
<li><a href="#org0000008">1.3. Xavier initialization</a></li>
<li><a href="#org000000d">1.4. He initialization</a></li>
<li><a href="#org0000010">1.5. Batch Normalization</a></li>
<li><a href="#org0000019">1.6. 不适当的权重初始化导致训练无法收敛</a>
<ul>
<li><a href="#org0000013">1.6.1. 加上 bias</a></li>
<li><a href="#org0000016">1.6.2. 加上 activation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org000001c" class="outline-2">
<h2 id="org000001c"><span class="section-number-2">1.</span> Weight Initialization</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000000" class="outline-3">
<h3 id="org0000000"><span class="section-number-3">1.1.</span> 初始化为相同的值</h3>
<div class="outline-text-3" id="text-1-1">
<p>
若权重被初始化为相同的值, 或者更具体的, 被初始化为相同的列, 而所有神经元会一直有相同的输出, 导致神经元失去意义
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-10-14 22:36</span>
<span class="org-keyword">import</span> torch

<span class="org-variable-name">N</span> = 3


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = torch.nn.Parameter(
            torch.randn(out_features).repeat(in_features, 1).transpose(0, 1)
        )

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(<span class="org-string">"------"</span>)
        <span class="org-builtin">print</span>(<span class="org-builtin">input</span>)
        <span class="org-builtin">print</span>(<span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(<span class="org-keyword">self</span>.w.grad)
        <span class="org-builtin">print</span>(ret)
        <span class="org-builtin">print</span>(<span class="org-string">"------"</span>)
        <span class="org-keyword">return</span> ret


<span class="org-variable-name">net</span> = torch.nn.Sequential()
net.add_module(<span class="org-string">"linear"</span>, Layer(N, N))

<span class="org-variable-name">optimizer</span> = torch.optim.SGD(net.parameters(), lr=1)
<span class="org-variable-name">criterion</span> = torch.nn.MSELoss()

<span class="org-variable-name">x</span> = torch.rand(1, N)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(3):
    <span class="org-builtin">print</span>(<span class="org-string">"epoch: %d"</span> % (i))
    <span class="org-variable-name">out</span> = net(x)
    optimizer.zero_grad()
    <span class="org-variable-name">loss</span> = criterion(out, torch.ones((1, N)))
    loss.backward()
    optimizer.step()
</pre>
</div>

<p>
epoch: 0
</p>
<hr />
<p>
tensor()
Parameter containing:
tensor([[ 2.2574,  2.2574,  2.2574],
        [-1.2911, -1.2911, -1.2911],
        [-1.2469, -1.2469, -1.2469]], requires_grad=True)
None
tensor(, grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
<p>
epoch: 1
</p>
<hr />
<p>
tensor()
Parameter containing:
tensor([[ 2.5595,  2.5595,  2.5595],
        [-1.1240, -1.1240, -1.1240],
        [-0.5409, -0.5409, -0.5409]], requires_grad=True)
tensor([[-0.3021, -0.3021, -0.3021],
        [-0.1671, -0.1671, -0.1671],
        [-0.7060, -0.7060, -0.7060]])
tensor(, grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
<p>
epoch: 2
</p>
<hr />
<p>
tensor()
Parameter containing:
tensor([[ 2.7254,  2.7254,  2.7254],
        [-1.0322, -1.0322, -1.0322],
        [-0.1532, -0.1532, -0.1532]], requires_grad=True)
tensor([[-0.1659, -0.1659, -0.1659],
        [-0.0918, -0.0918, -0.0918],
        [-0.3876, -0.3876, -0.3876]])
tensor(, grad_fn=&lt;MmBackward&gt;)
</p>
<hr />

<p>
由于初始权重每列均相同, 导致同一层的神经元的输出都相同, 若 label x 也相同, 则反向传播时同一层神经元的梯度也相同
</p>
</div>
</div>

<div id="outline-container-org0000004" class="outline-3">
<h3 id="org0000004"><span class="section-number-3">1.2.</span> 初始化为随机值</h3>
<div class="outline-text-3" id="text-1-2">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> torch

<span class="org-variable-name">N</span> = 100
plt.style.use(<span class="org-string">"default"</span>)

plt.ylim(0, 40)


<span class="org-keyword">def</span> <span class="org-function-name">init_weight</span>(in_features, out_features):
    <span class="org-keyword">return</span> torch.nn.Parameter(torch.randn(in_features, out_features))


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features, n):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">n</span> = n
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = init_weight(in_features, out_features)

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w)
        <span class="org-variable-name">ret</span> = torch.nn.functional.sigmoid(ret)
        plt.subplot(1, 10, <span class="org-keyword">self</span>.n)
        plt.xlim(-1, 1)
        plt.hist(ret.detach().numpy().reshape(-1))
        <span class="org-keyword">return</span> ret


<span class="org-keyword">def</span> <span class="org-function-name">train</span>():
    <span class="org-variable-name">net</span> = torch.nn.Sequential()

    <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10):
        net.add_module(<span class="org-string">"linear%d"</span> % (i), Layer(N, N, i + 1))

    <span class="org-variable-name">x</span> = torch.rand(1, N)
    net(x)
    plt.show()


train()
</pre>
</div>


<div id="org0000003" class="figure">
<p><img src="../extra/init_weight.png" alt="init_weight.png" />
</p>
</div>

<p>
当权重初始化为 randn (均值为 0, 方差为 1 的正态分布) 时, 各层的输出都在 0 或 1
附近, sigmoid 在 0 或 1 附近时梯度很小, 这时神经元称为饱和状态, 难以梯度下降.
</p>

<p>
sigmoid 饱和是因为计算 \(w^T x\) 时过大(例如大于 1).
</p>

<p>
我们假设最初的输入 x 是经过 scale 的值, 假设范围为 [-1,1], 我们的目标是通过调整
\(w\) 的分布, 使得 \(w^Tx\) 在 [-1,1] 范围内.
</p>

<p>
在上面的例子中, 我们使用 `randn` 初始化 \(w\), 它是一个 \(\mu=0, \delta=1\) 的标准正态分布. 假设每一层网络输入的 x 均为 1, 则 \(w^Tx\) 为 100 个标准正态分布的叠加, 可以证明这个输出是一个 \(\mu=0, \delta=\sqrt{100}\) 的正态分布, 非常容易取到远大于 1 的值.
</p>
</div>
</div>

<div id="outline-container-org0000008" class="outline-3">
<h3 id="org0000008"><span class="section-number-3">1.3.</span> Xavier initialization</h3>
<div class="outline-text-3" id="text-1-3">
<p>
Xavier initialization 基本思想是通过调用 \(w\) 的值使得输入和输出的方差基本一致
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">init_weight</span>(in_features, out_features):
    <span class="org-keyword">return</span> torch.nn.Parameter(
        torch.randn(in_features, out_features) / np.sqrt(in_features))

train()
</pre>
</div>


<div id="org0000007" class="figure">
<p><img src="../extra/xavier.png" alt="xavier.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org000000d" class="outline-3">
<h3 id="org000000d"><span class="section-number-3">1.4.</span> He initialization</h3>
<div class="outline-text-3" id="text-1-4">
<p>
Xavier initialization 对 sigmoid 是有效的, 试试看对 relu 是否有效
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">init_weight</span>(in_features, out_features):
    <span class="org-keyword">return</span> torch.nn.Parameter(torch.randn(in_features, out_features) / np.sqrt(in_features))


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features, n):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">n</span> = n
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = init_weight(in_features, out_features)

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w)
        <span class="org-variable-name">ret</span> = torch.nn.functional.relu(ret)
        plt.subplot(1, 10, <span class="org-keyword">self</span>.n)
        plt.xlim(-1, 1)
        plt.hist(ret.detach().numpy().reshape(-1))
        <span class="org-keyword">return</span> ret

train()
</pre>
</div>


<div id="org000000b" class="figure">
<p><img src="../extra/xavier_relu.png" alt="xavier_relu.png" />
</p>
</div>

<p>
后面的神经元的输出趋向于 0.
</p>

<p>
虽然对于 relu 来说, 不论输出为多少, 梯度都固定为 1, 但由于反向传播时对 w 求偏导时需要乘上对应的 x, 所以 x 趋向于 0 同样会使 w 的梯度很小.
</p>

<p>
He initialization的思想是：在 ReLU 网络中，假定每一层有一半的神经元被激活，另一半为 0，所以，要保持方差不变，只需要在 Xavier 的基础上再除以 2
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">init_weight</span>(in_features, out_features):
    <span class="org-keyword">return</span> torch.nn.Parameter(
        torch.randn(in_features, out_features) / np.sqrt(in_features / 2))


train()
</pre>
</div>


<div id="org000000c" class="figure">
<p><img src="../extra/he_init.png" alt="he_init.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000010" class="outline-3">
<h3 id="org0000010"><span class="section-number-3">1.5.</span> <a href="batchnorm.html#ID-ffa885c6-f5a3-4939-8e61-4ebd2bbbe836">Batch Normalization</a></h3>
</div>

<div id="outline-container-org0000019" class="outline-3">
<h3 id="org0000019"><span class="section-number-3">1.6.</span> 不适当的权重初始化导致训练无法收敛</h3>
<div class="outline-text-3" id="text-1-6">
<p>
以一个三层的网络为例: \(y_1=w_1*x, y_2=w_2*y_1, l=mse(y_2,y)\), 且设置 \(w_1,
w_2\) 均为 0
</p>

<p>
\(\frac{dl}{dw_2}=\frac{dl}{dy_2}*\frac{dy_2}{dw_2}=\frac{dl}{dy_2}*y_1\)
</p>

<p>
\(w_1=0 \implies y_1=0 \implies \frac{dl}{d{w_2}} = 0\)
</p>

<p>
\(\frac{dl}{dw_1}=\frac{dl}{dy_2}*\frac{dy_2}{dy_1}*\frac{dy_1}{dw_1}=\frac{dl}{dy_2}*w_2*\frac{dy_1}{dw_1}\)
</p>

<p>
\(w_2=0 \implies \frac{dl}{dw1} = 0\)
</p>

<p>
导致模型无法收敛
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-10-14 22:36</span>
<span class="org-keyword">import</span> torch

<span class="org-variable-name">N</span> = 3


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features, init_func):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = torch.nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-comment-delimiter"># </span><span class="org-comment">self.w = torch.nn.init.xavier_uniform_(self.w)</span>
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = init_func(<span class="org-keyword">self</span>.w)

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(f<span class="org-string">"------"</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"input:"</span>, <span class="org-builtin">input</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"weight:"</span>, <span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(<span class="org-string">"grad:"</span>, <span class="org-keyword">self</span>.w.grad)
        <span class="org-builtin">print</span>(<span class="org-string">"output:"</span>, ret)
        <span class="org-builtin">print</span>(<span class="org-string">"------"</span>)
        <span class="org-keyword">return</span> ret


<span class="org-variable-name">x</span> = torch.rand(10, N)

<span class="org-keyword">for</span> init_func <span class="org-keyword">in</span> [
    <span class="org-comment-delimiter"># </span><span class="org-comment">torch.nn.init.xavier_uniform_,</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">torch.nn.init.uniform_,</span>
    <span class="org-keyword">lambda</span> x: x,
]:
    <span class="org-builtin">print</span>(init_func.<span class="org-builtin">__name__</span>)
    <span class="org-variable-name">net</span> = torch.nn.Sequential()
    net.add_module(<span class="org-string">"encoder"</span>, Layer(N, 10, init_func))
    net.add_module(<span class="org-string">"decoder"</span>, Layer(10, N, init_func))

    <span class="org-variable-name">optimizer</span> = torch.optim.SGD(net.parameters(), lr=0.01)
    <span class="org-variable-name">criterion</span> = torch.nn.MSELoss()

    <span class="org-variable-name">loss_val</span> = <span class="org-constant">None</span>
    <span class="org-variable-name">EPOCH</span> = 2
    <span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
        <span class="org-builtin">print</span>(f<span class="org-string">"----------epoch: </span>{epoch}<span class="org-string">----------"</span>)
        <span class="org-variable-name">out</span> = net(x)
        optimizer.zero_grad()
        <span class="org-variable-name">loss</span> = criterion(out, x)
        loss.backward()
        optimizer.step()
</pre>
</div>

<p>
&lt;lambda&gt;
-----&#x2013;&#x2014;epoch: 0-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.5559, 0.7492, 0.7514],
        [0.6140, 0.7566, 0.6718],
        [0.1971, 0.1813, 0.5641],
        [0.1271, 0.4532, 0.4344],
        [0.4615, 0.1038, 0.7771],
        [0.5174, 0.9104, 0.3995],
        [0.9099, 0.5128, 0.0951],
        [0.8051, 0.4550, 0.7187],
        [0.6267, 0.9796, 0.4795],
        [0.0192, 0.1095, 0.6044]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
grad: None
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
grad: None
output: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
<p>
-----&#x2013;&#x2014;epoch: 1-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.5559, 0.7492, 0.7514],
        [0.6140, 0.7566, 0.6718],
        [0.1971, 0.1813, 0.5641],
        [0.1271, 0.4532, 0.4344],
        [0.4615, 0.1038, 0.7771],
        [0.5174, 0.9104, 0.3995],
        [0.9099, 0.5128, 0.0951],
        [0.8051, 0.4550, 0.7187],
        [0.6267, 0.9796, 0.4795],
        [0.0192, 0.1095, 0.6044]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
grad: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
grad: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
output: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], grad_fn=&lt;MmBackward&gt;)
</p>
<hr />
</div>

<div id="outline-container-org0000013" class="outline-4">
<h4 id="org0000013"><span class="section-number-4">1.6.1.</span> 加上 bias</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
\(y_1=w_1*x+b_1, y_2=w_2*y_1+b_2, l=mse(y2,y)\)
</p>

<p>
\(\frac{dl}{db_2}=2*(y_2-y)\)
</p>

<p>
\(\frac{dl}{db_1}=2*(y_2-y)*w_2\)
</p>

<p>
所以 \(b_2\) 可以被更新, 但其它参数还是会维持为 0 不变, 因为它们的梯度不依赖 \(b_2\)
或 \(y_2\) 的值
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-10-14 22:36</span>
<span class="org-keyword">import</span> torch

<span class="org-variable-name">N</span> = 3


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = torch.nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-keyword">self</span>.<span class="org-variable-name">b</span> = torch.nn.Parameter(torch.zeros(out_features))

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w) + <span class="org-keyword">self</span>.b
        <span class="org-builtin">print</span>(f<span class="org-string">"------"</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"input:"</span>, <span class="org-builtin">input</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"weight:"</span>, <span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(<span class="org-string">"bias:"</span>, <span class="org-keyword">self</span>.b)
        <span class="org-builtin">print</span>(<span class="org-string">"grad:"</span>, <span class="org-keyword">self</span>.w.grad)
        <span class="org-builtin">print</span>(<span class="org-string">"bias_grad:"</span>, <span class="org-keyword">self</span>.b.grad)
        <span class="org-builtin">print</span>(<span class="org-string">"output:"</span>, ret)
        <span class="org-builtin">print</span>(<span class="org-string">"------"</span>)
        <span class="org-keyword">return</span> ret


<span class="org-variable-name">x</span> = torch.rand(10, N)

<span class="org-variable-name">net</span> = torch.nn.Sequential()
net.add_module(<span class="org-string">"encoder"</span>, Layer(N, 10))
net.add_module(<span class="org-string">"decoder"</span>, Layer(10, N))

<span class="org-variable-name">optimizer</span> = torch.optim.SGD(net.parameters(), lr=0.01)
<span class="org-variable-name">criterion</span> = torch.nn.MSELoss()

<span class="org-variable-name">loss_val</span> = <span class="org-constant">None</span>
<span class="org-variable-name">EPOCH</span> = 2
<span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
    <span class="org-builtin">print</span>(f<span class="org-string">"----------epoch: </span>{epoch}<span class="org-string">----------"</span>)
    <span class="org-variable-name">out</span> = net(x)
    optimizer.zero_grad()
    <span class="org-variable-name">loss</span> = criterion(out, x)
    loss.backward()
    optimizer.step()
</pre>
</div>

<p>
-----&#x2013;&#x2014;epoch: 0-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.0892, 0.0043, 0.1385],
        [0.5969, 0.6326, 0.0483],
        [0.6149, 0.3503, 0.9675],
        [0.8038, 0.6002, 0.6653],
        [0.7709, 0.9145, 0.9097],
        [0.3827, 0.4187, 0.3746],
        [0.4997, 0.8725, 0.9211],
        [0.3268, 0.2441, 0.0907],
        [0.2871, 0.0215, 0.1810],
        [0.0679, 0.7250, 0.1056]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)
grad: None
bias_grad: None
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;AddBackward0&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;AddBackward0&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0.], requires_grad=True)
grad: None
bias_grad: None
output: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], grad_fn=&lt;AddBackward0&gt;)
</p>
<hr />
<p>
-----&#x2013;&#x2014;epoch: 1-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.0892, 0.0043, 0.1385],
        [0.5969, 0.6326, 0.0483],
        [0.6149, 0.3503, 0.9675],
        [0.8038, 0.6002, 0.6653],
        [0.7709, 0.9145, 0.9097],
        [0.3827, 0.4187, 0.3746],
        [0.4997, 0.8725, 0.9211],
        [0.3268, 0.2441, 0.0907],
        [0.2871, 0.0215, 0.1810],
        [0.0679, 0.7250, 0.1056]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)
grad: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
bias_grad: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;AddBackward0&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;AddBackward0&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0.0030, 0.0032, 0.0029], requires_grad=True)
grad: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
bias_grad: tensor([-0.2960, -0.3189, -0.2935])
output: tensor([[0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029],
        [0.0030, 0.0032, 0.0029]], grad_fn=&lt;AddBackward0&gt;)
</p>
<hr />
</div>
</div>

<div id="outline-container-org0000016" class="outline-4">
<h4 id="org0000016"><span class="section-number-4">1.6.2.</span> 加上 activation</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
加上 relu 后对每一层的梯度和输出 (0) 都没有影响, 所以不会有什么改变
</p>

<p>
加上 sigmoid 后对每一层的输出 (0) 都会有影响, 所以各个参数的梯度会发生改变
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-10-14 22:36</span>
<span class="org-keyword">import</span> torch

<span class="org-variable-name">N</span> = 3


<span class="org-keyword">class</span> <span class="org-type">Layer</span>(torch.nn.Module):
    <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, in_features, out_features):
        <span class="org-builtin">super</span>().__init__()
        <span class="org-keyword">self</span>.<span class="org-variable-name">w</span> = torch.nn.Parameter(torch.zeros(in_features, out_features))
        <span class="org-keyword">self</span>.<span class="org-variable-name">b</span> = torch.nn.Parameter(torch.zeros(out_features))

    <span class="org-keyword">def</span> <span class="org-function-name">forward</span>(<span class="org-keyword">self</span>, <span class="org-builtin">input</span>):
        <span class="org-variable-name">ret</span> = torch.matmul(<span class="org-builtin">input</span>, <span class="org-keyword">self</span>.w) + <span class="org-keyword">self</span>.b
        <span class="org-variable-name">ret</span> = torch.relu(ret)
        <span class="org-builtin">print</span>(f<span class="org-string">"------"</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"input:"</span>, <span class="org-builtin">input</span>)
        <span class="org-builtin">print</span>(<span class="org-string">"weight:"</span>, <span class="org-keyword">self</span>.w)
        <span class="org-builtin">print</span>(<span class="org-string">"bias:"</span>, <span class="org-keyword">self</span>.b)
        <span class="org-builtin">print</span>(<span class="org-string">"grad:"</span>, <span class="org-keyword">self</span>.w.grad)
        <span class="org-builtin">print</span>(<span class="org-string">"bias_grad:"</span>, <span class="org-keyword">self</span>.b.grad)
        <span class="org-builtin">print</span>(<span class="org-string">"output:"</span>, ret)
        <span class="org-builtin">print</span>(<span class="org-string">"------"</span>)
        <span class="org-keyword">return</span> ret


<span class="org-variable-name">x</span> = torch.rand(10, N)

<span class="org-variable-name">net</span> = torch.nn.Sequential()
net.add_module(<span class="org-string">"encoder"</span>, Layer(N, 10))
net.add_module(<span class="org-string">"decoder"</span>, Layer(10, N))

<span class="org-variable-name">optimizer</span> = torch.optim.SGD(net.parameters(), lr=0.01)
<span class="org-variable-name">criterion</span> = torch.nn.MSELoss()

<span class="org-variable-name">loss_val</span> = <span class="org-constant">None</span>
<span class="org-variable-name">EPOCH</span> = 2
<span class="org-keyword">for</span> epoch <span class="org-keyword">in</span> <span class="org-builtin">range</span>(EPOCH):
    <span class="org-builtin">print</span>(f<span class="org-string">"----------epoch: </span>{epoch}<span class="org-string">----------"</span>)
    <span class="org-variable-name">out</span> = net(x)
    optimizer.zero_grad()
    <span class="org-variable-name">loss</span> = criterion(out, x)
    loss.backward()
    optimizer.step()
</pre>
</div>

<p>
-----&#x2013;&#x2014;epoch: 0-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.7440, 0.9623, 0.0264],
        [0.1071, 0.7099, 0.7681],
        [0.0640, 0.9640, 0.5351],
        [0.5744, 0.1441, 0.1794],
        [0.9339, 0.7973, 0.6490],
        [0.3939, 0.9950, 0.1150],
        [0.6462, 0.4156, 0.9414],
        [0.1071, 0.8234, 0.0591],
        [0.8225, 0.9480, 0.4412],
        [0.7329, 0.7617, 0.5860]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)
grad: None
bias_grad: None
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0.], requires_grad=True)
grad: None
bias_grad: None
output: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
</p>
<hr />
<p>
-----&#x2013;&#x2014;epoch: 1-----&#x2013;&#x2014;
</p>
<hr />
<p>
input: tensor([[0.7440, 0.9623, 0.0264],
        [0.1071, 0.7099, 0.7681],
        [0.0640, 0.9640, 0.5351],
        [0.5744, 0.1441, 0.1794],
        [0.9339, 0.7973, 0.6490],
        [0.3939, 0.9950, 0.1150],
        [0.6462, 0.4156, 0.9414],
        [0.1071, 0.8234, 0.0591],
        [0.8225, 0.9480, 0.4412],
        [0.7329, 0.7617, 0.5860]])
weight: Parameter containing:
tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)
grad: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
bias_grad: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
output: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
</p>
<hr />
<hr />
<p>
input: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
weight: Parameter containing:
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], requires_grad=True)
bias: Parameter containing:
tensor([0., 0., 0.], requires_grad=True)
grad: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
bias_grad: tensor([0., 0., 0.])
output: tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]], grad_fn=&lt;ReluBackward0&gt;)
</p>
<hr />
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2018-07-26 Thu 00:00<br />
Last updated: 2022-07-17 Sun 11:39</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>
