<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>PCA</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">PCA</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000021">1. PCA</a>
<ul>
<li><a href="#org0000009">1.1. 计算方法</a>
<ul>
<li><a href="#org0000000">1.1.1. eigen decomposition</a></li>
<li><a href="#org0000003">1.1.2. SVD</a></li>
<li><a href="#org0000006">1.1.3. PCA</a></li>
</ul>
</li>
<li><a href="#org0000014">1.2. 实际意义</a>
<ul>
<li><a href="#org000000d">1.2.1. eigen decomposition</a></li>
<li><a href="#org0000011">1.2.2. PCA</a></li>
</ul>
</li>
<li><a href="#org000001e">1.3. 举例</a>
<ul>
<li><a href="#org0000017">1.3.1. 使用 Eigen, SVD 及 sklearn 进行 PCA</a></li>
<li><a href="#org000001b">1.3.2. 使用 PCA 进行特征提取</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000021" class="outline-2">
<h2 id="org0000021"><span class="section-number-2">1</span> PCA</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000009" class="outline-3">
<h3 id="org0000009"><span class="section-number-3">1.1</span> 计算方法</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-org0000000" class="outline-4">
<h4 id="org0000000"><span class="section-number-4">1.1.1</span> eigen decomposition</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
对于一个方阵 A, 存在向量 x 及实数 \(\lambda\), 使得 \(Ax=\lambda x\), 则 \(\lambda\)
为 A 的一个特征值, x 为 A 的一个特征向量.
</p>

<p>
计算时, 令 \(Ax-\lambda x=0 \implies (A-\lambda I)x=0\implies det(A-\lambda I)=0\), 计算出 \(\lambda\), 然后把 \(\lambda\) 代入 \((A-\lambda I)x=0\), 计算出 x
</p>

<p>
例如:
</p>

<p>
\(A=\begin{bmatrix}2&3\\2&1\end{bmatrix}\), \(A-\lambda
I=\begin{bmatrix}2-\lambda&3\\2&1-\lambda\end{bmatrix}\),
</p>

<p>
\(det(A-\lambda I)=(2-\lambda)(1-\lambda)-6=0 \implies \lambda_1=-1, \lambda_2=4\)
</p>

<p>
代入 \(\lambda\) 到 \((A-\lambda I)x=0\):
</p>

<p>
当 \(\lambda_1=-1\) 时, 得到 \(x_1=\begin{bmatrix}1\\-1\end{bmatrix}\)
</p>

<p>
当 \(\lambda_2=4\) 时, 得到 \(x_2=\begin{bmatrix}3\\2\end{bmatrix}\)
</p>

<p>
写成矩阵的形式为 \(A=X\lambda X^{-1}\), 其中
</p>

<p>
\(X=\begin{bmatrix}1&3\\-1&2\end{bmatrix}\), \(\lambda\) 为 \(\begin{bmatrix}-1&0\\0&4\end{bmatrix}\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-variable-name">A</span> = np.matrix([[2, 3], [2, 1]])
<span class="org-variable-name">x</span> = np.matrix([[1, 3], [-1, 2]])
<span class="org-variable-name">lamdas</span> = np.matrix([[-1, 0], [0, 4]])

<span class="org-keyword">print</span>(A.dot(x))
<span class="org-keyword">print</span>(x.dot(lamdas))

<span class="org-keyword">print</span>(np.linalg.eig(A))
</pre>
</div>

<p>
[[-1 12]
 [ 1  8]]
[[-1 12]
 [ 1  8]]
(array([ 4., -1.]), matrix([[ 0.83205029, -0.70710678],
        [ 0.5547002 ,  0.70710678]]))
</p>

<p>
另外, 若 A 为对称矩阵, 则 X 为正交矩阵, 则 \(A=X\lambda X^T\)
</p>
</div>
</div>

<div id="outline-container-org0000003" class="outline-4">
<h4 id="org0000003"><span class="section-number-4">1.1.2</span> SVD</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
若 A 不是方阵, 则无法应用 eigen decomposition, 但可以应用 SVD 使得
</p>

<p>
\(A= U\Sigma V\), 其中:
</p>

<p>
\(A \in R^{m\times n}\), \(U \in R^{m\times q}\), \(\Sigma \in R^{q\times q}\), \(V \in
R^{q\times n}\), \(q=min(m,n)\)
</p>

<p>
其中 \(U, V\) 是正交矩阵, \(\Sigma\) 是对角矩阵.
</p>

<p>
如何确定 \(U, \Sigma, V\):
</p>

<p>
令 \(AA^T=U\Sigma V\cdot V^T\Sigma^T U^T=U\Sigma^{2}U^T\), 这实际是在求 \(AA^T\) 的特征值与特征向量. 由于 \(AA^T\) 是对称矩阵, 所以 \(U\) 是正交矩阵
</p>

<p>
同理, 通过求解 \(A^T A\) 的特征向量可得 \(V\)
</p>

<p>
另外, 实际应用中求解 SVD 并不会使用上述方法, 因为比较慢<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>
</div>
</div>

<div id="outline-container-org0000006" class="outline-4">
<h4 id="org0000006"><span class="section-number-4">1.1.3</span> PCA</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
\(X \in R^{m\times n}\) 是我们原始的数据矩阵, m 个样本, n 个 feature
</p>

<p>
对 \(X^TX\) 进行 eigen decomposition, 使得 \(X^TX=V\lambda V^T\).
</p>

<p>
定义一个变换 \(Y=XV\), 由于 \(Y^TY=V^TX^TXV=\lambda\) 是一个对角矩阵, 所以 Y 所有
feature 都是不相关的. \(\lambda\) 对角线上的值代表了不同 feature 的方差, 我们选择较大的方差对应的 feature \(Y'\), 由 \(X\) 得到 \(Y'\) 过程即 PCA
</p>

<p>
计算 \(X^TX\) 可能比较慢, 所以也可以通过 SVD 来求解:
</p>

<p>
先计算 X 的 SVD, 然后由于 \(X^TX=V^T\Sigma^T U^T\cdot U\Sigma V=V^T\Sigma^{2}V\),
所以 SVD 的 \(V^T\) 即是 eigen decomposition 的 V
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> time
<span class="org-keyword">import</span> sklearn.decomposition

<span class="org-variable-name">A</span> = np.random.randn(1000, 1000)

<span class="org-variable-name">start</span> = time.time()
np.linalg.eig(A.dot(A.T))
<span class="org-variable-name">end</span> = time.time()
<span class="org-keyword">print</span>(<span class="org-string">"PCA using eig:"</span>, end - start)

<span class="org-variable-name">start</span> = time.time()
np.linalg.svd(A, full_matrices=<span class="org-constant">False</span>)
<span class="org-variable-name">end</span> = time.time()
<span class="org-keyword">print</span>(<span class="org-string">"PCA using svd:"</span>, end - start)

<span class="org-variable-name">start</span> = time.time()
<span class="org-variable-name">pca</span> = sklearn.decomposition.PCA(n_components=1000)
pca.fit(A)
<span class="org-variable-name">end</span> = time.time()
<span class="org-keyword">print</span>(<span class="org-string">"PCA using pca:"</span>, end - start)
</pre>
</div>

<p>
PCA using eig: 0.7494478225708008
PCA using svd: 0.4002048969268799
PCA using pca: 0.3598144054412842
</p>
</div>
</div>
</div>

<div id="outline-container-org0000014" class="outline-3">
<h3 id="org0000014"><span class="section-number-3">1.2</span> 实际意义</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org000000d" class="outline-4">
<h4 id="org000000d"><span class="section-number-4">1.2.1</span> eigen decomposition</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
\(Ax\) 表示对向量 x 进行矩阵变换. A 代表了具体的变换, 包括旋转和拉伸. 通过特征向量和特征值可以直观的描述这种变换
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> math

plt.style.use(<span class="org-string">"classic"</span>)


<span class="org-keyword">def</span> <span class="org-function-name">colockwise_rotate</span>(theta, orig):
    <span class="org-comment-delimiter"># </span><span class="org-comment">radian = theta * np.pi / 180</span>
    <span class="org-variable-name">radian</span> = math.radians(theta)
    <span class="org-variable-name">m</span> = np.array([[np.cos(radian), np.sin(radian)],
                  [-np.sin(radian), np.cos(radian)]])
    <span class="org-keyword">return</span> m.dot(orig.T).T, m


<span class="org-keyword">def</span> <span class="org-function-name">counter_colockwise_rotate</span>(theta, orig):
    <span class="org-comment-delimiter"># </span><span class="org-comment">radian = theta * np.pi / 180</span>
    <span class="org-variable-name">radian</span> = math.radians(theta)
    <span class="org-variable-name">m</span> = np.array([[np.cos(radian), -np.sin(radian)],
                  [np.sin(radian), np.cos(radian)]])
    <span class="org-keyword">return</span> m.dot(orig.T).T, m


<span class="org-keyword">def</span> <span class="org-function-name">strech</span>(ratio, orig):
    <span class="org-variable-name">m</span> = np.array([[ratio[0], 0], [0, ratio[1]]])
    <span class="org-keyword">return</span> m.dot(orig.T).T, m


<span class="org-keyword">def</span> <span class="org-function-name">transform</span>(m, orig):
    <span class="org-keyword">return</span> m.dot(orig.T).T


plt.axis(<span class="org-string">"equal"</span>)

<span class="org-variable-name">r0</span> = np.array([[0., 0.], [0., 1.], [1., 1.], [1., 0.], [0., 0.]])
plt.plot(r0[:, 0], r0[:, 1])

<span class="org-variable-name">r1</span>, <span class="org-variable-name">m1</span> = colockwise_rotate(45, r0)
plt.plot(r1[:, 0], r1[:, 1])

<span class="org-variable-name">r1</span>, <span class="org-variable-name">m2</span> = strech((1.5, 0.5), r1)
plt.plot(r1[:, 0], r1[:, 1])

<span class="org-variable-name">r1</span>, <span class="org-variable-name">m3</span> = counter_colockwise_rotate(45, r1)
plt.plot(r1[:, 0], r1[:, 1])

<span class="org-variable-name">m4</span> = m3.dot(m2.dot(m1))
<span class="org-variable-name">r2</span> = transform(m4, r0)
np.allclose(r1, r2)

<span class="org-variable-name">s</span>, <span class="org-variable-name">v</span> = np.linalg.eig(m4)

<span class="org-variable-name">xx</span> = np.linspace(-0.5, 2, 100)
<span class="org-variable-name">yy</span> = -xx * (v[0, 0] / v[1, 0])
<span class="org-variable-name">yy2</span> = -xx * (v[0, 1] / v[1, 1])
plt.plot(xx, yy, xx, yy2)

plt.show()
</pre>
</div>


<div id="org000000c" class="figure">
<p><img src="../extra/matrix_transformation.png" alt="matrix_transformation.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000011" class="outline-4">
<h4 id="org0000011"><span class="section-number-4">1.2.2</span> PCA</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
PCA 的意义是把原来的 feature 映射到一个新的坐标系中, 使得:
</p>

<ol class="org-ol">
<li>新 feature 的方差尽可能大</li>
<li>不同 feature 两两正交, 没有相关性</li>
</ol>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> seaborn <span class="org-keyword">as</span> sb
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_regression
<span class="org-keyword">from</span> sklearn.decomposition <span class="org-keyword">import</span> PCA
plt.axis(<span class="org-string">'equal'</span>)

<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = make_regression(n_samples=100, n_features=1, n_targets=1, noise=5, random_state=4)
<span class="org-variable-name">X</span> = np.c_[X, Y / 100]
plt.scatter(X[:, 0], X[:, 1])

<span class="org-variable-name">pca</span> = PCA(n_components=2)
pca.fit(X)

<span class="org-comment-delimiter"># </span><span class="org-comment">sklearn &#31639;&#20986;&#26469;&#30340;&#29305;&#24449;&#21521;&#37327;&#26159;&#34892;&#21521;&#37327;</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#29305;&#24449;&#21521;&#37327;&#26377;&#20004;&#20010;: v1=[ 0.79696636  0.60402369], v2=[-0.60402369  0.79696636]</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#20004;&#20010;&#21521;&#37327;&#26159;&#27491;&#20132;&#30340;, &#20854; v1 &#23545;&#24212; l1, v2 &#23545;&#24212; l2</span>
<span class="org-comment-delimiter"># </span>
<span class="org-comment-delimiter"># </span><span class="org-comment">PCA &#20250;&#25226; X &#26144;&#23556;&#21040; XV, &#25152;&#20197;&#26032;&#30340; feature Y &#30456;&#24403;&#20110;&#21407;&#26469;&#30340;&#28857; (x1,y1) &#36716;&#25442;&#21040; v1, v2</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#20026;&#22522;&#30340;&#22352;&#26631;&#31995;&#20013;&#30340;&#26032;&#22352;&#26631;. &#20854;&#20013; XV1 &#23545;&#24212;&#30340;&#26159; X &#21040; l1 &#30340;&#36317;&#31163;, XV2 &#23545;&#24212;&#30340;&#26159; X &#21040; l2</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#30340;&#36317;&#31163;. &#26174;&#28982; XV1 &#30340;&#26041;&#24046;&#24456;&#22823;, XV2 &#30340;&#26041;&#24046;&#24456;&#23567;. &#26041;&#24046;&#23567;&#24847;&#21619;&#30528;&#21306;&#20998;&#24230;&#19981;&#39640;. &#25152;&#20197;&#25105;&#20204;</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#21482;&#20445;&#30041; XV1 &#20570;&#20026;&#26032; feature.</span>
<span class="org-comment-delimiter"># </span>

<span class="org-keyword">print</span>(<span class="org-string">"eigen vector:\n"</span>, pca.components_)

<span class="org-variable-name">v</span> = pca.components_
<span class="org-keyword">print</span>(<span class="org-string">"std from Y:\n"</span>, X.dot(v.T).std(axis=0))
<span class="org-keyword">print</span>(<span class="org-string">"singular_values_:\n"</span>, pca.singular_values_)

<span class="org-variable-name">xx</span> = np.linspace(-3, 3, 100)
<span class="org-variable-name">yy</span> = -xx * (v[0, 0] / v[0, 1])
plt.plot(xx, yy, label=<span class="org-string">"l1"</span>)

<span class="org-variable-name">yy2</span> = -xx * (v[1, 0] / v[1, 1])
plt.plot(xx, yy2, label=<span class="org-string">"l2"</span>)

plt.legend()
plt.show()
</pre>
</div>


<div id="org0000010" class="figure">
<p><img src="../extra/pca2.png" alt="pca2.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org000001e" class="outline-3">
<h3 id="org000001e"><span class="section-number-3">1.3</span> 举例</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-org0000017" class="outline-4">
<h4 id="org0000017"><span class="section-number-4">1.3.1</span> 使用 Eigen, SVD 及 sklearn 进行 PCA</h4>
<div class="outline-text-4" id="text-1-3-1">
<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">from</span> numpy <span class="org-keyword">import</span> linalg
np.set_printoptions(formatter={<span class="org-string">'float'</span>: <span class="org-string">'{: 0.3f}'</span>.<span class="org-builtin">format</span>})

<span class="org-comment-delimiter"># </span><span class="org-comment">X &#20026; 5x4 &#30340;&#30697;&#38453;, 4 &#20010; features, 5 &#20010;&#26679;&#26412;</span>
<span class="org-variable-name">X</span> = np.array([[1, 2, 3, 4], [2, 4, 6, 8], [1, 3, 5, 7], [3, 9, 15, 21],
              [0, 1, 2, 3]]).astype(np.<span class="org-builtin">float</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">X^TX &#26159;&#21327;&#26041;&#24046;&#30340;&#21069;&#25552;&#26159;&#21508; features &#22343;&#20540;&#20026; 0</span>
<span class="org-variable-name">X</span> -= np.mean(X, axis=0)

<span class="org-keyword">print</span>(<span class="org-string">"------ PCA with eigen ------"</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">PCA with eigen decomposition</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#21327;&#26041;&#24046;&#30697;&#38453;, &#24573;&#30053;&#20102; 1/(m-1)</span>
<span class="org-variable-name">cov</span> = np.dot(X.T, X)

<span class="org-variable-name">s</span>, <span class="org-variable-name">v</span> = linalg.eig(cov)
<span class="org-keyword">print</span>(<span class="org-string">"eigen value:"</span>, s)
<span class="org-keyword">print</span>(<span class="org-string">"eigen vector:\n"</span>, v)
<span class="org-comment-delimiter"># </span><span class="org-comment">A=V \lambda V.T</span>
<span class="org-variable-name">cov2</span> = np.dot(v * s, v.T)
<span class="org-keyword">print</span>(<span class="org-string">"t2==t"</span>, np.allclose(cov2, cov))

<span class="org-variable-name">v2</span> = v[:, :2]
<span class="org-variable-name">Y</span> = np.dot(X, v2)
<span class="org-keyword">print</span>(<span class="org-string">"X after PCA using eigen:\n"</span>, Y)

<span class="org-comment-delimiter"># </span><span class="org-comment">PCA with SVD</span>
<span class="org-keyword">print</span>(<span class="org-string">"------ PCA with SVD ------"</span>)
<span class="org-variable-name">u</span>, <span class="org-variable-name">s</span>, <span class="org-variable-name">v</span> = linalg.svd(X, full_matrices=<span class="org-constant">False</span>)

<span class="org-keyword">print</span>(<span class="org-string">"eigen value from svd:"</span>, s ** 2)
<span class="org-keyword">print</span>(<span class="org-string">"eigen vector from svd:\n"</span>, v.T)

<span class="org-variable-name">X2</span> = np.dot(u * s, v)
<span class="org-keyword">print</span>(<span class="org-string">"X2 == X"</span>, np.allclose(X2, X))

<span class="org-variable-name">v2</span> = v.T[:, :2]
<span class="org-variable-name">Y</span> = np.dot(X, v2)
<span class="org-keyword">print</span>(<span class="org-string">"X after PCA using SVD:\n"</span>, Y)

<span class="org-comment-delimiter"># </span><span class="org-comment">sklearn</span>
<span class="org-keyword">print</span>(<span class="org-string">"------ PCA with sklearn PCA ------"</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">sklearn &#35201;&#27714; X &#30340; shape &#20026; mxn, &#20854;&#20013; m &#20026;&#26679;&#26412;&#25968;, n &#20026; feature &#20010;&#25968;</span>
<span class="org-keyword">from</span> sklearn.decomposition <span class="org-keyword">import</span> PCA
<span class="org-variable-name">pca</span> = PCA(n_components=2)
pca.fit(X)

<span class="org-keyword">print</span>(<span class="org-string">"eigen value from PCA:"</span>, pca.singular_values_ ** 2)
<span class="org-keyword">print</span>(<span class="org-string">"eigen vector from PCA:\n"</span>, pca.components_)

<span class="org-variable-name">Y</span> = pca.transform(X)
<span class="org-keyword">print</span>(<span class="org-string">"X after PCA using PCA:\n"</span>, Y)
</pre>
</div>

<p>
-&#x2013;&#x2014; PCA with eigen -&#x2013;&#x2014;
eigen value: [ 358.762  1.238  0.000 -0.000]
eigen vector:
 [[-0.110  0.829 -0.512  0.150]
 [-0.328  0.439  0.828  0.192]
 [-0.546  0.048 -0.119 -0.835]
 [-0.763 -0.342 -0.196  0.493]]
t2==t True
X after PCA using eigen:
 [[ 5.892  0.300]
 [ 0.436  0.781]
 [ 2.182 -0.193]
 [-16.148 -0.214]
 [ 7.638 -0.674]]
-&#x2013;&#x2014; PCA with SVD -&#x2013;&#x2014;
eigen value from svd: [ 358.762  1.238  0.000  0.000]
eigen vector from svd:
 [[ 0.110 -0.829 -0.223  0.500]
 [ 0.328 -0.439 -0.076 -0.833]
 [ 0.546 -0.048  0.820  0.165]
 [ 0.763  0.342 -0.521  0.168]]
X2 == X True
X after PCA using SVD:
 [[-5.892 -0.300]
 [-0.436 -0.781]
 [-2.182  0.193]
 [ 16.148  0.214]
 [-7.638  0.674]]
-&#x2013;&#x2014; PCA with sklearn PCA -&#x2013;&#x2014;
eigen value from PCA: [ 358.762  1.238]
eigen vector from PCA:
 [[ 0.110  0.328  0.546  0.763]
 [ 0.829  0.439  0.048 -0.342]]
X after PCA using PCA:
 [[-5.892  0.300]
 [-0.436  0.781]
 [-2.182 -0.193]
 [ 16.148 -0.214]
 [-7.638 -0.674]]
</p>
</div>
</div>

<div id="outline-container-org000001b" class="outline-4">
<h4 id="org000001b"><span class="section-number-4">1.3.2</span> 使用 PCA 进行特征提取</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
特征提取是指对原特征进行变换(线性或非线性变换)后得到新的特征, 新的特征相比旧的特征更容易区分.
</p>

<p>
PCA 通过线性变换得到的特征是正交的, 且方差更大, 所以更容易区分. 特别的, 通过选择方差最大的几个特征, 可以达到降维的效果
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">import</span> matplotlib.pyplot <span class="org-keyword">as</span> plt
<span class="org-keyword">import</span> seaborn <span class="org-keyword">as</span> sb
<span class="org-keyword">import</span> pandas <span class="org-keyword">as</span> pd
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-keyword">from</span> sklearn.datasets <span class="org-keyword">import</span> make_blobs
<span class="org-keyword">from</span> sklearn <span class="org-keyword">import</span> decomposition

plt.style.use(<span class="org-string">"classic"</span>)
<span class="org-variable-name">X</span>, <span class="org-variable-name">Y</span> = make_blobs(n_features=100, n_samples=1000, centers=5, cluster_std=1)

<span class="org-variable-name">pca</span> = decomposition.PCA(n_components=100)
pca.fit(X)
<span class="org-comment-delimiter"># </span><span class="org-comment">print(pca.singular_values_)</span>

<span class="org-variable-name">pca</span> = decomposition.PCA(n_components=2)
pca.fit(X)

<span class="org-variable-name">X</span> = pca.transform(X)

<span class="org-variable-name">df</span> = pd.DataFrame({<span class="org-string">"X1"</span>: X[:, 0], <span class="org-string">"X2"</span>: X[:, 1], <span class="org-string">"Y"</span>: Y})

sb.lmplot(
    x=<span class="org-string">"X1"</span>,
    y=<span class="org-string">"X2"</span>,
    data=df,
    fit_reg=<span class="org-constant">False</span>,
    hue=<span class="org-string">'Y'</span>,
    legend=<span class="org-constant">True</span>,
    scatter_kws={<span class="org-string">"s"</span>: 20})

plt.show()
</pre>
</div>


<div id="org000001a" class="figure">
<p><img src="../extra/pca.png" alt="pca.png" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://software.intel.com/en-us/mkl-developer-reference-c-singular-value-decomposition-lapack-computational-routines">Singular Value Decomposition: LAPACK Computational Routines</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2018-08-22 Wed 00:00<br />
Last updated: 2021-10-26 Tue 19:50</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
