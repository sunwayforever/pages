<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-25 二 15:53 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>PCA</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">PCA</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org5b2d698">1. PCA</a>
<ul>
<li><a href="#org1cb6e5f">1.1. 计算方法</a>
<ul>
<li><a href="#org95e190d">1.1.1. eigen decomposition</a></li>
<li><a href="#org527adb8">1.1.2. SVD</a></li>
<li><a href="#orgcdd4351">1.1.3. PCA</a></li>
</ul>
</li>
<li><a href="#org07bf92a">1.2. 实际意义</a>
<ul>
<li><a href="#org3f60963">1.2.1. eigen decomposition</a></li>
<li><a href="#orgff644d3">1.2.2. PCA</a></li>
</ul>
</li>
<li><a href="#orgba7b009">1.3. 举例</a>
<ul>
<li><a href="#orge1cef2f">1.3.1. 使用 Eigen, SVD 及 sklearn 进行 PCA</a></li>
<li><a href="#orge055cb4">1.3.2. 使用 PCA 进行特征提取</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org5b2d698" class="outline-2">
<h2 id="org5b2d698"><span class="section-number-2">1</span> PCA</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org1cb6e5f" class="outline-3">
<h3 id="org1cb6e5f"><span class="section-number-3">1.1</span> 计算方法</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<div id="outline-container-org95e190d" class="outline-4">
<h4 id="org95e190d"><span class="section-number-4">1.1.1</span> eigen decomposition</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
对于一个方阵 A, 存在向量 x 及实数 \(\lambda\), 使得 \(Ax=\lambda x\), 则 \(\lambda\)
为 A 的一个特征值, x 为 A 的一个特征向量.
</p>

<p>
计算时, 令 \(Ax-\lambda x=0 \implies (A-\lambda I)x=0\implies det(A-\lambda I)=0\), 计算出 \(\lambda\), 然后把 \(\lambda\) 代入 \((A-\lambda I)x=0\), 计算出 x
</p>

<p>
例如:
</p>

<p>
\(A=\begin{bmatrix}2&3\\2&1\end{bmatrix}\), \(A-\lambda
I=\begin{bmatrix}2-\lambda&3\\2&1-\lambda\end{bmatrix}\),
</p>

<p>
\(det(A-\lambda I)=(2-\lambda)(1-\lambda)-6=0 \implies \lambda_1=-1, \lambda_2=4\)
</p>

<p>
代入 \(\lambda\) 到 \((A-\lambda I)x=0\):
</p>

<p>
当 \(\lambda_1=-1\) 时, 得到 \(x_1=\begin{bmatrix}1\\-1\end{bmatrix}\)
</p>

<p>
当 \(\lambda_2=4\) 时, 得到 \(x_2=\begin{bmatrix}3\\2\end{bmatrix}\)
</p>

<p>
写成矩阵的形式为 \(A=X\lambda X^{-1}\), 其中
</p>

<p>
\(X=\begin{bmatrix}1&3\\-1&2\end{bmatrix}\), \(\lambda\) 为 \(\begin{bmatrix}-1&0\\0&4\end{bmatrix}\)
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np

<span style="color: #268bd2;">A</span> = np.matrix<span style="color: #757575;">(</span>[[2<span style="color: #757575;">,</span> 3]<span style="color: #757575;">,</span> [2<span style="color: #757575;">,</span> 1]]<span style="color: #757575;">)</span>
<span style="color: #268bd2;">x</span> = np.matrix<span style="color: #757575;">(</span>[[1<span style="color: #757575;">,</span> 3]<span style="color: #757575;">,</span> [-1<span style="color: #757575;">,</span> 2]]<span style="color: #757575;">)</span>
<span style="color: #268bd2;">lamdas</span> = np.matrix<span style="color: #757575;">(</span>[[-1<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> [0<span style="color: #757575;">,</span> 4]]<span style="color: #757575;">)</span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span>A.dot<span style="color: #757575;">(</span>x<span style="color: #757575;">))</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span>x.dot<span style="color: #757575;">(</span>lamdas<span style="color: #757575;">))</span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span>np.linalg.eig<span style="color: #757575;">(</span>A<span style="color: #757575;">))</span>
</pre>
</div>

<p>
[[-1 12]
 [ 1  8]]
[[-1 12]
 [ 1  8]]
(array([ 4., -1.]), matrix([[ 0.83205029, -0.70710678],
        [ 0.5547002 ,  0.70710678]]))
</p>

<p>
另外, 若 A 为对称矩阵, 则 X 为正交矩阵, 则 \(A=X\lambda X^T\)
</p>
</div>
</div>

<div id="outline-container-org527adb8" class="outline-4">
<h4 id="org527adb8"><span class="section-number-4">1.1.2</span> SVD</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
若 A 不是方阵, 则无法应用 eigen decomposition, 但可以应用 SVD 使得
</p>

<p>
\(A= U\Sigma V\), 其中:
</p>

<p>
\(A \in R^{m\times n}\), \(U \in R^{m\times q}\), \(\Sigma \in R^{q\times q}\), \(V \in
R^{q\times n}\), \(q=min(m,n)\)
</p>

<p>
其中 \(U, V\) 是正交矩阵, \(\Sigma\) 是对角矩阵.
</p>

<p>
如何确定 \(U, \Sigma, V\):
</p>

<p>
令 \(AA^T=U\Sigma V\cdot V^T\Sigma^T U^T=U\Sigma^{2}U^T\), 这实际是在求 \(AA^T\) 的特征值与特征向量. 由于 \(AA^T\) 是对称矩阵, 所以 \(U\) 是正交矩阵
</p>

<p>
同理, 通过求解 \(A^T A\) 的特征向量可得 \(V\)
</p>

<p>
另外, 实际应用中求解 SVD 并不会使用上述方法, 因为比较慢<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>
</div>
</div>

<div id="outline-container-orgcdd4351" class="outline-4">
<h4 id="orgcdd4351"><span class="section-number-4">1.1.3</span> PCA</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>
\(X \in R^{m\times n}\) 是我们原始的数据矩阵, m 个样本, n 个 feature
</p>

<p>
对 \(X^TX\) 进行 eigen decomposition, 使得 \(X^TX=V\lambda V^T\).
</p>

<p>
定义一个变换 \(Y=XV\), 由于 \(Y^TY=V^TX^TXV=\lambda\) 是一个对角矩阵, 所以 Y 所有
feature 都是不相关的. \(\lambda\) 对角线上的值代表了不同 feature 的方差, 我们选择较大的方差对应的 feature \(Y'\), 由 \(X\) 得到 \(Y'\) 过程即 PCA
</p>

<p>
计算 \(X^TX\) 可能比较慢, 所以也可以通过 SVD 来求解:
</p>

<p>
先计算 X 的 SVD, 然后由于 \(X^TX=V^T\Sigma^T U^T\cdot U\Sigma V=V^T\Sigma^{2}V\),
所以 SVD 的 \(V^T\) 即是 eigen decomposition 的 V
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np
<span style="color: #859900;">import</span> time
<span style="color: #859900;">import</span> sklearn.decomposition

<span style="color: #268bd2;">A</span> = np.random.randn<span style="color: #757575;">(</span>1000<span style="color: #757575;">,</span> 1000<span style="color: #757575;">)</span>

<span style="color: #268bd2;">start</span> = time.time<span style="color: #757575;">()</span>
np.linalg.eig<span style="color: #757575;">(</span>A.dot<span style="color: #757575;">(</span>A.T<span style="color: #757575;">))</span>
<span style="color: #268bd2;">end</span> = time.time<span style="color: #757575;">()</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"PCA using eig:"</span><span style="color: #757575;">,</span> end - start<span style="color: #757575;">)</span>

<span style="color: #268bd2;">start</span> = time.time<span style="color: #757575;">()</span>
np.linalg.svd<span style="color: #757575;">(</span>A<span style="color: #757575;">,</span> full_matrices=<span style="color: #268bd2; font-weight: bold;">False</span><span style="color: #757575;">)</span>
<span style="color: #268bd2;">end</span> = time.time<span style="color: #757575;">()</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"PCA using svd:"</span><span style="color: #757575;">,</span> end - start<span style="color: #757575;">)</span>

<span style="color: #268bd2;">start</span> = time.time<span style="color: #757575;">()</span>
<span style="color: #268bd2;">pca</span> = sklearn.decomposition.PCA<span style="color: #757575;">(</span>n_components=1000<span style="color: #757575;">)</span>
pca.fit<span style="color: #757575;">(</span>A<span style="color: #757575;">)</span>
<span style="color: #268bd2;">end</span> = time.time<span style="color: #757575;">()</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"PCA using pca:"</span><span style="color: #757575;">,</span> end - start<span style="color: #757575;">)</span>
</pre>
</div>

<p>
PCA using eig: 0.7494478225708008
PCA using svd: 0.4002048969268799
PCA using pca: 0.3598144054412842
</p>
</div>
</div>
</div>

<div id="outline-container-org07bf92a" class="outline-3">
<h3 id="org07bf92a"><span class="section-number-3">1.2</span> 实际意义</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org3f60963" class="outline-4">
<h4 id="org3f60963"><span class="section-number-4">1.2.1</span> eigen decomposition</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
\(Ax\) 表示对向量 x 进行矩阵变换. A 代表了具体的变换, 包括旋转和拉伸. 通过特征向量和特征值可以直观的描述这种变换
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np
<span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt
<span style="color: #859900;">import</span> math

plt.style.use<span style="color: #757575;">(</span><span style="color: #2aa198;">"classic"</span><span style="color: #757575;">)</span>


<span style="color: #859900;">def</span> <span style="color: #268bd2;">colockwise_rotate</span><span style="color: #757575;">(</span>theta<span style="color: #757575;">,</span> orig<span style="color: #757575;">)</span>:
    <span style="color: #586e75;"># </span><span style="color: #586e75;">radian = theta * np.pi / 180</span>
    <span style="color: #268bd2;">radian</span> = math.radians<span style="color: #757575;">(</span>theta<span style="color: #757575;">)</span>
    <span style="color: #268bd2;">m</span> = np.array<span style="color: #757575;">(</span>[[np.cos<span style="color: #757575;">(</span>radian<span style="color: #757575;">),</span> np.sin<span style="color: #757575;">(</span>radian<span style="color: #757575;">)</span>]<span style="color: #757575;">,</span>
                  [-np.sin<span style="color: #757575;">(</span>radian<span style="color: #757575;">),</span> np.cos<span style="color: #757575;">(</span>radian<span style="color: #757575;">)</span>]]<span style="color: #757575;">)</span>
    <span style="color: #859900;">return</span> m.dot<span style="color: #757575;">(</span>orig.T<span style="color: #757575;">)</span>.T<span style="color: #757575;">,</span> m


<span style="color: #859900;">def</span> <span style="color: #268bd2;">counter_colockwise_rotate</span><span style="color: #757575;">(</span>theta<span style="color: #757575;">,</span> orig<span style="color: #757575;">)</span>:
    <span style="color: #586e75;"># </span><span style="color: #586e75;">radian = theta * np.pi / 180</span>
    <span style="color: #268bd2;">radian</span> = math.radians<span style="color: #757575;">(</span>theta<span style="color: #757575;">)</span>
    <span style="color: #268bd2;">m</span> = np.array<span style="color: #757575;">(</span>[[np.cos<span style="color: #757575;">(</span>radian<span style="color: #757575;">),</span> -np.sin<span style="color: #757575;">(</span>radian<span style="color: #757575;">)</span>]<span style="color: #757575;">,</span>
                  [np.sin<span style="color: #757575;">(</span>radian<span style="color: #757575;">),</span> np.cos<span style="color: #757575;">(</span>radian<span style="color: #757575;">)</span>]]<span style="color: #757575;">)</span>
    <span style="color: #859900;">return</span> m.dot<span style="color: #757575;">(</span>orig.T<span style="color: #757575;">)</span>.T<span style="color: #757575;">,</span> m


<span style="color: #859900;">def</span> <span style="color: #268bd2;">strech</span><span style="color: #757575;">(</span>ratio<span style="color: #757575;">,</span> orig<span style="color: #757575;">)</span>:
    <span style="color: #268bd2;">m</span> = np.array<span style="color: #757575;">(</span>[[ratio[0]<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> [0<span style="color: #757575;">,</span> ratio[1]]]<span style="color: #757575;">)</span>
    <span style="color: #859900;">return</span> m.dot<span style="color: #757575;">(</span>orig.T<span style="color: #757575;">)</span>.T<span style="color: #757575;">,</span> m


<span style="color: #859900;">def</span> <span style="color: #268bd2;">transform</span><span style="color: #757575;">(</span>m<span style="color: #757575;">,</span> orig<span style="color: #757575;">)</span>:
    <span style="color: #859900;">return</span> m.dot<span style="color: #757575;">(</span>orig.T<span style="color: #757575;">)</span>.T


plt.axis<span style="color: #757575;">(</span><span style="color: #2aa198;">"equal"</span><span style="color: #757575;">)</span>

<span style="color: #268bd2;">r0</span> = np.array<span style="color: #757575;">(</span>[[0.<span style="color: #757575;">,</span> 0.]<span style="color: #757575;">,</span> [0.<span style="color: #757575;">,</span> 1.]<span style="color: #757575;">,</span> [1.<span style="color: #757575;">,</span> 1.]<span style="color: #757575;">,</span> [1.<span style="color: #757575;">,</span> 0.]<span style="color: #757575;">,</span> [0.<span style="color: #757575;">,</span> 0.]]<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>r0[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> r0[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>

<span style="color: #268bd2;">r1</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">m1</span> = colockwise_rotate<span style="color: #757575;">(</span>45<span style="color: #757575;">,</span> r0<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>r1[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> r1[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>

<span style="color: #268bd2;">r1</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">m2</span> = strech<span style="color: #757575;">((</span>1.5<span style="color: #757575;">,</span> 0.5<span style="color: #757575;">),</span> r1<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>r1[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> r1[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>

<span style="color: #268bd2;">r1</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">m3</span> = counter_colockwise_rotate<span style="color: #757575;">(</span>45<span style="color: #757575;">,</span> r1<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>r1[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> r1[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>

<span style="color: #268bd2;">m4</span> = m3.dot<span style="color: #757575;">(</span>m2.dot<span style="color: #757575;">(</span>m1<span style="color: #757575;">))</span>
<span style="color: #268bd2;">r2</span> = transform<span style="color: #757575;">(</span>m4<span style="color: #757575;">,</span> r0<span style="color: #757575;">)</span>
np.allclose<span style="color: #757575;">(</span>r1<span style="color: #757575;">,</span> r2<span style="color: #757575;">)</span>

<span style="color: #268bd2;">s</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">v</span> = np.linalg.eig<span style="color: #757575;">(</span>m4<span style="color: #757575;">)</span>

<span style="color: #268bd2;">xx</span> = np.linspace<span style="color: #757575;">(</span>-0.5<span style="color: #757575;">,</span> 2<span style="color: #757575;">,</span> 100<span style="color: #757575;">)</span>
<span style="color: #268bd2;">yy</span> = -xx * <span style="color: #757575;">(</span>v[0<span style="color: #757575;">,</span> 0] / v[1<span style="color: #757575;">,</span> 0]<span style="color: #757575;">)</span>
<span style="color: #268bd2;">yy2</span> = -xx * <span style="color: #757575;">(</span>v[0<span style="color: #757575;">,</span> 1] / v[1<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>xx<span style="color: #757575;">,</span> yy<span style="color: #757575;">,</span> xx<span style="color: #757575;">,</span> yy2<span style="color: #757575;">)</span>

plt.show<span style="color: #757575;">()</span>
</pre>
</div>


<div id="org45aeb3c" class="figure">
<p><img src="../extra/matrix_transformation.png" alt="matrix_transformation.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-orgff644d3" class="outline-4">
<h4 id="orgff644d3"><span class="section-number-4">1.2.2</span> PCA</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
PCA 的意义是把原来的 feature 映射到一个新的坐标系中, 使得:
</p>

<ol class="org-ol">
<li>新 feature 的方差尽可能大</li>
<li>不同 feature 两两正交, 没有相关性</li>
</ol>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt
<span style="color: #859900;">import</span> seaborn <span style="color: #859900;">as</span> sb
<span style="color: #859900;">import</span> pandas <span style="color: #859900;">as</span> pd
<span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np

<span style="color: #859900;">from</span> sklearn.datasets <span style="color: #859900;">import</span> make_regression
<span style="color: #859900;">from</span> sklearn.decomposition <span style="color: #859900;">import</span> PCA
plt.axis<span style="color: #757575;">(</span><span style="color: #2aa198;">'equal'</span><span style="color: #757575;">)</span>

<span style="color: #268bd2;">X</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">Y</span> = make_regression<span style="color: #757575;">(</span>n_samples=100<span style="color: #757575;">,</span> n_features=1<span style="color: #757575;">,</span> n_targets=1<span style="color: #757575;">,</span> noise=5<span style="color: #757575;">,</span> random_state=4<span style="color: #757575;">)</span>
<span style="color: #268bd2;">X</span> = np.c_[X<span style="color: #757575;">,</span> Y / 100]
plt.scatter<span style="color: #757575;">(</span>X[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> X[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>

<span style="color: #268bd2;">pca</span> = PCA<span style="color: #757575;">(</span>n_components=2<span style="color: #757575;">)</span>
pca.fit<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>

<span style="color: #586e75;"># </span><span style="color: #586e75;">sklearn &#31639;&#20986;&#26469;&#30340;&#29305;&#24449;&#21521;&#37327;&#26159;&#34892;&#21521;&#37327;</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#29305;&#24449;&#21521;&#37327;&#26377;&#20004;&#20010;: v1=[ 0.79696636  0.60402369], v2=[-0.60402369  0.79696636]</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#20004;&#20010;&#21521;&#37327;&#26159;&#27491;&#20132;&#30340;, &#20854; v1 &#23545;&#24212; l1, v2 &#23545;&#24212; l2</span>
<span style="color: #586e75;"># </span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">PCA &#20250;&#25226; X &#26144;&#23556;&#21040; XV, &#25152;&#20197;&#26032;&#30340; feature Y &#30456;&#24403;&#20110;&#21407;&#26469;&#30340;&#28857; (x1,y1) &#36716;&#25442;&#21040; v1, v2</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#20026;&#22522;&#30340;&#22352;&#26631;&#31995;&#20013;&#30340;&#26032;&#22352;&#26631;. &#20854;&#20013; XV1 &#23545;&#24212;&#30340;&#26159; X &#21040; l1 &#30340;&#36317;&#31163;, XV2 &#23545;&#24212;&#30340;&#26159; X &#21040; l2</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#30340;&#36317;&#31163;. &#26174;&#28982; XV1 &#30340;&#26041;&#24046;&#24456;&#22823;, XV2 &#30340;&#26041;&#24046;&#24456;&#23567;. &#26041;&#24046;&#23567;&#24847;&#21619;&#30528;&#21306;&#20998;&#24230;&#19981;&#39640;. &#25152;&#20197;&#25105;&#20204;</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#21482;&#20445;&#30041; XV1 &#20570;&#20026;&#26032; feature.</span>
<span style="color: #586e75;"># </span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen vector:\n"</span><span style="color: #757575;">,</span> pca.components_<span style="color: #757575;">)</span>

<span style="color: #268bd2;">v</span> = pca.components_
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"std from Y:\n"</span><span style="color: #757575;">,</span> X.dot<span style="color: #757575;">(</span>v.T<span style="color: #757575;">)</span>.std<span style="color: #757575;">(</span>axis=0<span style="color: #757575;">))</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"singular_values_:\n"</span><span style="color: #757575;">,</span> pca.singular_values_<span style="color: #757575;">)</span>

<span style="color: #268bd2;">xx</span> = np.linspace<span style="color: #757575;">(</span>-3<span style="color: #757575;">,</span> 3<span style="color: #757575;">,</span> 100<span style="color: #757575;">)</span>
<span style="color: #268bd2;">yy</span> = -xx * <span style="color: #757575;">(</span>v[0<span style="color: #757575;">,</span> 0] / v[0<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>xx<span style="color: #757575;">,</span> yy<span style="color: #757575;">,</span> label=<span style="color: #2aa198;">"l1"</span><span style="color: #757575;">)</span>

<span style="color: #268bd2;">yy2</span> = -xx * <span style="color: #757575;">(</span>v[1<span style="color: #757575;">,</span> 0] / v[1<span style="color: #757575;">,</span> 1]<span style="color: #757575;">)</span>
plt.plot<span style="color: #757575;">(</span>xx<span style="color: #757575;">,</span> yy2<span style="color: #757575;">,</span> label=<span style="color: #2aa198;">"l2"</span><span style="color: #757575;">)</span>

plt.legend<span style="color: #757575;">()</span>
plt.show<span style="color: #757575;">()</span>
</pre>
</div>


<div id="org4247c17" class="figure">
<p><img src="../extra/pca2.png" alt="pca2.png" />
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgba7b009" class="outline-3">
<h3 id="orgba7b009"><span class="section-number-3">1.3</span> 举例</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<div id="outline-container-orge1cef2f" class="outline-4">
<h4 id="orge1cef2f"><span class="section-number-4">1.3.1</span> 使用 Eigen, SVD 及 sklearn 进行 PCA</h4>
<div class="outline-text-4" id="text-1-3-1">
<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np
<span style="color: #859900;">from</span> numpy <span style="color: #859900;">import</span> linalg
np.set_printoptions<span style="color: #757575;">(</span>formatter=<span style="color: #757575;">{</span><span style="color: #2aa198;">'float'</span>: <span style="color: #2aa198;">'{: 0.3f}'</span>.<span style="color: #839496;">format</span><span style="color: #757575;">})</span>

<span style="color: #586e75;"># </span><span style="color: #586e75;">X &#20026; 5x4 &#30340;&#30697;&#38453;, 4 &#20010; features, 5 &#20010;&#26679;&#26412;</span>
<span style="color: #268bd2;">X</span> = np.array<span style="color: #757575;">(</span>[[1<span style="color: #757575;">,</span> 2<span style="color: #757575;">,</span> 3<span style="color: #757575;">,</span> 4]<span style="color: #757575;">,</span> [2<span style="color: #757575;">,</span> 4<span style="color: #757575;">,</span> 6<span style="color: #757575;">,</span> 8]<span style="color: #757575;">,</span> [1<span style="color: #757575;">,</span> 3<span style="color: #757575;">,</span> 5<span style="color: #757575;">,</span> 7]<span style="color: #757575;">,</span> [3<span style="color: #757575;">,</span> 9<span style="color: #757575;">,</span> 15<span style="color: #757575;">,</span> 21]<span style="color: #757575;">,</span>
              [0<span style="color: #757575;">,</span> 1<span style="color: #757575;">,</span> 2<span style="color: #757575;">,</span> 3]]<span style="color: #757575;">)</span>.astype<span style="color: #757575;">(</span>np.<span style="color: #839496;">float</span><span style="color: #757575;">)</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">X^TX &#26159;&#21327;&#26041;&#24046;&#30340;&#21069;&#25552;&#26159;&#21508; features &#22343;&#20540;&#20026; 0</span>
<span style="color: #268bd2;">X</span> -= np.mean<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> axis=0<span style="color: #757575;">)</span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"------ PCA with eigen ------"</span><span style="color: #757575;">)</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">PCA with eigen decomposition</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">&#21327;&#26041;&#24046;&#30697;&#38453;, &#24573;&#30053;&#20102; 1/(m-1)</span>
<span style="color: #268bd2;">cov</span> = np.dot<span style="color: #757575;">(</span>X.T<span style="color: #757575;">,</span> X<span style="color: #757575;">)</span>

<span style="color: #268bd2;">s</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">v</span> = linalg.eig<span style="color: #757575;">(</span>cov<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen value:"</span><span style="color: #757575;">,</span> s<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen vector:\n"</span><span style="color: #757575;">,</span> v<span style="color: #757575;">)</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">A=V \lambda V.T</span>
<span style="color: #268bd2;">cov2</span> = np.dot<span style="color: #757575;">(</span>v * s<span style="color: #757575;">,</span> v.T<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"t2==t"</span><span style="color: #757575;">,</span> np.allclose<span style="color: #757575;">(</span>cov2<span style="color: #757575;">,</span> cov<span style="color: #757575;">))</span>

<span style="color: #268bd2;">v2</span> = v[:<span style="color: #757575;">,</span> :2]
<span style="color: #268bd2;">Y</span> = np.dot<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> v2<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"X after PCA using eigen:\n"</span><span style="color: #757575;">,</span> Y<span style="color: #757575;">)</span>

<span style="color: #586e75;"># </span><span style="color: #586e75;">PCA with SVD</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"------ PCA with SVD ------"</span><span style="color: #757575;">)</span>
<span style="color: #268bd2;">u</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">s</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">v</span> = linalg.svd<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> full_matrices=<span style="color: #268bd2; font-weight: bold;">False</span><span style="color: #757575;">)</span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen value from svd:"</span><span style="color: #757575;">,</span> s ** 2<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen vector from svd:\n"</span><span style="color: #757575;">,</span> v.T<span style="color: #757575;">)</span>

<span style="color: #268bd2;">X2</span> = np.dot<span style="color: #757575;">(</span>u * s<span style="color: #757575;">,</span> v<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"X2 == X"</span><span style="color: #757575;">,</span> np.allclose<span style="color: #757575;">(</span>X2<span style="color: #757575;">,</span> X<span style="color: #757575;">))</span>

<span style="color: #268bd2;">v2</span> = v.T[:<span style="color: #757575;">,</span> :2]
<span style="color: #268bd2;">Y</span> = np.dot<span style="color: #757575;">(</span>X<span style="color: #757575;">,</span> v2<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"X after PCA using SVD:\n"</span><span style="color: #757575;">,</span> Y<span style="color: #757575;">)</span>

<span style="color: #586e75;"># </span><span style="color: #586e75;">sklearn</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"------ PCA with sklearn PCA ------"</span><span style="color: #757575;">)</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">sklearn &#35201;&#27714; X &#30340; shape &#20026; mxn, &#20854;&#20013; m &#20026;&#26679;&#26412;&#25968;, n &#20026; feature &#20010;&#25968;</span>
<span style="color: #859900;">from</span> sklearn.decomposition <span style="color: #859900;">import</span> PCA
<span style="color: #268bd2;">pca</span> = PCA<span style="color: #757575;">(</span>n_components=2<span style="color: #757575;">)</span>
pca.fit<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>

<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen value from PCA:"</span><span style="color: #757575;">,</span> pca.singular_values_ ** 2<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"eigen vector from PCA:\n"</span><span style="color: #757575;">,</span> pca.components_<span style="color: #757575;">)</span>

<span style="color: #268bd2;">Y</span> = pca.transform<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>
<span style="color: #859900;">print</span><span style="color: #757575;">(</span><span style="color: #2aa198;">"X after PCA using PCA:\n"</span><span style="color: #757575;">,</span> Y<span style="color: #757575;">)</span>
</pre>
</div>

<p>
-&#x2013;&#x2014; PCA with eigen -&#x2013;&#x2014;
eigen value: [ 358.762  1.238  0.000 -0.000]
eigen vector:
 [[-0.110  0.829 -0.512  0.150]
 [-0.328  0.439  0.828  0.192]
 [-0.546  0.048 -0.119 -0.835]
 [-0.763 -0.342 -0.196  0.493]]
t2==t True
X after PCA using eigen:
 [[ 5.892  0.300]
 [ 0.436  0.781]
 [ 2.182 -0.193]
 [-16.148 -0.214]
 [ 7.638 -0.674]]
-&#x2013;&#x2014; PCA with SVD -&#x2013;&#x2014;
eigen value from svd: [ 358.762  1.238  0.000  0.000]
eigen vector from svd:
 [[ 0.110 -0.829 -0.223  0.500]
 [ 0.328 -0.439 -0.076 -0.833]
 [ 0.546 -0.048  0.820  0.165]
 [ 0.763  0.342 -0.521  0.168]]
X2 == X True
X after PCA using SVD:
 [[-5.892 -0.300]
 [-0.436 -0.781]
 [-2.182  0.193]
 [ 16.148  0.214]
 [-7.638  0.674]]
-&#x2013;&#x2014; PCA with sklearn PCA -&#x2013;&#x2014;
eigen value from PCA: [ 358.762  1.238]
eigen vector from PCA:
 [[ 0.110  0.328  0.546  0.763]
 [ 0.829  0.439  0.048 -0.342]]
X after PCA using PCA:
 [[-5.892  0.300]
 [-0.436  0.781]
 [-2.182 -0.193]
 [ 16.148 -0.214]
 [-7.638 -0.674]]
</p>
</div>
</div>

<div id="outline-container-orge055cb4" class="outline-4">
<h4 id="orge055cb4"><span class="section-number-4">1.3.2</span> 使用 PCA 进行特征提取</h4>
<div class="outline-text-4" id="text-1-3-2">
<p>
特征提取是指对原特征进行变换(线性或非线性变换)后得到新的特征, 新的特征相比旧的特征更容易区分.
</p>

<p>
PCA 通过线性变换得到的特征是正交的, 且方差更大, 所以更容易区分. 特别的, 通过选择方差最大的几个特征, 可以达到降维的效果
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span style="color: #859900;">import</span> matplotlib.pyplot <span style="color: #859900;">as</span> plt
<span style="color: #859900;">import</span> seaborn <span style="color: #859900;">as</span> sb
<span style="color: #859900;">import</span> pandas <span style="color: #859900;">as</span> pd
<span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np

<span style="color: #859900;">from</span> sklearn.datasets <span style="color: #859900;">import</span> make_blobs
<span style="color: #859900;">from</span> sklearn <span style="color: #859900;">import</span> decomposition

plt.style.use<span style="color: #757575;">(</span><span style="color: #2aa198;">"classic"</span><span style="color: #757575;">)</span>
<span style="color: #268bd2;">X</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">Y</span> = make_blobs<span style="color: #757575;">(</span>n_features=100<span style="color: #757575;">,</span> n_samples=1000<span style="color: #757575;">,</span> centers=5<span style="color: #757575;">,</span> cluster_std=1<span style="color: #757575;">)</span>

<span style="color: #268bd2;">pca</span> = decomposition.PCA<span style="color: #757575;">(</span>n_components=100<span style="color: #757575;">)</span>
pca.fit<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">print(pca.singular_values_)</span>

<span style="color: #268bd2;">pca</span> = decomposition.PCA<span style="color: #757575;">(</span>n_components=2<span style="color: #757575;">)</span>
pca.fit<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>

<span style="color: #268bd2;">X</span> = pca.transform<span style="color: #757575;">(</span>X<span style="color: #757575;">)</span>

<span style="color: #268bd2;">df</span> = pd.DataFrame<span style="color: #757575;">({</span><span style="color: #2aa198;">"X1"</span>: X[:<span style="color: #757575;">,</span> 0]<span style="color: #757575;">,</span> <span style="color: #2aa198;">"X2"</span>: X[:<span style="color: #757575;">,</span> 1]<span style="color: #757575;">,</span> <span style="color: #2aa198;">"Y"</span>: Y<span style="color: #757575;">})</span>

sb.lmplot<span style="color: #757575;">(</span>
    x=<span style="color: #2aa198;">"X1"</span><span style="color: #757575;">,</span>
    y=<span style="color: #2aa198;">"X2"</span><span style="color: #757575;">,</span>
    data=df<span style="color: #757575;">,</span>
    fit_reg=<span style="color: #268bd2; font-weight: bold;">False</span><span style="color: #757575;">,</span>
    hue=<span style="color: #2aa198;">'Y'</span><span style="color: #757575;">,</span>
    legend=<span style="color: #268bd2; font-weight: bold;">True</span><span style="color: #757575;">,</span>
    scatter_kws=<span style="color: #757575;">{</span><span style="color: #2aa198;">"s"</span>: 20<span style="color: #757575;">})</span>

plt.show<span style="color: #757575;">()</span>
</pre>
</div>


<div id="orgfdd141c" class="figure">
<p><img src="../extra/pca.png" alt="pca.png" />
</p>
</div>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
<a href="https://software.intel.com/en-us/mkl-developer-reference-c-singular-value-decomposition-lapack-computational-routines">Singular Value Decomposition: LAPACK Computational Routines</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2018-08-22 三 00:00<br />
Last updated: 2021-10-26 二 19:43</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
