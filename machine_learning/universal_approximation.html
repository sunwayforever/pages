<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Universal Approximation</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Universal Approximation</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000011">1. Universal Approximation</a>
<ul>
<li><a href="#org0000001">1.1. Sigmoid 做为激活函数</a></li>
<li><a href="#org0000005">1.2. ReLU 做为激活函数</a></li>
<li><a href="#org0000008">1.3. 线性函数无法做为激活函数</a></li>
<li><a href="#org000000b">1.4. 函数能做为激活函数的条件</a></li>
<li><a href="#org000000e">1.5. ANN 并不是万能</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000011" class="outline-2">
<h2 id="org0000011"><span class="section-number-2">1</span> Universal Approximation</h2>
<div class="outline-text-2" id="text-1">
<p>
Universal Approximation (通用逼近理论) 是指：如果一个前馈神经网络具有线性输出层和至少一层隐藏层，只要给予网
络足够数量的神经元，便可以实现以足够高精度来逼近任意一个在 Rn 的紧子集 (Compact
subset) 上的连续函数。
</p>
</div>

<div id="outline-container-org0000001" class="outline-3">
<h3 id="org0000001"><span class="section-number-3">1.1</span> Sigmoid 做为激活函数</h3>
<div class="outline-text-3" id="text-1-1">
<p>
<a href="http://neuralnetworksanddeeplearning.com/chap4.html">http://neuralnetworksanddeeplearning.com/chap4.html</a>
</p>

<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
import matplotlib.pyplot as plt

plt.style.use("seaborn")

epsilon = 0.001

def sigmoid(x):
    return 1. / (1. + np.exp(-x))

def bump_sigmoid(h, a, b):
    x = np.linspace(0, 5, 100)
    left = sigmoid(1 / epsilon * x - 1 / epsilon * a)
    right = sigmoid(1 / epsilon * x - 1 / epsilon * b)

    out = (left - right) * h
    plt.plot(x, out)


bump_sigmoid(-10, 1, 2)

plt.show()

</pre>
</div>


<div id="org0000000" class="figure">
<p><img src="../extra/universal_approximation_sigmoid.png" alt="universal_approximation_sigmoid.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000005" class="outline-3">
<h3 id="org0000005"><span class="section-number-3">1.2</span> ReLU 做为激活函数</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator">https://www.quora.com/Is-a-single-layered-ReLu-network-still-a-universal-approximator</a>
</p>

<div class="org-src-container">
<pre class="src src-ipython">def relu(x):
    return np.maximum(0, x)


def bump_relu(h, a, b):
    x = np.linspace(0, 5, 100)
    plt.plot(
        x, h / epsilon * (relu(x - a) - relu(x - a - epsilon) -
                          (relu(x - b) - relu(x - b - epsilon))))

bump_relu(10, 2, 4)
plt.show()
</pre>
</div>


<div id="org0000004" class="figure">
<p><img src="universal_approximator_relu.png" alt="universal_approximator_relu.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000008" class="outline-3">
<h3 id="org0000008"><span class="section-number-3">1.3</span> 线性函数无法做为激活函数</h3>
<div class="outline-text-3" id="text-1-3">
<p>
线性函数做为激活函数时, 最终输出必然还是 x,b 的线性组合
</p>
</div>
</div>

<div id="outline-container-org000000b" class="outline-3">
<h3 id="org000000b"><span class="section-number-3">1.4</span> 函数能做为激活函数的条件</h3>
</div>

<div id="outline-container-org000000e" class="outline-3">
<h3 id="org000000e"><span class="section-number-3">1.5</span> ANN 并不是万能</h3>
<div class="outline-text-3" id="text-1-5">
<p>
通用逼近理论的前提是逼近`连续函数`, 所以有些问题无法用 ANN 解决, 例如
\(f(x)=x\pmod{K}\)
</p>

<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch

N_CLASSES = 2
model = torch.nn.Sequential(
    torch.nn.Linear(1, 10), torch.nn.ReLU(), torch.nn.Linear(10, N_CLASSES))


class OddsAndEvenDataset(Dataset):
    def __init__(self, low, high, size):
        X = np.random.randint(low, high, size)
        Y = X % N_CLASSES
        self.X = torch.from_numpy(X).float().view(-1, 1)
        self.Y = torch.from_numpy(Y).long().view(-1)

    def __getitem__(self, index):
        return self.X[index], self.Y[index]

    def __len__(self):
        return len(self.X)


training_set = OddsAndEvenDataset(0, 1000, 500)
training_loader = DataLoader(training_set, batch_size=100)

test_set = OddsAndEvenDataset(500, 2000, 500)
test_loader = DataLoader(test_set, batch_size=500)

# criterion = torch.nn.BCEWithLogitsLoss()
criterion = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)


def train():
    model.train()
    for i in range(1000):
        for x, y in training_loader:
            loss = criterion(model(x), y)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    # if i % 20 == 0:
    print("loss:",loss.item())


def test():
    model.eval()
    for x, y in training_loader:
        y_hat = model(x)
        # y_hat = F.sigmoid(y_hat)
        # y_hat = y_hat &gt; 0.5
        y_hat = torch.argmax(y_hat, dim=1)
        accu = torch.sum(y_hat.byte() == y.byte()).item() / 100
        print("train:", accu)
        break

    for x, y in test_loader:
        y_hat = model(x)
        # y_hat = F.sigmoid(y_hat)
        # y_hat = y_hat &gt; 0.5
        y_hat = torch.argmax(y_hat, dim=1)
        accu = torch.sum(y_hat.byte() == y.byte()).item() / 500
        print("test:", accu)


train()
test()
</pre>
</div>

<p>
loss: 0.6905694007873535
train: 0.47
test: 0.536
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2018-07-31 Tue 00:00<br />
Last updated: 2022-01-03 Mon 10:53</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
