<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>RNN</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">RNN</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org000002a">1. RNN</a>
<ul>
<li><a href="#org0000000">1.1. Overview</a></li>
<li><a href="#org0000021">1.2. RNN Cell</a>
<ul>
<li><a href="#org0000007">1.2.1. Basic RNN</a></li>
<li><a href="#org000000b">1.2.2. GRU</a></li>
<li><a href="#org0000015">1.2.3. LSTM</a></li>
<li><a href="#org000001e">1.2.4. Example</a></li>
</ul>
</li>
<li><a href="#org0000024">1.3. Word2vec</a></li>
<li><a href="#org0000027">1.4. Seq2Seq Model</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org000002a" class="outline-2">
<h2 id="org000002a"><span class="section-number-2">1.</span> RNN</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org0000000" class="outline-3">
<h3 id="org0000000"><span class="section-number-3">1.1.</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
RNN (Recurrent Neural Network)
</p>

<p>
问题: 识别负面言论
</p>

<ol class="org-ol">
<li>I hate you -&gt; negative</li>
<li>I love you -&gt; positive</li>
</ol>

<p>
如果用机器学习来解决这个问题, feature 和 label 分别是什么?
</p>

<ol class="org-ol">
<li><p>
label
</p>

<p>
label 很好定义, 我们用 0 表示 negative, 1 表示 positive.
在网络的输出端使用一个 sigmoid 即可.
</p></li>

<li><p>
feature
</p>

<ul class="org-ul">
<li>分词</li>
</ul>

<p>
首先, 需要以单词为单位进行分词, 因为单个字母无所谓正面负面.<br />
然后, 要以构造一个 vocabulary, 给每个英文单词赋一个值, 例如:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-variable-name">vocabulary</span> = {<span class="org-string">"I"</span>:0, <span class="org-string">"hate"</span>:1, <span class="org-string">"you"</span>: 2, <span class="org-string">"love"</span>: 3, <span class="org-string">"hello"</span>:4, <span class="org-string">"world"</span>: 5,...}
</pre>
</div>

<p>
vocabulary 可以通过扫描 wikipedia 或英文字典的方式得到.
</p>

<ul class="org-ul">
<li>one-hot</li>
</ul>

<p>
然后我们可以对单词的索引使用 one-hot 编码. 假设 vocabulary 大小为
1000,000, 而 one-hot 编码后 feature 中的每个单词都变 成一个大小为
1000,000 的 vector
</p>

<ul class="org-ul">
<li>then?</li>
</ul>

<p>
feature size 是 [X, 1000,000], X 代表一个样本里有多少个单词.
但与之前的机器问题不同的是, X 大小不是固定的. 对于 <code>I love you</code>, X
为 3, 而对于 <code>hello world</code>, X 为 2
</p></li>
</ol>

<p>
上面这个问题实际上称为 sequence model, 它的 feature 不是固定的大小,
而是一个变长的序列, 还有一些问题,
它的输出也是一个变长的序列(例如语言翻译), 通过 RNN, 我们可以解决这类问题
</p>
</div>
</div>

<div id="outline-container-org0000021" class="outline-3">
<h3 id="org0000021"><span class="section-number-3">1.2.</span> RNN Cell</h3>
<div class="outline-text-3" id="text-1-2">
<p>
RNN 的结构都是类似的, 即序列中的元素依次被同一个 cell 处理, 不同的是, 不同的
cell 内部可以有不同的结构.
</p>

<p>
RNN (例如 torch 的 LSTM) 与 RNN Cell (例如 torch 的 LSTMCell) 的区别在于: LSTM
会自己去处理输入序列, 而 LSTMCell 需要外部自己通过循环去处理输入序列
</p>
</div>

<div id="outline-container-org0000007" class="outline-4">
<h4 id="org0000007"><span class="section-number-4">1.2.1.</span> Basic RNN</h4>
<div class="outline-text-4" id="text-1-2-1">

<div id="org0000003" class="figure">
<p><img src="../extra/rnn.png" alt="rnn.png" />
</p>
<p><span class="figure-number">Figure 1: </span>rnn</p>
</div>

<p>
\(h_t = tanh(W_R(h_{t-1})+W_I(X_t))\)
</p>

<p>
\(y_t = W_Oh_t\)
</p>
</div>

<div id="outline-container-ID-5bba409c-0a66-4fba-9925-961db87e0e58" class="outline-5">
<h5 id="ID-5bba409c-0a66-4fba-9925-961db87e0e58"><span class="section-number-5">1.2.1.1.</span> Basic RNN 难以训练</h5>
<div class="outline-text-5" id="text-1-2-1-1">
<p>
<a href="https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html">https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html</a>
</p>

<p>
现在只考虑 loss 对 W_h 的梯度:
</p>

<p>
假设输入为 x0..xt, 输出为 y0..yt, 根据 BPTT (backprop throught time), 总的 error 为
</p>

<p>
\(E=\sum_{i=0}^{t}{E_i}\), 则 \(\frac{\partial E}{\partial
W_h}=\sum_{i=0}^{t}\ldots\frac{\partial h_t}{\partial h_i}\ldots\), 其中
\(\frac{\partial h_t}{\partial h_i}\) 又需要链式求导, 以 \(\frac{\partial
h_t}{\partial h_0}\) 为例:
</p>

<p>
\(\frac{\partial h_t}{\partial h_0}=\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\ldots\frac{\partial
h_1}{\partial h_0}=\prod_{k=i}^{t-1}\frac{\partial h_{k+1}}{\partial h_k}\)
</p>

<p>
其中 \(\frac{\partial h_t}{\partial h_{t-1}}=tanh'W_h\), 由于 \(tanh'\) 上限为 1, 所以 \(\frac{\partial E_t}{W_h}\) 结果类似于 \(\prod_{i=0}^{t}W_h\)
</p>

<ul class="org-ul">
<li>当 \(W_h < 1\) 时, 发生梯度消失</li>
<li>当 \(W_h > 1\) 时, 发生梯度爆炸</li>
</ul>

<p>
究其原因, 和普通 dnn 中的梯度消失和爆炸问题是一致的: dnn 中的梯度问题的原因是 bp
时中间节点的梯度过大或过小, 对于 rnn 来说, 中间节点的梯度即是 W_h 自身!
</p>

<p>
实际上, \(h_1\) 传播到 \(h_0\) 还是很容易的 (因为连乘中的 \(W_h\) 只有一项), 但 \(h_t\) 传播到 \(h_0\) 就非常困难了 (连乘中 \(W_h\) 有 t 项), 所以 RNN 无法 `记住` 很长的序列
</p>
</div>


<div id="outline-container-org0000004" class="outline-6 references">
<h6 id="org0000004">Backlinks</h6>
<div class="outline-text-6" id="text-org0000004">
<p>
<a href="attention.html#ID-10c23985-ebef-4878-805c-fe79cbb55fae">Attention</a>
(<i>Attention &gt; Self Attention &gt; Self Attention vs. RNN/CNN/FFN &gt; RNN</i>):  2) rnn 通过 hidden state 能传递的 context 有限, 因为 <a href="#ID-5bba409c-0a66-4fba-9925-961db87e0e58">Basic RNN 难以训练</a>, 无法处    理太长的序列
</p>
</div>
</div>
</div>
</div>


<div id="outline-container-org000000b" class="outline-4">
<h4 id="org000000b"><span class="section-number-4">1.2.2.</span> GRU</h4>
<div class="outline-text-4" id="text-1-2-2">

<div id="org000000a" class="figure">
<p><img src="../extra/gru.png" alt="gru.png" />
</p>
</div>
</div>
</div>

<div id="outline-container-org0000015" class="outline-4">
<h4 id="org0000015"><span class="section-number-4">1.2.3.</span> LSTM</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
LSTM 内部比 basic rnn 和 GRU 复杂, 其中除了 H, 还有一个 C 也会在 cell 之间传递,
这个 C 为 cell state, H 为 hidden state
</p>


<div id="org000000e" class="figure">
<p><img src="../extra/lstm.png" alt="lstm.png" />
</p>
<p><span class="figure-number">Figure 2: </span>lstm</p>
</div>

<p>
上图中:
</p>

<ol class="org-ol">
<li>粉色的三个 x 是 gate, 从左到右依次为 forget gate, input gate, output gate</li>
<li>正常情况下 lstm 的输出是最后一个 h_t, 但也可以输出最后一个 (h_t, c_t) 或者所有的 (h_1, h_2, &#x2026; h<sub>t-1</sub>, h_t)</li>
</ol>


<p>
\(f_t=\sigma(W_f[h_{t-1},X_t])\)
\(i_t=\sigma(W_i[h_{t-1},X_t])\)
\(o_t=\sigma(W_o[h_{t-1},X_t])\)
\(c_t=tanh(W_c[h_{t-1},X_t])\)
\(C_t=f_t*C_{t-1}+i_t*c_t\)
\(h_t=o_ttanh(C_t)\)
</p>
</div>


<div id="outline-container-org000000f" class="outline-5">
<h5 id="org000000f"><span class="section-number-5">1.2.3.1.</span> LSTM 可以解决 RNN 的梯度消失问题</h5>
<div class="outline-text-5" id="text-1-2-3-1">
<p>
根本原因在于 forget gate
</p>

<p>
根据 lstm 的公式: \(C_t=f_t*C_{t-1}+i_t*c_t\), 得到 \(\frac{\partial C_t}{\partial
C_{t-1}}=f_t+\ldots\), 其中省略的部分会和 rnn 类似, 包含 \(W_c\) 的值, 但是与仅仅包含 \(W_c\) 的 rnn 不同, lstm 可以通过调整 \(f_t\) 的值 (0~1) 避免
\(\frac{\partial C_t}{\partial C_{t-1}}\) 过小, 从而避免梯度消失, 但无法避免梯度爆炸
</p>
</div>
</div>

<div id="outline-container-org0000012" class="outline-5">
<h5 id="org0000012"><span class="section-number-5">1.2.3.2.</span> <a href="nn_benchmark.html#ID-2fe223a9-d975-44f4-9245-1954084b3a35">LSTM Profiling</a></h5>
</div>
</div>

<div id="outline-container-org000001e" class="outline-4">
<h4 id="org000001e"><span class="section-number-4">1.2.4.</span> Example</h4>
<div class="outline-text-4" id="text-1-2-4">
</div>
<div id="outline-container-org0000018" class="outline-5">
<h5 id="org0000018"><span class="section-number-5">1.2.4.1.</span> tensorflow</h5>
<div class="outline-text-5" id="text-1-2-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> tensorflow <span class="org-keyword">as</span> tf

<span class="org-comment-delimiter"># </span><span class="org-comment">batch size &#20026; 32, &#24207;&#21015;&#38271;&#24230;&#20026; 10, &#24207;&#21015;&#20013;&#27599;&#20010;&#25968;&#25454;&#30340;&#38271;&#24230;&#20026; 8</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#20197; binary adder &#38382;&#39064;&#20026;&#20363;, &#20551;&#35774;&#36755;&#20837;&#25968;&#25454;&#26159;&#20004;&#20010;&#22235;&#20301;&#20108;&#36827;&#21046;&#24418;&#24335;&#30340;&#25968;, &#20363;&#22914; 1010+0101</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#21017; inputs shape &#20026; [batch, 4, 2], 4 &#34920;&#31034;&#24207;&#21015;&#38271;&#24230;&#20026; 4, 2 &#34920;&#31034;&#20004;&#20010;&#25968;&#30456;&#21152;</span>
<span class="org-variable-name">inputs</span> = tf.random.normal([32, 10, 8])
<span class="org-comment-delimiter"># </span><span class="org-comment">LSTM cell &#30340; units &#20026; 4, &#25351; H(&#21644; C) &#30340;&#22823;&#23567;&#20026;4</span>
<span class="org-variable-name">rnn</span> = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))
<span class="org-variable-name">output</span> = rnn(inputs)
<span class="org-comment-delimiter"># </span><span class="org-comment">&#27491;&#24120;&#24773;&#20917;&#19979; RNN &#30340;&#36755;&#20986;&#26159;&#26368;&#21518;&#19968;&#20010; cell &#30340; H, &#25152;&#20197;&#36825;&#37324;&#30340;&#32467;&#26524;&#26159; [32,4]</span>
<span class="org-builtin">print</span>(output.shape)

<span class="org-comment-delimiter"># </span><span class="org-comment">return_sequences &#34920;&#31034; rnn &#36820;&#22238; sequence &#30340;&#22823;&#23567;(&#21508;&#20010; cell &#30340; H &#32452;&#25104;&#30340; sequence)</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#32780;&#19981;&#26159;&#26368;&#21518;&#19968;&#20010; cell &#30340; H</span>
<span class="org-comment-delimiter">#</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">return_state &#26159;&#25351;&#36820;&#22238;&#26368;&#21518;&#19968;&#20010; cell &#30340;&#25152;&#26377; state, &#23545;&#20110; LSTM &#21363; C</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">(final_carry_state) &#21644; H (final_memory_state)</span>
<span class="org-variable-name">rnn</span> = tf.keras.layers.RNN(
    tf.keras.layers.LSTMCell(4), return_sequences=<span class="org-constant">True</span>, return_state=<span class="org-constant">True</span>
)
whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)

<span class="org-builtin">print</span>(whole_seq_output.shape)
<span class="org-builtin">print</span>(final_memory_state.shape)
<span class="org-builtin">print</span>(final_carry_state.shape)
</pre>
</div>

<p>
(32, 4)
(32, 10, 4)
(32, 4)
(32, 4)
</p>
</div>
</div>

<div id="outline-container-org000001b" class="outline-5">
<h5 id="org000001b"><span class="section-number-5">1.2.4.2.</span> torch</h5>
<div class="outline-text-5" id="text-1-2-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter"># </span><span class="org-comment">input &#21521;&#37327;&#38271;&#24230;&#20026; 10, hidden state &#38271;&#24230;&#20026; 20, 2 &#23618; ltsm cell stack &#22312;&#19968;&#36215;</span>
<span class="org-variable-name">rnn</span> = nn.LSTM(10, 20, 2)
<span class="org-comment-delimiter"># </span><span class="org-comment">batch &#20026; 3, &#24207;&#21015;&#38271;&#24230;&#20026; 5, &#22240;&#20026; nn.LSTM &#27809;&#26377;&#25351;&#23450; batch_first, &#25152;&#20197; first(5) &#26159;</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">&#24207;&#21015;&#38271;&#24230;&#32780;&#19981;&#26159; batch</span>
<span class="org-builtin">input</span> = torch.randn(5, 3, 10)
<span class="org-comment-delimiter"># </span><span class="org-comment">h0,c0 &#30340;&#21021;&#22987;&#29366;&#24577; (layer, batch, hidden_size)</span>
<span class="org-variable-name">h0</span> = torch.randn(2, 3, 20)
<span class="org-variable-name">c0</span> = torch.randn(2, 3, 20)
output, (<span class="org-variable-name">hn</span>, <span class="org-variable-name">cn</span>) = rnn(<span class="org-builtin">input</span>, (h0, c0))
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000024" class="outline-3">
<h3 id="org0000024"><span class="section-number-3">1.3.</span> <a href="word2vec.html#ID-164e18f2-14b2-486b-8e39-8c69d9f09bc1">Word2vec</a></h3>
</div>

<div id="outline-container-org0000027" class="outline-3">
<h3 id="org0000027"><span class="section-number-3">1.4.</span> <a href="attention.html#ID-2c8d1d7d-388d-4734-b55d-e50ad26a511b">Seq2Seq Model</a></h3>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: <br />
Last updated: 2022-01-25 Tue 14:14</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
