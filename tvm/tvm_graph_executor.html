<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-26 Wed 00:35 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>TVM Graph Executor</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">TVM Graph Executor</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgad02625">1. TVM Graph Executor</a>
<ul>
<li><a href="#orge82ea9a">1.1. Example</a></li>
<li><a href="#orga188d35">1.2. Impl</a>
<ul>
<li><a href="#orgd4d0d1e">1.2.1. init</a></li>
<li><a href="#orga28da50">1.2.2. run</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgad02625" class="outline-2">
<h2 id="orgad02625"><span class="section-number-2">1</span> TVM Graph Executor</h2>
<div class="outline-text-2" id="text-1">
<p>
对于一个 nn 来说, codegen 会编译成多个 function, function 之间的调用关系由
relay.build 返回的 graph 来描述.
</p>

<p>
graph executor 的作用是根据 graph 依次执行所有相关的 module
</p>
</div>

<div id="outline-container-orge82ea9a" class="outline-3">
<h3 id="orge82ea9a"><span class="section-number-3">1.1</span> Example</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
import tvm
from tvm import relay

a = relay.var("a", shape=(1, 10), dtype="float32")
b = relay.var("b", shape=(1, 10), dtype="float32")
c = relay.var("c", shape=(1, 10), dtype="float32")
out = relay.add(a, b)
out = relay.add(out, c)

func = relay.Function([a, b, c], out)
mod = tvm.IRModule.from_expr(func)

print(mod)
with tvm.transform.PassContext(opt_level=0):
    graph, lib, params = relay.build(mod, target="c", params=None)
print(graph)

# x, y, z = np.ones((1, 10)), np.ones((1, 10)), np.ones((1, 10))
# intrp = relay.create_executor("graph", device=tvm.cpu(0), target="llvm")
# op_res = intrp.evaluate(func)(x, y, z)
</pre>
</div>

<p>
def @main(%a: Tensor[(1, 10), float32], %b: Tensor[(1, 10), float32], %c: Tensor[(1, 10), float32]) {
  %0 = add(%a, %b);
  add(%0, %c)
}
</p>

<p>
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "c", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_inputs": "2", 
        "num_outputs": "1", 
        "hash": "8ec3c08331cd92b5", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_inputs": "2", 
        "num_outputs": "1", 
        "hash": "8ec3c08331cd92b5", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          2, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1, 2], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}

/tmp/ipykernel_218993/2066100685.py:16: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.
  graph, lib, params = relay.build(mod, target="c", params=None)
</p>

<div class="org-src-container">
<pre class="src src-ipython">import numpy as np

x, y, z = np.ones((1, 10)), np.ones((1, 10)), np.ones((1, 10))
intrp = relay.create_executor("graph", device=tvm.cpu(0), target="llvm")
op_res = intrp.evaluate(func)(x, y, z)
print(op_res)
</pre>
</div>
</div>
</div>

<div id="outline-container-orga188d35" class="outline-3">
<h3 id="orga188d35"><span class="section-number-3">1.2</span> Impl</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-orgd4d0d1e" class="outline-4">
<h4 id="orgd4d0d1e"><span class="section-number-4">1.2.1</span> init</h4>
<div class="outline-text-4" id="text-1-2-1">
<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">runtime</span>::<span style="font-weight: bold; text-decoration: underline;">PackedFunc</span>* <span style="font-weight: bold; font-style: italic;">graph_executor</span> = <span style="font-weight: bold; text-decoration: underline;">tvm</span>::<span style="font-weight: bold; text-decoration: underline;">runtime</span>::<span style="font-weight: bold; text-decoration: underline;">Registry</span>::Get(<span style="font-style: italic;">"tvm.graph_executor.create"</span>);
  <span style="font-weight: bold; text-decoration: underline;">Module</span> <span style="font-weight: bold;">GraphExecutorCreate</span>(<span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">std</span>::<span style="font-weight: bold; text-decoration: underline;">string</span>&amp; <span style="font-weight: bold; font-style: italic;">sym_json</span>, <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">tvm</span>::<span style="font-weight: bold; text-decoration: underline;">runtime</span>::<span style="font-weight: bold; text-decoration: underline;">Module</span>&amp; <span style="font-weight: bold; font-style: italic;">m</span>,
                           <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">std</span>::<span style="font-weight: bold; text-decoration: underline;">vector</span>&lt;Device&gt;&amp; <span style="font-weight: bold; font-style: italic;">devs</span>,
                           <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">PackedFunc</span> <span style="font-weight: bold; font-style: italic;">lookup_linked_param_func</span>)
    <span style="font-weight: bold;">auto</span> <span style="font-weight: bold; font-style: italic;">exec</span> = make_object&lt;GraphExecutor&gt;();
    exec-&gt;Init(sym_json, m, devs, lookup_linked_param_func);
    <span style="font-weight: bold;">return</span> Module(exec);

<span style="font-weight: bold; text-decoration: underline;">Init</span>:
  <span style="font-weight: bold;">this</span>-&gt;SetupStorage();
  <span style="font-weight: bold;">this</span>-&gt;SetupOpExecs();
</pre>
</div>
</div>

<div id="outline-container-org7340fa7" class="outline-5">
<h5 id="org7340fa7"><span class="section-number-5">1.2.1.1</span> graph format</h5>
<div class="outline-text-5" id="text-1-2-1-1">
<pre class="example" id="org7ee47b6">
# 一共有 5 个 node
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "c", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      },
      # 第一个 add 操作有两个 input:
      # 第1个为 [0,0,0], 表示 node[0] 的 第 0 个输出, 即 a
      # 第2个为 [1,0,0], 表示 node[1] 的 第 0 个输出, 即 b
      # 这三个数的意义为 [node,index,version]
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      },
      # 第二个 add 操作的两个 input:
      # 第1个为 [3,0,0], 表示 node[3] 的 第 0 个输出, 即第一个 add 操作的唯一的输出
      # 第2个为 [2,0,0], 表示 node[2] 的 第 0 个输出, 即 c
      # 这三个数的意义为 [node,index,version]      
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          2, 
          0, 
          0
        ]
      ]
    }
  ],
  # arg_nodes 表示模型的输入 node, 即 a,b,c 三个 node 是输入
  "arg_nodes": [0, 1, 2],
  # heads 表示模型的输出, [4,0,0] 即 node[4] 的第 0 个输出, 即第二个 add 操作的输出
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ],
    #storage_id 是代表的是每个输出的 storage_id, 如果两个输出不会同时使用, 则它
    #们可能使用相同的 storage_id 以节省内存, 由于 storage_id 在 graph 中已经计算
    #好, SetupStorage 的代码会比较简单
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ]
  },
  # node_row_ptr 用来快速给每一个输出编码, 进而获得它对应的 storage, 例如
  # 三个 node 输出个数分别为 1,3,1, 则 node_row_ptr 为
  # [0, 1, 4, 5]
  # 则 node[0][0] 为 0, node[2,0] = node_row_ptr[2]+0=4
  # 相当于把二维的 [node,index] 转换为一维, 参考 entry_id 函数
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}

</pre>
</div>
</div>

<div id="outline-container-org90f873c" class="outline-5">
<h5 id="org90f873c"><span class="section-number-5">1.2.1.2</span> SetupStorage</h5>
<div class="outline-text-5" id="text-1-2-1-2">
<p>
SetupStorage 的作用基本上是:
</p>
</div>

<div id="outline-container-org2ecdd0d" class="outline-6">
<h6 id="org2ecdd0d"><span class="section-number-6">1.2.1.2.1</span> device_index</h6>
<div class="outline-text-6" id="text-1-2-1-2-1">
<p>
根据 graph 中的 attrs.device_index 确定 expr 使用的 device, 将来会在 device
上分配 DLTensor
</p>

<p>
其中 attris.device_index 是由 GraphExecutorCodegen 生成的 (根据 on_device
annotation）
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold;">for</span> (<span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>&amp; <span style="font-weight: bold; font-style: italic;">pit</span> : pool_entry) {
    <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">This for loop is very fast since there are usually only a couple of</span>
    <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">devices available on the same hardware.</span>
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>&amp; <span style="font-weight: bold; font-style: italic;">cit</span> =
        <span style="font-weight: bold; text-decoration: underline;">std</span>::find_if(devices_.begin(), devices_.end(), [&amp;<span style="font-weight: bold; font-style: italic;">pit</span>](<span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Device</span>&amp; <span style="font-weight: bold; font-style: italic;">d</span>) {
            <span style="font-weight: bold;">return</span> pit.device_type == <span style="font-weight: bold;">static_cast</span>&lt;<span style="font-weight: bold; text-decoration: underline;">int</span>&gt;(d.device_type);
        });
    <span style="font-weight: bold; text-decoration: underline;">Device</span> <span style="font-weight: bold; font-style: italic;">dev</span> = cit == devices_.end() ? devices_[0] : *cit;

    <span style="font-weight: bold; text-decoration: underline;">std</span>::<span style="font-weight: bold; text-decoration: underline;">vector</span>&lt;<span style="font-weight: bold; text-decoration: underline;">int64_t</span>&gt; <span style="font-weight: bold; font-style: italic;">shape</span>;
    shape.push_back(<span style="font-weight: bold;">static_cast</span>&lt;<span style="font-weight: bold; text-decoration: underline;">int64_t</span>&gt;(pit.size + 3) / 4);
    storage_pool_.push_back(
        <span style="font-weight: bold; text-decoration: underline;">NDArray</span>::Empty(shape, DLDataType{kDLFloat, 32, 1}, dev));
}

<span style="font-weight: bold; text-decoration: underline;">NDArray</span> <span style="font-weight: bold; text-decoration: underline;">NDArray</span>::<span style="font-weight: bold;">Empty</span>(
    <span style="font-weight: bold; text-decoration: underline;">ShapeTuple</span> <span style="font-weight: bold; font-style: italic;">shape</span>, <span style="font-weight: bold; text-decoration: underline;">DLDataType</span> <span style="font-weight: bold; font-style: italic;">dtype</span>, <span style="font-weight: bold; text-decoration: underline;">Device</span> <span style="font-weight: bold; font-style: italic;">dev</span>,
    <span style="font-weight: bold; text-decoration: underline;">Optional</span>&lt;String&gt; <span style="font-weight: bold; font-style: italic;">mem_scope</span>) {
    <span style="font-weight: bold; text-decoration: underline;">NDArray</span> <span style="font-weight: bold; font-style: italic;">ret</span> = <span style="font-weight: bold; text-decoration: underline;">Internal</span>::Create(shape, dtype, dev);
    ret.get_mutable()-&gt;dl_tensor.data =
        <span style="font-weight: bold; text-decoration: underline;">DeviceAPI</span>::Get(ret-&gt;device)
            -&gt;AllocDataSpace(
                ret-&gt;device, shape.size(), shape.data(), ret-&gt;dtype, mem_scope);
    <span style="font-weight: bold;">return</span> ret;
}

<span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">opencl example</span>
<span style="font-weight: bold; text-decoration: underline;">void</span>* <span style="font-weight: bold; text-decoration: underline;">OpenCLWorkspace</span>::<span style="font-weight: bold;">AllocDataSpace</span>(
    <span style="font-weight: bold; text-decoration: underline;">Device</span> <span style="font-weight: bold; font-style: italic;">dev</span>, <span style="font-weight: bold; text-decoration: underline;">size_t</span> <span style="font-weight: bold; font-style: italic;">size</span>, <span style="font-weight: bold; text-decoration: underline;">size_t</span> <span style="font-weight: bold; font-style: italic;">alignment</span>, <span style="font-weight: bold; text-decoration: underline;">DLDataType</span> <span style="font-weight: bold; font-style: italic;">type_hint</span>) {
    <span style="font-weight: bold; text-decoration: underline;">cl</span>::<span style="font-weight: bold; text-decoration: underline;">BufferDescriptor</span>* <span style="font-weight: bold; font-style: italic;">desc</span> = <span style="font-weight: bold;">new</span> <span style="font-weight: bold; text-decoration: underline;">cl</span>::<span style="font-weight: bold; text-decoration: underline;">BufferDescriptor</span>;
    desc-&gt;buffer = clCreateBuffer(
        <span style="font-weight: bold;">this</span>-&gt;context, CL_MEM_READ_WRITE, size, <span style="font-weight: bold; text-decoration: underline;">nullptr</span>, &amp;err_code);
    desc-&gt;layout = <span style="font-weight: bold; text-decoration: underline;">cl</span>::<span style="font-weight: bold; text-decoration: underline;">BufferDescriptor</span>::<span style="font-weight: bold; text-decoration: underline;">MemoryLayout</span>::kBuffer1D;
    <span style="font-weight: bold;">return</span> desc;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org09bb147" class="outline-6">
<h6 id="org09bb147"><span class="section-number-6">1.2.1.2.2</span> storage_id</h6>
<div class="outline-text-6" id="text-1-2-1-2-2">
<p>
根据 graph 中的 attrs.storage_id 给每一个输出设置一个 DLTensor, 保存在
data_entry_ 中
</p>

<p>
attris.storage_id 也是在 GraphExecutorCodegen 生成的
</p>

<div class="org-src-container">
<pre class="src src-c++">data_entry_.resize(num_node_entries());
data_alignment_.resize(num_node_entries());
<span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">size_t</span> <span style="font-weight: bold; font-style: italic;">i</span> = 0; i &lt; data_entry_.size(); ++i) {
    <span style="font-weight: bold; text-decoration: underline;">int</span> <span style="font-weight: bold; font-style: italic;">storage_id</span> = attrs_.storage_id[i];
    ICHECK_LT(<span style="font-weight: bold;">static_cast</span>&lt;<span style="font-weight: bold; text-decoration: underline;">size_t</span>&gt;(storage_id), storage_pool_.size());
    data_entry_[i] = storage_pool_[storage_id].CreateView(attrs_.shape[i], vtype[i]);

    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">DLTensor</span>* <span style="font-weight: bold; font-style: italic;">tmp</span> = data_entry_[i].<span style="font-weight: bold;">operator</span>-&gt;();
}
</pre>
</div>
</div>

<ol class="org-ol">
<li><a id="org12cd570"></a>storage_id example<br />
<div class="outline-text-7" id="text-1-2-1-2-2-1">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay

<span style="font-weight: bold; font-style: italic;">a</span> = relay.var(<span style="font-style: italic;">"a"</span>, shape=(1, 10), dtype=<span style="font-style: italic;">"float32"</span>)
<span style="font-weight: bold; font-style: italic;">b</span> = relay.var(<span style="font-style: italic;">"b"</span>, shape=(1, 10), dtype=<span style="font-style: italic;">"float32"</span>)
<span style="font-weight: bold; font-style: italic;">out1</span> = relay.add(a, b)
<span style="font-weight: bold; font-style: italic;">out2</span> = relay.add(out1, a)
<span style="font-weight: bold; font-style: italic;">out</span> = relay.add(out2, b)

<span style="font-weight: bold; font-style: italic;">func</span> = relay.Function([a, b], out)
<span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule.from_expr(func)

<span style="font-weight: bold;">with</span> tvm.transform.PassContext(opt_level=0):
    <span style="font-weight: bold; font-style: italic;">graph</span>, <span style="font-weight: bold; font-style: italic;">lib</span>, <span style="font-weight: bold; font-style: italic;">params</span> = relay.build(mod, target=<span style="font-style: italic;">"c"</span>, params=<span style="font-weight: bold; text-decoration: underline;">None</span>)
<span style="font-weight: bold;">print</span>(graph)

</pre>
</div>

<p>
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          2, 
          0, 
          0
        ], 
        [
          0, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add2", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 2]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}
</p>

<p>
storage_id 为 [0, 1, 2, 3, 2]，因为 output (即 [5,0,0]) 与 out1 (即 [2,0,0]) 使
用会相同的 DLTensor
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgf3fd26e" class="outline-5">
<h5 id="orgf3fd26e"><span class="section-number-5">1.2.1.3</span> GraphExecutorCodegen</h5>
<div class="outline-text-5" id="text-1-2-1-3">
<p>
SetupStorage 时需的 graph 信息，包括 device_index 和 storage_id, 都来自
GraphExecutorCodegen, 具体的, 来自 StaticMemoryPlan
</p>

<p>
例如:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-09-08 18:19</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.contrib <span style="font-weight: bold;">import</span> graph_executor
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_on_device</span>():
    <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(1, 5))
    <span style="font-weight: bold; font-style: italic;">y</span> = relay.var(<span style="font-style: italic;">"y"</span>, shape=(1, 5))
    <span style="font-weight: bold; font-style: italic;">x_data</span> = np.random.rand(1, 5).astype(<span style="font-style: italic;">"float32"</span>)
    <span style="font-weight: bold; font-style: italic;">y_data</span> = np.random.rand(1, 5).astype(<span style="font-style: italic;">"float32"</span>)

    <span style="font-weight: bold; font-style: italic;">cpu_dev</span> = tvm.device(<span style="font-style: italic;">"cpu"</span>)
    <span style="font-weight: bold; font-style: italic;">opencl_dev</span> = tvm.device(<span style="font-style: italic;">"opencl"</span>)

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">get_function</span>():
        <span style="font-weight: bold; font-style: italic;">add</span> = relay.add(x, y)
        <span style="font-weight: bold; font-style: italic;">_add</span> = relay.annotation.on_device(add, opencl_dev)
        <span style="font-weight: bold; font-style: italic;">log</span> = relay.log(_add)
        <span style="font-weight: bold; font-style: italic;">func</span> = relay.Function([x, y], log)
        <span style="font-weight: bold;">return</span> func

    <span style="font-weight: bold; font-style: italic;">func</span> = get_function()
    <span style="font-weight: bold;">print</span>(func)

    <span style="font-weight: bold;">with</span> tvm.transform.PassContext(opt_level=1):
        <span style="font-weight: bold; font-style: italic;">lib</span> = relay.build(
            func, {<span style="font-style: italic;">"cpu"</span>: <span style="font-style: italic;">"llvm"</span>, <span style="font-style: italic;">"opencl"</span>: <span style="font-style: italic;">"opencl"</span>}, params={<span style="font-style: italic;">"x"</span>: x_data, <span style="font-style: italic;">"y"</span>: y_data}
        )
        <span style="font-weight: bold;">print</span>(lib.graph_json)
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">rt_mod = graph_executor.GraphModule(</span>
        <span style="font-weight: bold; font-style: italic;">#     </span><span style="font-weight: bold; font-style: italic;">lib["default"](tvm.cpu(0), tvm.device("opencl"))</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">)</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">rt_mod.run()</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">tvm_res = rt_mod.get_output(0).numpy()</span>
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print(tvm_res)</span>


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_on_device()
</pre>
</div>

<p>
fn (%x: Tensor[(1, 5), float32], %y: Tensor[(1, 5), float32]) {
  %0 = add(%x, %y);
  %1 = on_device(%0, device_type=4);
  log(%1)
}
{
  "nodes": [
    {
      "op": "null", 
      "name": "p0", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "p1", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "77c3516efdd817bd"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "__copy", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "__copy", 
        "hash": "b78177ac726e601e"
      }, 
      "inputs": [
        [
          2, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_log", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_log", 
        "hash": "8a435bddfed54dab"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "device_index": [
      "list_int", 
      [4, 4, 4, 1, 1]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 5], 
        [1, 5], 
        [1, 5], 
        [1, 5], 
        [1, 5]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}
</p>

<p>
device_index 为 [4,4,4,1,1], 意味着 node[2] (即 add) 的输出会被分配在 device 4
(opencl) 上, 同时它的输入 (node[0],node[1], 即 x,y) 也需要分配在 opencl 上
</p>

<p>
GraphExecutor 的 set_input 会负责最终调用 opencl 的 CopyDataFromTo 把输入复制到
x, y
</p>
</div>
</div>

<div id="outline-container-orgc7678cb" class="outline-5">
<h5 id="orgc7678cb"><span class="section-number-5">1.2.1.4</span> SetupOpExecs</h5>
<div class="outline-text-5" id="text-1-2-1-4">
<p>
SetupOpExecs 的作用有两个:
</p>

<ol class="org-ol">
<li>找到所有 PackedFunc (通过 module-&gt;GetFunction)</li>
<li>确定每个 PackedFunc 的参数和返回值对应的具体的 DLTensor (使用 SetupStorage 时分配的 data_entry_)</li>
</ol>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold; text-decoration: underline;">void</span> <span style="font-weight: bold; text-decoration: underline;">GraphExecutor</span>::<span style="font-weight: bold;">SetupOpExecs</span>() {
    <span style="font-weight: bold; text-decoration: underline;">std</span>::<span style="font-weight: bold; text-decoration: underline;">unordered_set</span>&lt;<span style="font-weight: bold; text-decoration: underline;">uint32_t</span>&gt; <span style="font-weight: bold; font-style: italic;">input_node_eids</span>;
    <span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">size_t</span> <span style="font-weight: bold; font-style: italic;">i</span> = 0; i &lt; input_nodes_.size(); i++) {
        <span style="font-weight: bold; text-decoration: underline;">uint32_t</span> <span style="font-weight: bold; font-style: italic;">nid</span> = input_nodes_[i];
        input_node_eids.insert(entry_id(nid, 0));
    }

    <span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">uint32_t</span> <span style="font-weight: bold; font-style: italic;">nid</span> = 0; nid &lt; <span style="font-weight: bold;">this</span>-&gt;GetNumOfNodes(); ++nid) {
        <span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>&amp; <span style="font-weight: bold; font-style: italic;">inode</span> = nodes_[nid];
        <span style="font-weight: bold; text-decoration: underline;">std</span>::<span style="font-weight: bold; text-decoration: underline;">vector</span>&lt;DLTensor&gt; <span style="font-weight: bold; font-style: italic;">args</span>;
        <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#36755;&#20837;&#21442;&#25968;</span>
        <span style="font-weight: bold;">for</span> (<span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>&amp; <span style="font-weight: bold; font-style: italic;">e</span> : inode.inputs) {
            <span style="font-weight: bold; text-decoration: underline;">uint32_t</span> <span style="font-weight: bold; font-style: italic;">eid</span> = <span style="font-weight: bold;">this</span>-&gt;entry_id(e);
            <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#20351;&#29992; data_entry_ &#20013;&#20998;&#37197;&#32473; eid &#30340; DLTensor, &#26377;&#21487;&#33021;&#19981;&#21516;&#30340; eid &#26368;&#32456;</span>
            <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#20250;&#20351;&#29992;&#30456;&#21516;&#30340; DLTensor, &#22240;&#20026;&#23427;&#20204;&#26377;&#30456;&#21516;&#30340; storage_id</span>
            args.push_back(*(data_entry_[eid].<span style="font-weight: bold;">operator</span>-&gt;()));
        }
        <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#36755;&#20986;</span>
        <span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">uint32_t</span> <span style="font-weight: bold; font-style: italic;">index</span> = 0; index &lt; inode.param.num_outputs; ++index) {
            <span style="font-weight: bold; text-decoration: underline;">uint32_t</span> <span style="font-weight: bold; font-style: italic;">eid</span> = <span style="font-weight: bold;">this</span>-&gt;entry_id(nid, index);
            args.push_back(*(data_entry_[eid].<span style="font-weight: bold;">operator</span>-&gt;()));
        }

        <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#36755;&#20837;&#21644;&#36755;&#20986;&#30340; DLTensor &#37117;&#25171;&#21253;&#22312;&#21516;&#19968;&#20010; args &#20013;&#20570;&#20026; tvm op &#30340;&#21442;&#25968;</span>
        <span style="font-weight: bold; text-decoration: underline;">std</span>::tie(op_execs_[nid], op_args) =
            CreateTVMOp(inode.param, args, inode.inputs.size());
    }
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orga28da50" class="outline-4">
<h4 id="orga28da50"><span class="section-number-4">1.2.2</span> run</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
执行 GraphExecutor 的代码大约是这样:
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold; text-decoration: underline;">PackedFunc</span> <span style="font-weight: bold; font-style: italic;">set_input</span> = mod.GetFunction(<span style="font-style: italic;">"set_input"</span>, <span style="font-weight: bold; text-decoration: underline;">false</span>);
<span style="font-weight: bold; text-decoration: underline;">PackedFunc</span> <span style="font-weight: bold; font-style: italic;">run</span> = mod.GetFunction(<span style="font-style: italic;">"run"</span>, <span style="font-weight: bold; text-decoration: underline;">false</span>);
<span style="font-weight: bold; text-decoration: underline;">PackedFunc</span> <span style="font-weight: bold; font-style: italic;">get_output</span> = mod.GetFunction(<span style="font-style: italic;">"get_output"</span>, <span style="font-weight: bold; text-decoration: underline;">false</span>);
set_input(<span style="font-style: italic;">"A"</span>, a_val);
set_input(<span style="font-style: italic;">"B"</span>, b_val);
set_input(<span style="font-style: italic;">"C"</span>, c_val);
run();
<span style="font-weight: bold; text-decoration: underline;">tvm</span>::<span style="font-weight: bold; text-decoration: underline;">runtime</span>::<span style="font-weight: bold; text-decoration: underline;">NDArray</span> <span style="font-weight: bold; font-style: italic;">out</span> = get_output(0);
</pre>
</div>

<p>
其中 "run" 的实现为:
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold; text-decoration: underline;">void</span> <span style="font-weight: bold; text-decoration: underline;">GraphExecutor</span>::<span style="font-weight: bold;">Run</span>() {
  <span style="font-weight: bold;">for</span> (<span style="font-weight: bold; text-decoration: underline;">size_t</span> <span style="font-weight: bold; font-style: italic;">i</span> = 0; i &lt; op_execs_.size(); ++i) {
    <span style="font-weight: bold;">if</span> (op_execs_[i]) op_execs_[i]();
  }
}
</pre>
</div>

<p>
而 op_execs_ 即各个 module 中针对 graph 中的 symbol 的具体的实现, 例如
DNNLJSONRuntime 中的 tvmgen_default_dnnl_main_0
</p>

<p>
由于各个 op_execs_ 的输入输出在上一步的 SetupOpExecs 时已经设置好了, 这时只负责
执行即可, 不需要再考虑参数和返回值的问题.
</p>

<p>
另外, 由于 op_execs_ 是线性执行的, 所以生成 graph 时需要保证是一个拓扑排序
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-09-24 Fri 00:00<br />
Last updated: 2022-01-24 Mon 21:41</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
