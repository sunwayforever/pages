<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>TVM Graph Executor</title>


<link rel="stylesheet" type="text/css" href="/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="./htmlize.css"/>
<link rel="stylesheet" type="text/css" href="../htmlize.css"/>
<link rel="stylesheet" type="text/css" href="/readtheorg.css"/>
<link rel="stylesheet" type="text/css" href="./readtheorg.css"/>
<link rel="stylesheet" type="text/css" href="../readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/src/readtheorg_theme/js/readtheorg.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
</head>
<body>
<div id="content" class="content">
<h1 class="title">TVM Graph Executor</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org0000022">1. TVM Graph Executor</a>
<ul>
<li><a href="#org0000000">1.1. Example</a></li>
<li><a href="#org000001f">1.2. Impl</a>
<ul>
<li><a href="#org0000019">1.2.1. init</a></li>
<li><a href="#org000001c">1.2.2. run</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000022" class="outline-2">
<h2 id="org0000022"><span class="section-number-2">1.</span> TVM Graph Executor</h2>
<div class="outline-text-2" id="text-1">
<p>
对于一个 nn 来说, codegen 会编译成多个 function, function 之间的调用关系由
relay.build 返回的 graph 来描述.
</p>

<p>
graph executor 的作用是根据 graph 依次执行所有相关的 module
</p>
</div>

<div id="outline-container-org0000000" class="outline-3">
<h3 id="org0000000"><span class="section-number-3">1.1.</span> Example</h3>
<div class="outline-text-3" id="text-1-1">
<div class="org-src-container">
<pre class="src src-ipython">import numpy as np
import tvm
from tvm import relay

a = relay.var("a", shape=(1, 10), dtype="float32")
b = relay.var("b", shape=(1, 10), dtype="float32")
c = relay.var("c", shape=(1, 10), dtype="float32")
out = relay.add(a, b)
out = relay.add(out, c)

func = relay.Function([a, b, c], out)
mod = tvm.IRModule.from_expr(func)

print(mod)
with tvm.transform.PassContext(opt_level=0):
    graph, lib, params = relay.build(mod, target="c", params=None)
print(graph)

# x, y, z = np.ones((1, 10)), np.ones((1, 10)), np.ones((1, 10))
# intrp = relay.create_executor("graph", device=tvm.cpu(0), target="llvm")
# op_res = intrp.evaluate(func)(x, y, z)
</pre>
</div>

<p>
def @main(%a: Tensor[(1, 10), float32], %b: Tensor[(1, 10), float32], %c: Tensor[(1, 10), float32]) {
  %0 = add(%a, %b);
  add(%0, %c)
}
</p>

<p>
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "c", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_inputs": "2", 
        "num_outputs": "1", 
        "hash": "8ec3c08331cd92b5", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_inputs": "2", 
        "num_outputs": "1", 
        "hash": "8ec3c08331cd92b5", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          2, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1, 2], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}

/tmp/ipykernel_218993/2066100685.py:16: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.
  graph, lib, params = relay.build(mod, target="c", params=None)
</p>

<div class="org-src-container">
<pre class="src src-ipython">import numpy as np

x, y, z = np.ones((1, 10)), np.ones((1, 10)), np.ones((1, 10))
intrp = relay.create_executor("graph", device=tvm.cpu(0), target="llvm")
op_res = intrp.evaluate(func)(x, y, z)
print(op_res)
</pre>
</div>
</div>
</div>

<div id="outline-container-org000001f" class="outline-3">
<h3 id="org000001f"><span class="section-number-3">1.2.</span> Impl</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org0000019" class="outline-4">
<h4 id="org0000019"><span class="section-number-4">1.2.1.</span> init</h4>
<div class="outline-text-4" id="text-1-2-1">
<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">const</span> <span class="org-constant">runtime</span>::<span class="org-type">PackedFunc</span>* <span class="org-variable-name">graph_executor</span> = <span class="org-constant">tvm</span>::<span class="org-constant">runtime</span>::<span class="org-constant">Registry</span>::Get(<span class="org-string">"tvm.graph_executor.create"</span>);
  <span class="org-type">Module</span> <span class="org-function-name">GraphExecutorCreate</span>(<span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">string</span>&amp; <span class="org-variable-name">sym_json</span>, <span class="org-keyword">const</span> <span class="org-constant">tvm</span>::<span class="org-constant">runtime</span>::<span class="org-type">Module</span>&amp; <span class="org-variable-name">m</span>,
                           <span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">vector</span>&lt;Device&gt;&amp; <span class="org-variable-name">devs</span>,
                           <span class="org-keyword">const</span> <span class="org-type">PackedFunc</span> <span class="org-variable-name">lookup_linked_param_func</span>)
    <span class="org-keyword">auto</span> <span class="org-variable-name">exec</span> = make_object&lt;GraphExecutor&gt;();
    exec-&gt;Init(sym_json, m, devs, lookup_linked_param_func);
    <span class="org-keyword">return</span> Module(exec);

<span class="org-constant">Init</span>:
  <span class="org-keyword">this</span>-&gt;SetupStorage();
  <span class="org-keyword">this</span>-&gt;SetupOpExecs();
</pre>
</div>
</div>

<div id="outline-container-org0000004" class="outline-5">
<h5 id="org0000004"><span class="section-number-5">1.2.1.1.</span> graph format</h5>
<div class="outline-text-5" id="text-1-2-1-1">
<pre class="example" id="org0000003">
# 一共有 5 个 node
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "c", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      },
      # 第一个 add 操作有两个 input:
      # 第1个为 [0,0,0], 表示 node[0] 的 第 0 个输出, 即 a
      # 第2个为 [1,0,0], 表示 node[1] 的 第 0 个输出, 即 b
      # 这三个数的意义为 [node,index,version]
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      },
      # 第二个 add 操作的两个 input:
      # 第1个为 [3,0,0], 表示 node[3] 的 第 0 个输出, 即第一个 add 操作的唯一的输出
      # 第2个为 [2,0,0], 表示 node[2] 的 第 0 个输出, 即 c
      # 这三个数的意义为 [node,index,version]      
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          2, 
          0, 
          0
        ]
      ]
    }
  ],
  # arg_nodes 表示模型的输入 node, 即 a,b,c 三个 node 是输入
  "arg_nodes": [0, 1, 2],
  # heads 表示模型的输出, [4,0,0] 即 node[4] 的第 0 个输出, 即第二个 add 操作的输出
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ],
    #storage_id 是代表的是每个输出的 storage_id, 如果两个输出不会同时使用, 则它
    #们可能使用相同的 storage_id 以节省内存, 由于 storage_id 在 graph 中已经计算
    #好, SetupStorage 的代码会比较简单
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ]
  },
  # node_row_ptr 用来快速给每一个输出编码, 进而获得它对应的 storage, 例如
  # 三个 node 输出个数分别为 1,3,1, 则 node_row_ptr 为
  # [0, 1, 4, 5]
  # 则 node[0][0] 为 0, node[2,0] = node_row_ptr[2]+0=4
  # 相当于把二维的 [node,index] 转换为一维, 参考 entry_id 函数
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}

</pre>
</div>
</div>

<div id="outline-container-org0000010" class="outline-5">
<h5 id="org0000010"><span class="section-number-5">1.2.1.2.</span> SetupStorage</h5>
<div class="outline-text-5" id="text-1-2-1-2">
<p>
SetupStorage 的作用基本上是:
</p>
</div>

<div id="outline-container-org0000007" class="outline-6">
<h6 id="org0000007"><span class="section-number-6">1.2.1.2.1.</span> device_index</h6>
<div class="outline-text-6" id="text-1-2-1-2-1">
<p>
根据 graph 中的 attrs.device_index 确定 expr 使用的 device, 将来会在 device
上分配 DLTensor
</p>

<p>
其中 attris.device_index 是由 GraphExecutorCodegen 生成的 (根据 on_device
annotation）
</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">for</span> (<span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">pit</span> : pool_entry) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">This for loop is very fast since there are usually only a couple of</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">devices available on the same hardware.</span>
    <span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">cit</span> =
        <span class="org-constant">std</span>::find_if(devices_.begin(), devices_.end(), [&amp;<span class="org-variable-name">pit</span>](<span class="org-keyword">const</span> <span class="org-type">Device</span>&amp; <span class="org-variable-name">d</span>) {
            <span class="org-keyword">return</span> pit.device_type == <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int</span>&gt;(d.device_type);
        });
    <span class="org-type">Device</span> <span class="org-variable-name">dev</span> = cit == devices_.end() ? devices_[0] : *cit;

    <span class="org-constant">std</span>::<span class="org-type">vector</span>&lt;<span class="org-type">int64_t</span>&gt; <span class="org-variable-name">shape</span>;
    shape.push_back(<span class="org-keyword">static_cast</span>&lt;<span class="org-type">int64_t</span>&gt;(pit.size + 3) / 4);
    storage_pool_.push_back(
        <span class="org-constant">NDArray</span>::Empty(shape, DLDataType{kDLFloat, 32, 1}, dev));
}

<span class="org-type">NDArray</span> <span class="org-constant">NDArray</span>::<span class="org-function-name">Empty</span>(
    <span class="org-type">ShapeTuple</span> <span class="org-variable-name">shape</span>, <span class="org-type">DLDataType</span> <span class="org-variable-name">dtype</span>, <span class="org-type">Device</span> <span class="org-variable-name">dev</span>,
    <span class="org-type">Optional</span>&lt;String&gt; <span class="org-variable-name">mem_scope</span>) {
    <span class="org-type">NDArray</span> <span class="org-variable-name">ret</span> = <span class="org-constant">Internal</span>::Create(shape, dtype, dev);
    ret.get_mutable()-&gt;dl_tensor.data =
        <span class="org-constant">DeviceAPI</span>::Get(ret-&gt;device)
            -&gt;AllocDataSpace(
                ret-&gt;device, shape.size(), shape.data(), ret-&gt;dtype, mem_scope);
    <span class="org-keyword">return</span> ret;
}

<span class="org-comment-delimiter">// </span><span class="org-comment">opencl example</span>
<span class="org-type">void</span>* <span class="org-constant">OpenCLWorkspace</span>::<span class="org-function-name">AllocDataSpace</span>(
    <span class="org-type">Device</span> <span class="org-variable-name">dev</span>, <span class="org-type">size_t</span> <span class="org-variable-name">size</span>, <span class="org-type">size_t</span> <span class="org-variable-name">alignment</span>, <span class="org-type">DLDataType</span> <span class="org-variable-name">type_hint</span>) {
    <span class="org-constant">cl</span>::<span class="org-type">BufferDescriptor</span>* <span class="org-variable-name">desc</span> = <span class="org-keyword">new</span> <span class="org-constant">cl</span>::<span class="org-type">BufferDescriptor</span>;
    desc-&gt;buffer = clCreateBuffer(
        <span class="org-keyword">this</span>-&gt;context, CL_MEM_READ_WRITE, size, <span class="org-constant">nullptr</span>, &amp;err_code);
    desc-&gt;layout = <span class="org-constant">cl</span>::<span class="org-constant">BufferDescriptor</span>::<span class="org-constant">MemoryLayout</span>::kBuffer1D;
    <span class="org-keyword">return</span> desc;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org000000d" class="outline-6">
<h6 id="org000000d"><span class="section-number-6">1.2.1.2.2.</span> storage_id</h6>
<div class="outline-text-6" id="text-1-2-1-2-2">
<p>
根据 graph 中的 attrs.storage_id 给每一个输出设置一个 DLTensor, 保存在
data_entry_ 中
</p>

<p>
attris.storage_id 也是在 GraphExecutorCodegen 生成的
</p>

<div class="org-src-container">
<pre class="src src-c++">data_entry_.resize(num_node_entries());
data_alignment_.resize(num_node_entries());
<span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; data_entry_.size(); ++i) {
    <span class="org-type">int</span> <span class="org-variable-name">storage_id</span> = attrs_.storage_id[i];
    ICHECK_LT(<span class="org-keyword">static_cast</span>&lt;<span class="org-type">size_t</span>&gt;(storage_id), storage_pool_.size());
    data_entry_[i] = storage_pool_[storage_id].CreateView(attrs_.shape[i], vtype[i]);

    <span class="org-keyword">const</span> <span class="org-type">DLTensor</span>* <span class="org-variable-name">tmp</span> = data_entry_[i].<span class="org-keyword">operator</span>-&gt;();
}
</pre>
</div>
</div>

<ol class="org-ol">
<li><a id="org000000a"></a>storage_id example<br />
<div class="outline-text-7" id="text-1-2-1-2-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np
<span class="org-keyword">import</span> tvm
<span class="org-keyword">from</span> tvm <span class="org-keyword">import</span> relay

<span class="org-variable-name">a</span> <span class="org-operator">=</span> relay.var(<span class="org-string">"a"</span>, shape<span class="org-operator">=</span>(1, 10), dtype<span class="org-operator">=</span><span class="org-string">"float32"</span>)
<span class="org-variable-name">b</span> <span class="org-operator">=</span> relay.var(<span class="org-string">"b"</span>, shape<span class="org-operator">=</span>(1, 10), dtype<span class="org-operator">=</span><span class="org-string">"float32"</span>)
<span class="org-variable-name">out1</span> <span class="org-operator">=</span> relay.add(a, b)
<span class="org-variable-name">out2</span> <span class="org-operator">=</span> relay.add(out1, a)
<span class="org-variable-name">out</span> <span class="org-operator">=</span> relay.add(out2, b)

<span class="org-variable-name">func</span> <span class="org-operator">=</span> relay.Function([a, b], out)
<span class="org-variable-name">mod</span> <span class="org-operator">=</span> tvm.IRModule.from_expr(func)

<span class="org-keyword">with</span> tvm.transform.PassContext(opt_level<span class="org-operator">=</span>0):
    <span class="org-variable-name">graph</span>, <span class="org-variable-name">lib</span>, <span class="org-variable-name">params</span> <span class="org-operator">=</span> relay.build(mod, target<span class="org-operator">=</span><span class="org-string">"c"</span>, params<span class="org-operator">=</span><span class="org-constant">None</span>)
<span class="org-builtin">print</span>(graph)

</pre>
</div>

<p>
{
  "nodes": [
    {
      "op": "null", 
      "name": "a", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "b", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          2, 
          0, 
          0
        ], 
        [
          0, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add2", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "8ec3c08331cd92b5"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 2]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10], 
        [1, 10]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}
</p>

<p>
storage_id 为 [0, 1, 2, 3, 2]，因为 output (即 [5,0,0]) 与 out1 (即 [2,0,0]) 使用会相同的 DLTensor
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org0000013" class="outline-5">
<h5 id="org0000013"><span class="section-number-5">1.2.1.3.</span> GraphExecutorCodegen</h5>
<div class="outline-text-5" id="text-1-2-1-3">
<p>
SetupStorage 时需的 graph 信息，包括 device_index 和 storage_id, 都来自
GraphExecutorCodegen, 具体的, 来自 StaticMemoryPlan
</p>

<p>
例如:
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-09-08 18:19</span>
<span class="org-keyword">import</span> tvm
<span class="org-keyword">from</span> tvm <span class="org-keyword">import</span> relay
<span class="org-keyword">from</span> tvm.contrib <span class="org-keyword">import</span> graph_executor
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np


<span class="org-keyword">def</span> <span class="org-function-name">test_on_device</span>():
    <span class="org-variable-name">x</span> <span class="org-operator">=</span> relay.var(<span class="org-string">"x"</span>, shape<span class="org-operator">=</span>(1, 5))
    <span class="org-variable-name">y</span> <span class="org-operator">=</span> relay.var(<span class="org-string">"y"</span>, shape<span class="org-operator">=</span>(1, 5))
    <span class="org-variable-name">x_data</span> <span class="org-operator">=</span> np.random.rand(1, 5).astype(<span class="org-string">"float32"</span>)
    <span class="org-variable-name">y_data</span> <span class="org-operator">=</span> np.random.rand(1, 5).astype(<span class="org-string">"float32"</span>)

    <span class="org-variable-name">cpu_dev</span> <span class="org-operator">=</span> tvm.device(<span class="org-string">"cpu"</span>)
    <span class="org-variable-name">opencl_dev</span> <span class="org-operator">=</span> tvm.device(<span class="org-string">"opencl"</span>)

    <span class="org-keyword">def</span> <span class="org-function-name">get_function</span>():
        <span class="org-variable-name">add</span> <span class="org-operator">=</span> relay.add(x, y)
        <span class="org-variable-name">_add</span> <span class="org-operator">=</span> relay.annotation.on_device(add, opencl_dev)
        <span class="org-variable-name">log</span> <span class="org-operator">=</span> relay.log(_add)
        <span class="org-variable-name">func</span> <span class="org-operator">=</span> relay.Function([x, y], log)
        <span class="org-keyword">return</span> func

    <span class="org-variable-name">func</span> <span class="org-operator">=</span> get_function()
    <span class="org-builtin">print</span>(func)

    <span class="org-keyword">with</span> tvm.transform.PassContext(opt_level<span class="org-operator">=</span>1):
        <span class="org-variable-name">lib</span> <span class="org-operator">=</span> relay.build(
            func, {<span class="org-string">"cpu"</span>: <span class="org-string">"llvm"</span>, <span class="org-string">"opencl"</span>: <span class="org-string">"opencl"</span>}, params<span class="org-operator">=</span>{<span class="org-string">"x"</span>: x_data, <span class="org-string">"y"</span>: y_data}
        )
        <span class="org-builtin">print</span>(lib.graph_json)
        <span class="org-comment-delimiter"># </span><span class="org-comment">rt_mod = graph_executor.GraphModule(</span>
        <span class="org-comment-delimiter">#     </span><span class="org-comment">lib["default"](tvm.cpu(0), tvm.device("opencl"))</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">)</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">rt_mod.run()</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">tvm_res = rt_mod.get_output(0).numpy()</span>
        <span class="org-comment-delimiter"># </span><span class="org-comment">print(tvm_res)</span>


<span class="org-keyword">if</span> <span class="org-builtin">__name__</span> <span class="org-operator">==</span> <span class="org-string">"__main__"</span>:
    test_on_device()
</pre>
</div>

<p>
fn (%x: Tensor[(1, 5), float32], %y: Tensor[(1, 5), float32]) {
  %0 = add(%x, %y);
  %1 = on_device(%0, device_type=4);
  log(%1)
}
{
  "nodes": [
    {
      "op": "null", 
      "name": "p0", 
      "inputs": []
    }, 
    {
      "op": "null", 
      "name": "p1", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "2", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "77c3516efdd817bd"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ], 
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "__copy", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "__copy", 
        "hash": "b78177ac726e601e"
      }, 
      "inputs": [
        [
          2, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_log", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_log", 
        "hash": "8a435bddfed54dab"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0, 1], 
  "heads": [
    [
      4, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "device_index": [
      "list_int", 
      [4, 4, 4, 1, 1]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 3, 4]
    ], 
    "shape": [
      "list_shape", 
      [
        [1, 5], 
        [1, 5], 
        [1, 5], 
        [1, 5], 
        [1, 5]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5]
}
</p>

<p>
device_index 为 [4,4,4,1,1], 意味着 node[2] (即 add) 的输出会被分配在 device 4
(opencl) 上, 同时它的输入 (node[0],node[1], 即 x,y) 也需要分配在 opencl 上
</p>

<p>
GraphExecutor 的 set_input 会负责最终调用 opencl 的 CopyDataFromTo 把输入复制到
x, y
</p>
</div>
</div>

<div id="outline-container-org0000016" class="outline-5">
<h5 id="org0000016"><span class="section-number-5">1.2.1.4.</span> SetupOpExecs</h5>
<div class="outline-text-5" id="text-1-2-1-4">
<p>
SetupOpExecs 的作用有两个:
</p>

<ol class="org-ol">
<li>找到所有 PackedFunc (通过 module-&gt;GetFunction)</li>
<li>确定每个 PackedFunc 的参数和返回值对应的具体的 DLTensor (使用 SetupStorage 时分配的 data_entry_)</li>
</ol>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-type">void</span> <span class="org-constant">GraphExecutor</span>::<span class="org-function-name">SetupOpExecs</span>() {
    <span class="org-constant">std</span>::<span class="org-type">unordered_set</span>&lt;<span class="org-type">uint32_t</span>&gt; <span class="org-variable-name">input_node_eids</span>;
    <span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; input_nodes_.size(); i++) {
        <span class="org-type">uint32_t</span> <span class="org-variable-name">nid</span> = input_nodes_[i];
        input_node_eids.insert(entry_id(nid, 0));
    }

    <span class="org-keyword">for</span> (<span class="org-type">uint32_t</span> <span class="org-variable-name">nid</span> = 0; nid &lt; <span class="org-keyword">this</span>-&gt;GetNumOfNodes(); ++nid) {
        <span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">inode</span> = nodes_[nid];
        <span class="org-constant">std</span>::<span class="org-type">vector</span>&lt;DLTensor&gt; <span class="org-variable-name">args</span>;
        <span class="org-comment-delimiter">// </span><span class="org-comment">&#36755;&#20837;&#21442;&#25968;</span>
        <span class="org-keyword">for</span> (<span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">e</span> : inode.inputs) {
            <span class="org-type">uint32_t</span> <span class="org-variable-name">eid</span> = <span class="org-keyword">this</span>-&gt;entry_id(e);
            <span class="org-comment-delimiter">// </span><span class="org-comment">&#20351;&#29992; data_entry_ &#20013;&#20998;&#37197;&#32473; eid &#30340; DLTensor, &#26377;&#21487;&#33021;&#19981;&#21516;&#30340; eid &#26368;&#32456;</span>
            <span class="org-comment-delimiter">// </span><span class="org-comment">&#20250;&#20351;&#29992;&#30456;&#21516;&#30340; DLTensor, &#22240;&#20026;&#23427;&#20204;&#26377;&#30456;&#21516;&#30340; storage_id</span>
            args.push_back(*(data_entry_[eid].<span class="org-keyword">operator</span>-&gt;()));
        }
        <span class="org-comment-delimiter">// </span><span class="org-comment">&#36755;&#20986;</span>
        <span class="org-keyword">for</span> (<span class="org-type">uint32_t</span> <span class="org-variable-name">index</span> = 0; index &lt; inode.param.num_outputs; ++index) {
            <span class="org-type">uint32_t</span> <span class="org-variable-name">eid</span> = <span class="org-keyword">this</span>-&gt;entry_id(nid, index);
            args.push_back(*(data_entry_[eid].<span class="org-keyword">operator</span>-&gt;()));
        }

        <span class="org-comment-delimiter">// </span><span class="org-comment">&#36755;&#20837;&#21644;&#36755;&#20986;&#30340; DLTensor &#37117;&#25171;&#21253;&#22312;&#21516;&#19968;&#20010; args &#20013;&#20570;&#20026; tvm op &#30340;&#21442;&#25968;</span>
        <span class="org-constant">std</span>::tie(op_execs_[nid], op_args) =
            CreateTVMOp(inode.param, args, inode.inputs.size());
    }
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org000001c" class="outline-4">
<h4 id="org000001c"><span class="section-number-4">1.2.2.</span> run</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
执行 GraphExecutor 的代码大约是这样:
</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-type">PackedFunc</span> <span class="org-variable-name">set_input</span> = mod.GetFunction(<span class="org-string">"set_input"</span>, <span class="org-constant">false</span>);
<span class="org-type">PackedFunc</span> <span class="org-variable-name">run</span> = mod.GetFunction(<span class="org-string">"run"</span>, <span class="org-constant">false</span>);
<span class="org-type">PackedFunc</span> <span class="org-variable-name">get_output</span> = mod.GetFunction(<span class="org-string">"get_output"</span>, <span class="org-constant">false</span>);
set_input(<span class="org-string">"A"</span>, a_val);
set_input(<span class="org-string">"B"</span>, b_val);
set_input(<span class="org-string">"C"</span>, c_val);
run();
<span class="org-constant">tvm</span>::<span class="org-constant">runtime</span>::<span class="org-type">NDArray</span> <span class="org-variable-name">out</span> = get_output(0);
</pre>
</div>

<p>
其中 "run" 的实现为:
</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-type">void</span> <span class="org-constant">GraphExecutor</span>::<span class="org-function-name">Run</span>() {
  <span class="org-keyword">for</span> (<span class="org-type">size_t</span> <span class="org-variable-name">i</span> = 0; i &lt; op_execs_.size(); ++i) {
    <span class="org-keyword">if</span> (op_execs_[i]) op_execs_[i]();
  }
}
</pre>
</div>

<p>
而 op_execs_ 即各个 module 中针对 graph 中的 symbol 的具体的实现, 例如
DNNLJSONRuntime 中的 tvmgen_default_dnnl_main_0
</p>

<p>
由于各个 op_execs_ 的输入输出在上一步的 SetupOpExecs 时已经设置好了, 这时只负责执行即可, 不需要再考虑参数和返回值的问题.
</p>

<p>
另外, 由于 op_execs_ 是线性执行的, 所以生成 graph 时需要保证是一个拓扑排序
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway@dogdog.run<br />
Date: 2021-09-24 Fri 00:00<br />
Last updated: 2022-01-24 Mon 19:34</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a>
</div>
</body>
</html>