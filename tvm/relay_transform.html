<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-26 Wed 20:10 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Relay Transform</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">Relay Transform</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org56bde5e">1. Relay Transform</a>
<ul>
<li><a href="#org1f28487">1.1. relay::qnn::transform::Legalize</a></li>
<li><a href="#orge13ed89">1.2. RemoveUnusedFunctions</a></li>
<li><a href="#orgbafc41d">1.3. SimplifyInference</a></li>
<li><a href="#orgec9f9da">1.4. Inline</a></li>
<li><a href="#org1e8d09c">1.5. RunDeviceAnnotationPass</a>
<ul>
<li><a href="#org0df5748">1.5.1. device_copy</a></li>
</ul>
</li>
<li><a href="#org8e39c7d">1.6. FuseOps</a>
<ul>
<li><a href="#org1a43e44">1.6.1. Example</a></li>
<li><a href="#org84f413b">1.6.2. 为什么需要 FuseOps</a></li>
</ul>
</li>
<li><a href="#orga8456bb">1.7. FoldScaleAxis</a>
<ul>
<li><a href="#orgb6bc6ed">1.7.1. Example</a></li>
<li><a href="#orgf0a3209">1.7.2. Impl</a></li>
</ul>
</li>
<li><a href="#org619e6c6">1.8. CombineParallelDense</a>
<ul>
<li><a href="#org86bd5d3">1.8.1. Example</a></li>
</ul>
</li>
<li><a href="#org4e4f48d">1.9. CombineParallelConv2D</a>
<ul>
<li><a href="#org4321a08">1.9.1. Example</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org56bde5e" class="outline-2">
<h2 id="org56bde5e"><span class="section-number-2">1</span> Relay Transform</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="file:///home/sunway/source/tvm/tests/python/relay">file:~/source/tvm/tests/python/relay</a>
</p>
</div>

<div id="outline-container-org1f28487" class="outline-3">
<h3 id="org1f28487"><span class="section-number-3">1.1</span> relay::qnn::transform::Legalize</h3>
<div class="outline-text-3" id="text-1-1">
<p>
这个 transform 用来把 qnn.xxx 转换成 relay IR <a href="tvm_quantization.html#orgded5701">relay.qnn</a>
</p>
</div>
</div>

<div id="outline-container-orge13ed89" class="outline-3">
<h3 id="orge13ed89"><span class="section-number-3">1.2</span> RemoveUnusedFunctions</h3>
<div class="outline-text-3" id="text-1-2">
<p>
从 entry_functions 开始, 递归的标记所有 GlobalVarNode (函数引用), 然后删除没有标记到的函数
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-08-03 11:11</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay

<span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule()

<span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(1, 1000))
<span style="font-weight: bold; font-style: italic;">mod</span>[<span style="font-style: italic;">"fn1"</span>] = relay.Function([x], x)

<span style="font-weight: bold; font-style: italic;">y</span> = relay.var(<span style="font-style: italic;">"y"</span>, shape=(1, 1000))
<span style="font-weight: bold; font-style: italic;">mod</span>[<span style="font-style: italic;">"fn2"</span>] = relay.Function([y], y)

<span style="font-weight: bold; font-style: italic;">z</span> = relay.var(<span style="font-style: italic;">"z"</span>, shape=(1, 1000))
<span style="font-weight: bold; font-style: italic;">mod</span>[<span style="font-style: italic;">"main"</span>] = relay.Function([z], relay.add(z, relay.GlobalVar(<span style="font-style: italic;">"fn1"</span>)(z)))

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------before----------"</span>)
<span style="font-weight: bold;">print</span>(mod.get_global_vars())
<span style="font-weight: bold;">print</span>(mod)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------after----------"</span>)
<span style="font-weight: bold; font-style: italic;">mod</span> = relay.transform.RemoveUnusedFunctions()(mod)
<span style="font-weight: bold;">print</span>(mod)
</pre>
</div>

<p>
-----&#x2013;&#x2014;before-----&#x2013;&#x2014;
[GlobalVar(fn1), GlobalVar(fn2), GlobalVar(main)]
def @fn1(%x: Tensor[(1, 1000), float32]) {
  %x
}
</p>

<p>
def @fn2(%y: Tensor[(1, 1000), float32]) {
  %y
}
</p>

<p>
def @main(%z: Tensor[(1, 1000), float32]) {
  %0 = @fn1(%z);
  add(%z, %0)
}
</p>

<p>
-----&#x2013;&#x2014;after-----&#x2013;&#x2014;
def @fn1(%x: Tensor[(1, 1000), float32]) {
  %x
}
</p>

<p>
def @main(%z: Tensor[(1, 1000), float32]) {
  %0 = @fn1(%z);
  add(%z, %0)
}
</p>
</div>
</div>

<div id="outline-container-orgbafc41d" class="outline-3">
<h3 id="orgbafc41d"><span class="section-number-3">1.3</span> SimplifyInference</h3>
<div class="outline-text-3" id="text-1-3">
<ol class="org-ol">
<li>把 batchnorm 转换为 multiply/add</li>
<li>去掉 dropout</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-08-03 11:11</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay

<span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule()

<span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">alpha</span>, <span style="font-weight: bold; font-style: italic;">gamma</span>, <span style="font-weight: bold; font-style: italic;">mean</span>, <span style="font-weight: bold; font-style: italic;">var</span> = (
    relay.var(<span style="font-style: italic;">"x"</span>, shape=(10,)),
    relay.var(<span style="font-style: italic;">"y"</span>, shape=(10,)),
    relay.var(<span style="font-style: italic;">"a"</span>, shape=(10,)),
    relay.var(<span style="font-style: italic;">"b"</span>, shape=(10,)),
)

<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fmt:off</span>
bn,_,_, = relay.nn.batch_norm(x, alpha, gamma, mean, var)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fmt:on</span>

<span style="font-weight: bold; font-style: italic;">mod</span>[<span style="font-style: italic;">"main"</span>] = relay.Function(
    [x, alpha, gamma, mean, var],
    relay.nn.dropout(bn),
)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------before----------"</span>)
<span style="font-weight: bold;">print</span>(mod)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------after----------"</span>)
<span style="font-weight: bold; font-style: italic;">mod</span> = relay.transform.InferType()(mod)
<span style="font-weight: bold; font-style: italic;">mod</span> = relay.transform.SimplifyInference()(mod)
<span style="font-weight: bold;">print</span>(mod)
</pre>
</div>

<p>
-----&#x2013;&#x2014;before-----&#x2013;&#x2014;
def @main(%x: Tensor[(1, 10), float32], %x1: Tensor[(10), float32], %y: Tensor[(10), float32], %a: Tensor[(10), float32], %b: Tensor[(10), float32]) {
  %0 = nn.batch_norm(%x, %x1, %y, %a, %b);
  %1 = %0.0;
  %2 = nn.dropout(%1);
  %2.0
}
</p>

<p>
-----&#x2013;&#x2014;after-----&#x2013;&#x2014;
def @main(%x: Tensor[(1, 10), float32], %x1: Tensor[(10), float32], %y: Tensor[(10), float32], %a: Tensor[(10), float32], %b: Tensor[(10), float32]) -&gt; Tensor[(1, 10), float32] {
  %0 = add(%b, 1e-05f <i>* ty=float32 *</i>) <i>* ty=Tensor[(10), float32] *</i>;
  %1 = sqrt(%0) <i>* ty=Tensor[(10), float32] *</i>;
  %2 = divide(1f <i>* ty=float32 *</i>, %1) <i>* ty=Tensor[(10), float32] *</i>;
  %3 = multiply(%2, %x1) <i>* ty=Tensor[(10), float32] *</i>;
  %4 = negative(%a) <i>* ty=Tensor[(10), float32] *</i>;
  %5 = multiply(%4, %3) <i>* ty=Tensor[(10), float32] *</i>;
  %6 = multiply(%x, %3) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %7 = add(%5, %y) <i>* ty=Tensor[(10), float32] *</i>;
  add(%6, %7) <i>* ty=Tensor[(1, 10), float32] *</i>
}
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold;">if</span> (<span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>* <span style="font-weight: bold; font-style: italic;">call</span> = new_n-&gt;tuple.as&lt;CallNode&gt;()) {
    <span style="font-weight: bold;">if</span> (call-&gt;op == batch_norm_op_) {
        <span style="font-weight: bold;">return</span> BatchNormToInferUnpack(
            call-&gt;attrs, call-&gt;args[0], call-&gt;args[1], call-&gt;args[2],
            call-&gt;args[3], call-&gt;args[4], ty_map_.at(call-&gt;args[0]));
    } <span style="font-weight: bold;">else</span> <span style="font-weight: bold;">if</span> (call-&gt;op == dropout_op_) {
        <span style="font-weight: bold;">return</span> call-&gt;args[0];
    }
}

<span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold;">BatchNormToInferUnpack</span>(
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Attrs</span> <span style="font-weight: bold; font-style: italic;">attrs</span>, <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">data</span>, <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">gamma</span>, <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">beta</span>, <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">moving_mean</span>,
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">moving_var</span>, <span style="font-weight: bold; text-decoration: underline;">Type</span> <span style="font-weight: bold; font-style: italic;">tdata</span>) {
    <span style="font-weight: bold;">if</span> (param-&gt;scale) {
        scale = Multiply(scale, gamma);
    }
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">neg_mean</span> = Negative(moving_mean);
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">shift</span> = Multiply(neg_mean, scale);
    <span style="font-weight: bold;">if</span> (param-&gt;center) {
        shift = Add(shift, beta);
    }

    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">out</span> = Multiply(data, scale);
    out = Add(out, shift);
    <span style="font-weight: bold;">return</span> out;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-orgec9f9da" class="outline-3">
<h3 id="orgec9f9da"><span class="section-number-3">1.4</span> Inline</h3>
<div class="outline-text-3" id="text-1-4">
<p>
函数可以被 inline 的条件:
</p>

<ol class="org-ol">
<li>有 `Inline` 属性</li>
<li>不是递归函数</li>
<li>调用的其它函数也是可以 inline 的</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-08-03 11:11</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay

<span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule()

<span style="font-weight: bold; font-style: italic;">x1</span> = relay.var(<span style="font-style: italic;">"x1"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">fn1</span> = relay.Function([x1], x1)
<span style="font-weight: bold; font-style: italic;">fn1</span> = fn1.with_attr(<span style="font-style: italic;">"Inline"</span>, tvm.tir.IntImm(<span style="font-style: italic;">"int32"</span>, 1))
<span style="font-weight: bold; font-style: italic;">g1</span> = relay.GlobalVar(<span style="font-style: italic;">"g1"</span>)
<span style="font-weight: bold; font-style: italic;">mod</span>[g1] = fn1

<span style="font-weight: bold; font-style: italic;">x2</span> = relay.var(<span style="font-style: italic;">"x2"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">fn2</span> = relay.Function([x2], x2)
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">fn2 = fn1.with_attr("Inline", tvm.tir.IntImm("int32", 1))</span>
<span style="font-weight: bold; font-style: italic;">g2</span> = relay.GlobalVar(<span style="font-style: italic;">"g2"</span>)
<span style="font-weight: bold; font-style: italic;">mod</span>[g2] = fn2

<span style="font-weight: bold; font-style: italic;">p0</span> = relay.var(<span style="font-style: italic;">"p0"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">mod</span>[<span style="font-style: italic;">"main"</span>] = relay.Function([p0], relay.add(g1(p0), g2(p0)))

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------before----------"</span>)
<span style="font-weight: bold;">print</span>(mod)
<span style="font-weight: bold; font-style: italic;">mod</span> = relay.transform.Inline()(mod)
<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------after----------"</span>)
<span style="font-weight: bold;">print</span>(mod)

</pre>
</div>

<p>
-----&#x2013;&#x2014;before-----&#x2013;&#x2014;
def @g1(%x1: Tensor[(1, 10), float32], Inline=1) {
  %x1
}
</p>

<p>
def @g2(%x2: Tensor[(1, 10), float32]) {
  %x2
}
</p>

<p>
def @main(%p0: Tensor[(1, 10), float32]) {
  %0 = @g1(%p0);
  %1 = @g2(%p0);
  add(%0, %1)
}
</p>

<p>
-----&#x2013;&#x2014;after-----&#x2013;&#x2014;
def @main(%p0: Tensor[(1, 10), float32]) {
  %0 = @g2(%p0);
  add(%p0, %0)
}
</p>

<p>
def @g2(%x2: Tensor[(1, 10), float32]) {
  %x2
}
</p>
</div>
</div>

<div id="outline-container-org1e8d09c" class="outline-3">
<h3 id="org1e8d09c"><span class="section-number-3">1.5</span> RunDeviceAnnotationPass</h3>
<div class="outline-text-3" id="text-1-5">
<p>
RunDeviceAnnotationPass 是为了处理 on_device annotation, vta build 时的
<a href="tvm_vta.html#org1f6c971">graph_pack</a> 依赖 build 时的 RunDeviceAnnotationPass 才能工作
</p>

<p>
NOTE: 新的代码里相关功能放在了 PlanDevices 里了
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-08-03 11:11</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay

<span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">y</span> = relay.var(<span style="font-style: italic;">"y"</span>, shape=(1, 10))
<span style="font-weight: bold; font-style: italic;">add</span> = relay.add(x, y)
<span style="font-weight: bold; font-style: italic;">sqrt</span> = relay.sqrt(add)
<span style="font-weight: bold; font-style: italic;">_sqrt</span> = relay.annotation.on_device(sqrt, <span style="font-style: italic;">"cuda"</span>)
<span style="font-weight: bold; font-style: italic;">log</span> = relay.log(add)
<span style="font-weight: bold; font-style: italic;">subtract</span> = relay.subtract(_sqrt, log)
<span style="font-weight: bold; font-style: italic;">exp</span> = relay.exp(subtract)
<span style="font-weight: bold; font-style: italic;">_exp</span> = relay.annotation.on_device(exp, <span style="font-style: italic;">"cuda"</span>)

<span style="font-weight: bold; font-style: italic;">func</span> = relay.Function([x, y], _exp)
<span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule.from_expr(func)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------before----------"</span>)
<span style="font-weight: bold;">print</span>(mod)

<span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"----------after----------"</span>)
<span style="font-weight: bold; font-style: italic;">mod</span> = relay.transform.RewriteAnnotatedOps(1)(mod)
<span style="font-weight: bold;">print</span>(mod[<span style="font-style: italic;">"main"</span>])
</pre>
</div>

<p>
-----&#x2013;&#x2014;before-----&#x2013;&#x2014;
def @main(%x: Tensor[(1, 10), float32], %y: Tensor[(1, 10), float32]) {
  %0 = add(%x, %y);
  %1 = sqrt(%0);
  %2 = on_device(%1, meta[relay.attrs.OnDeviceAttrs][0]);
  %3 = log(%0);
  %4 = subtract(%2, %3);
  %5 = exp(%4);
  on_device(%5, meta[relay.attrs.OnDeviceAttrs][1])
}
</p>


<p>
-----&#x2013;&#x2014;after-----&#x2013;&#x2014;
fn (%x: Tensor[(1, 10), float32], %y: Tensor[(1, 10), float32]) -&gt; Tensor[(1, 10), float32] {
  %0 = add(%x, %y) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %1 = device_copy(%0, meta[relay.attrs.DeviceCopyAttrs][0]) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %2 = sqrt(%1) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %3 = device_copy(%2, meta[relay.attrs.DeviceCopyAttrs][1]) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %4 = log(%0) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %5 = subtract(%3, %4) <i>* ty=Tensor[(1, 10), float32] *</i>;
  %6 = device_copy(%5, meta[relay.attrs.DeviceCopyAttrs][2]) <i>* ty=Tensor[(1, 10), float32] *</i>;
  exp(%6) <i>* ty=Tensor[(1, 10), float32] *</i>
}
</p>
</div>

<div id="outline-container-org0df5748" class="outline-4">
<h4 id="org0df5748"><span class="section-number-4">1.5.1</span> device_copy</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
device_copy 是由 在 LowerTE 时处理的:
</p>

<ol class="org-ol">
<li>一方面它被用来确定 expr 所在的 target, 以确定 topi strategy</li>
<li>另一方面运行时 graph_executor 会根据这个标记做真正的数据拷贝.</li>
</ol>

<p>

</p>

<p>
例如, graph_executor 在执行时碰到 `__copy` 时会调用设备相关的 CopyDataFromTo 等
函数, 对于 opencl 来说就是 clEnqueueCopyBuffer:
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="font-weight: bold; text-decoration: underline;">void</span> <span style="font-weight: bold; text-decoration: underline;">OpenCLWorkspace</span>::<span style="font-weight: bold;">CopyDataFromTo</span>(
    <span style="font-weight: bold; text-decoration: underline;">DLTensor</span>* <span style="font-weight: bold; font-style: italic;">from</span>, <span style="font-weight: bold; text-decoration: underline;">DLTensor</span>* <span style="font-weight: bold; font-style: italic;">to</span>, <span style="font-weight: bold; text-decoration: underline;">TVMStreamHandle</span> <span style="font-weight: bold; font-style: italic;">stream</span>) {
    <span style="font-weight: bold;">if</span> (IsOpenCLDevice(from-&gt;device) &amp;&amp; IsOpenCLDevice(to-&gt;device)) {
        <span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>* <span style="font-weight: bold; font-style: italic;">from_desc</span> =
            <span style="font-weight: bold;">static_cast</span>&lt;<span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">cl</span>::<span style="font-weight: bold; text-decoration: underline;">BufferDescriptor</span>*&gt;(from-&gt;data);
        <span style="font-weight: bold;">auto</span>* <span style="font-weight: bold; font-style: italic;">to_desc</span> = <span style="font-weight: bold;">static_cast</span>&lt;<span style="font-weight: bold; text-decoration: underline;">cl</span>::<span style="font-weight: bold; text-decoration: underline;">BufferDescriptor</span>*&gt;(to-&gt;data);
        clEnqueueCopyBuffer(
            <span style="font-weight: bold;">this</span>-&gt;GetQueue(to-&gt;device), from_desc-&gt;buffer, to_desc-&gt;buffer,
            from-&gt;byte_offset, to-&gt;byte_offset, nbytes, 0, <span style="font-weight: bold; text-decoration: underline;">nullptr</span>, <span style="font-weight: bold; text-decoration: underline;">nullptr</span>);
    }
    <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">...</span>
}
</pre>
</div>

<p>
除了上面 __copy, 使用 opencl 等 target 在 set_input/get_output 时也会通过
CopyDataFromTo 与设备交换数据.
</p>
</div>
</div>
</div>

<div id="outline-container-org8e39c7d" class="outline-3">
<h3 id="org8e39c7d"><span class="section-number-3">1.6</span> FuseOps</h3>
<div class="outline-text-3" id="text-1-6">
</div>
<div id="outline-container-org1a43e44" class="outline-4">
<h4 id="org1a43e44"><span class="section-number-4">1.6.1</span> Example</h4>
<div class="outline-text-4" id="text-1-6-1">
</div>
<div id="outline-container-orgec052a6" class="outline-5">
<h5 id="orgec052a6"><span class="section-number-5">1.6.1.1</span> 不使用 FuseOps:</h5>
<div class="outline-text-5" id="text-1-6-1-1">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-09-08 18:19</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform
<span style="font-weight: bold;">from</span> tvm.relay.testing <span style="font-weight: bold;">import</span> run_opt_pass
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_fuse_simple</span>():
    <span style="font-style: italic;">"""Simple testcase."""</span>

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">before</span>():
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(10, 20))
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(x, x)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">z</span> = relay.exp(y)
        <span style="font-weight: bold;">return</span> relay.Function([x], z)

    <span style="font-weight: bold; font-style: italic;">z</span> = before()
    <span style="font-weight: bold;">print</span>(z)
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">z = run_opt_pass(z, transform.FuseOps())</span>
    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">print(z)</span>
    <span style="font-weight: bold;">with</span> tvm.transform.PassContext(opt_level=0):
        <span style="font-weight: bold; font-style: italic;">graph</span>, <span style="font-weight: bold; font-style: italic;">lib</span>, <span style="font-weight: bold; font-style: italic;">params</span> = relay.build(z, target=<span style="font-style: italic;">"llvm"</span>, params=<span style="font-weight: bold; text-decoration: underline;">None</span>)

    <span style="font-weight: bold;">print</span>(graph)


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_fuse_simple()

</pre>
</div>

<p>
fn (%x: Tensor[(10, 20), float32]) {
  %0 = add(%x, %x);
  %1 = add(%0, %0);
  %2 = add(%1, %1);
  %3 = add(%2, %2);
  %4 = add(%3, %3);
  exp(%4)
}
{
  "nodes": [
    {
      "op": "null", 
      "name": "x", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "aadf70b47b6beaf4"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add1", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "aadf70b47b6beaf4"
      }, 
      "inputs": [
        [
          1, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add2", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "aadf70b47b6beaf4"
      }, 
      "inputs": [
        [
          2, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add3", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "aadf70b47b6beaf4"
      }, 
      "inputs": [
        [
          3, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add4", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add", 
        "hash": "aadf70b47b6beaf4"
      }, 
      "inputs": [
        [
          4, 
          0, 
          0
        ]
      ]
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_exp", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_exp", 
        "hash": "de3b50c71256954a"
      }, 
      "inputs": [
        [
          5, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0], 
  "heads": [
    [
      6, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32", 
        "float32"
      ]
    ], 
    "device_index": [
      "list_int", 
      [1, 1, 1, 1, 1, 1, 1]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1, 2, 1, 2, 1, 2]
    ], 
    "shape": [
      "list_shape", 
      [
        [10, 20], 
        [10, 20], 
        [10, 20], 
        [10, 20], 
        [10, 20], 
        [10, 20], 
        [10, 20]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2, 3, 4, 5, 6, 7]
}
</p>

<p>
可以看过每个 add 操作都会对应一个 graph 中的 node
</p>
</div>
</div>

<div id="outline-container-org9a72f42" class="outline-5">
<h5 id="org9a72f42"><span class="section-number-5">1.6.1.2</span> 使用 FuseOps</h5>
<div class="outline-text-5" id="text-1-6-1-2">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-09-08 18:19</span>
<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform
<span style="font-weight: bold;">from</span> tvm.relay.testing <span style="font-weight: bold;">import</span> run_opt_pass

<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_fuse_simple</span>():
    <span style="font-style: italic;">"""Simple testcase."""</span>

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">before</span>():
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(10, 20))
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(x, x)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y, y)
        <span style="font-weight: bold; font-style: italic;">z</span> = relay.exp(y)
        <span style="font-weight: bold;">return</span> relay.Function([x], z)

    <span style="font-weight: bold; font-style: italic;">z</span> = before()
    <span style="font-weight: bold; font-style: italic;">z</span> = run_opt_pass(z, transform.FuseOps())
    <span style="font-weight: bold;">print</span>(z)
    <span style="font-weight: bold;">with</span> tvm.transform.PassContext(opt_level=0):
        <span style="font-weight: bold; font-style: italic;">graph</span>, <span style="font-weight: bold; font-style: italic;">lib</span>, <span style="font-weight: bold; font-style: italic;">params</span> = relay.build(z, target=<span style="font-style: italic;">"llvm"</span>, params=<span style="font-weight: bold; text-decoration: underline;">None</span>)

    <span style="font-weight: bold;">print</span>(graph)


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_fuse_simple()
</pre>
</div>

<p>
fn (%x: Tensor[(10, 20), float32]) -&gt; Tensor[(10, 20), float32] {
  %5 = fn (%p0: Tensor[(10, 20), float32], Primitive=1) -&gt; Tensor[(10, 20), float32] {
    %0 = add(%p0, %p0) <i>* ty=Tensor[(10, 20), float32] *</i>;
    %1 = add(%0, %0) <i>* ty=Tensor[(10, 20), float32] *</i>;
    %2 = add(%1, %1) <i>* ty=Tensor[(10, 20), float32] *</i>;
    %3 = add(%2, %2) <i>* ty=Tensor[(10, 20), float32] *</i>;
    %4 = add(%3, %3) <i>* ty=Tensor[(10, 20), float32] *</i>;
    exp(%4) <i>* ty=Tensor[(10, 20), float32] *</i>
  };
  %5(%x) <i>* ty=Tensor[(10, 20), float32] *</i>
}
{
  "nodes": [
    {
      "op": "null", 
      "name": "x", 
      "inputs": []
    }, 
    {
      "op": "tvm_op", 
      "name": "tvmgen_default_fused_add_add_add_add_add_exp", 
      "attrs": {
        "num_outputs": "1", 
        "num_inputs": "1", 
        "flatten_data": "0", 
        "func_name": "tvmgen_default_fused_add_add_add_add_add_exp", 
        "hash": "bcd04c940b541895"
      }, 
      "inputs": [
        [
          0, 
          0, 
          0
        ]
      ]
    }
  ], 
  "arg_nodes": [0], 
  "heads": [
    [
      1, 
      0, 
      0
    ]
  ], 
  "attrs": {
    "dltype": [
      "list_str", 
      [
        "float32", 
        "float32"
      ]
    ], 
    "device_index": [
      "list_int", 
      [1, 1]
    ], 
    "storage_id": [
      "list_int", 
      [0, 1]
    ], 
    "shape": [
      "list_shape", 
      [
        [10, 20], 
        [10, 20]
      ]
    ]
  }, 
  "node_row_ptr": [0, 1, 2]
}
</p>

<p>
所有的 op 都被合并到同一个函数 tvmgen_default_fused_add_add_add_add_add_exp
</p>
</div>
</div>
</div>

<div id="outline-container-org84f413b" class="outline-4">
<h4 id="org84f413b"><span class="section-number-4">1.6.2</span> 为什么需要 FuseOps</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
通过 FuseOps 显然可以减少函数调用的开销. 那么, 每次都把所有的 op 都合并到一个函
数中不就可以了么?
</p>

<p>
每个 op 都有一个 schedule, 如果 schedule 不是 `相容` 的, 则它们是无法合并在一起的, 例如:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform
<span style="font-weight: bold;">from</span> tvm.relay.testing <span style="font-weight: bold;">import</span> run_opt_pass
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_fuse_simple</span>():
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">before</span>():
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(20, 20))
        <span style="font-weight: bold; font-style: italic;">y1</span> = relay.add(x, x)
        <span style="font-weight: bold; font-style: italic;">y2</span> = relay.add(y1, y1)
        <span style="font-weight: bold; font-style: italic;">y3</span> = relay.<span style="font-weight: bold;">sum</span>(y2, axis=-1)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.add(y3, y3)

        <span style="font-weight: bold;">return</span> relay.Function([x], y)

    <span style="font-weight: bold; font-style: italic;">z</span> = before()
    <span style="font-weight: bold; font-style: italic;">z</span> = run_opt_pass(z, transform.FuseOps())
    <span style="font-weight: bold;">print</span>(z)

    <span style="font-weight: bold;">with</span> tvm.transform.PassContext(opt_level=0):
        <span style="font-weight: bold; font-style: italic;">graph</span>, <span style="font-weight: bold; font-style: italic;">lib</span>, <span style="font-weight: bold; font-style: italic;">params</span> = relay.build(z, target=<span style="font-style: italic;">"c"</span>, params=<span style="font-weight: bold; text-decoration: underline;">None</span>)

    <span style="font-weight: bold;">print</span>(lib.get_source())


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_fuse_simple()
</pre>
</div>

<p>
fn (%x: Tensor[(20, 20), float32]) -&gt; Tensor[(20), float32] {
  %2 = fn (%p01: Tensor[(20, 20), float32], Primitive=1) -&gt; Tensor[(20), float32] {
    %0 = add(%p01, %p01) <i>* ty=Tensor[(20, 20), float32] *</i>;
    %1 = add(%0, %0) <i>* ty=Tensor[(20, 20), float32] *</i>;
    sum(%1, axis=[-1]) <i>* ty=Tensor[(20), float32] *</i>
  };
  %3 = %2(%x) <i>* ty=Tensor[(20), float32] *</i>;
  %4 = fn (%p0: Tensor[(20), float32], Primitive=1) -&gt; Tensor[(20), float32] {
    add(%p0, %p0) <i>* ty=Tensor[(20), float32] *</i>
  };
  %4(%3) <i>* ty=Tensor[(20), float32] *</i>
}
// tvm target: c -keys=cpu -link-params=0
#define TVM_EXPORTS
#include "tvm/runtime/c_runtime_api.h"
#include "tvm/runtime/c_backend_api.h"
#include &lt;math.h&gt;
#ifdef __cplusplus
extern "C"
#endif
TVM_DLL int32_t tvmgen_default_fused_add_add_sum(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
  void* arg0 = (((TVMValue*)args)[0].v_handle);
  int32_t arg0_code = ((int32_t*)arg_type_ids)[(0)];
  void* arg1 = (((TVMValue*)args)[1].v_handle);
  int32_t arg1_code = ((int32_t*)arg_type_ids)[(1)];
  void* placeholder = (((DLTensor*)arg0)[0].data);
  void* arg0_shape = (((DLTensor*)arg0)[0].shape);
  void* arg0_strides = (((DLTensor*)arg0)[0].strides);
  int32_t dev_id = (((DLTensor*)arg0)[0].device.device_id);
  void* T_add_red = (((DLTensor*)arg1)[0].data);
  void* arg1_shape = (((DLTensor*)arg1)[0].shape);
  void* arg1_strides = (((DLTensor*)arg1)[0].strides);
  if (!(arg0_strides == NULL)) {
  }
  if (!(arg1_strides == NULL)) {
  }
  for (int32_t ax0 = 0; ax0 &lt; 20; ++ax0) {
    ((float*)T_add_red)[(ax0)] = 0.000000e+00f;
    for (int32_t k1 = 0; k1 &lt; 20; ++k1) {
      ((float*)T_add_red)[(ax0)] = (((float*)T_add_red)[(ax0)] + ((((float*)placeholder)[(((ax0 * 20) + k1))] + ((float*)placeholder)[(((ax0 * 20) + k1))]) + (((float*)placeholder)[(((ax0 * 20) + k1))] + ((float*)placeholder)[(((ax0 * 20) + k1))])));
    }
  }
  return 0;
}
</p>

<p>
#ifdef __cplusplus
extern "C"
#endif
TVM_DLL int32_t tvmgen_default_fused_add(void* args, void* arg_type_ids, int32_t num_args, void* out_ret_value, void* out_ret_tcode, void* resource_handle) {
  void* arg0 = (((TVMValue*)args)[0].v_handle);
  int32_t arg0_code = ((int32_t*)arg_type_ids)[(0)];
  void* arg1 = (((TVMValue*)args)[1].v_handle);
  int32_t arg1_code = ((int32_t*)arg_type_ids)[(1)];
  void* placeholder = (((DLTensor*)arg0)[0].data);
  void* arg0_shape = (((DLTensor*)arg0)[0].shape);
  void* arg0_strides = (((DLTensor*)arg0)[0].strides);
  int32_t dev_id = (((DLTensor*)arg0)[0].device.device_id);
  void* T_add = (((DLTensor*)arg1)[0].data);
  void* arg1_shape = (((DLTensor*)arg1)[0].shape);
  void* arg1_strides = (((DLTensor*)arg1)[0].strides);
  if (!(arg0_strides == NULL)) {
  }
  if (!(arg1_strides == NULL)) {
  }
  for (int32_t ax0_outer = 0; ax0_outer &lt; 2; ++ax0_outer) {
    for (int32_t ax0_inner_s = 0; ax0_inner_s &lt; 16; ++ax0_inner_s) {
      if (((ax0_outer * 16) + ax0_inner_s) &lt; 20) {
        ((float*)T_add)[(((ax0_outer * 16) + ax0_inner_s))] = (((float*)placeholder)[(((ax0_outer * 16) + ax0_inner_s))] + ((float*)placeholder)[(((ax0_outer * 16) + ax0_inner_s))]);
      }
    }
  }
  return 0;
}
</p>

<p>
y1, y2, y3 三个操作使用的 schedule 都是要处理一个 20x20 的循环, 但 y 的 schdule
处理的是 20x1 的循环, 所以它们无法合并成一个 op. 
</p>
</div>
</div>
</div>

<div id="outline-container-orga8456bb" class="outline-3">
<h3 id="orga8456bb"><span class="section-number-3">1.7</span> FoldScaleAxis</h3>
<div class="outline-text-3" id="text-1-7">
<p>
FoldScaleAxis 可以把 Conv2D 和 BatchNorm 融合在一起.
</p>

<p>
算子融合的好处:
</p>

<ol class="org-ol">
<li>减少算子间的中间内存分配</li>
<li>两个算子的 loop 可以做成同一个 loop</li>
</ol>

<p>
FoldScaleAxis 除了上面的好处, 还有一个原因是许多加速器并不支持 scalar
multiplication, 例如 BatchNorm
</p>

<p>
TVM 的 FoldScaleAxis 实际上包含三个 transform:
</p>

<div class="org-src-container">
<pre class="src src-C++"><span style="font-weight: bold; text-decoration: underline;">Pass</span> <span style="font-weight: bold;">FoldScaleAxis</span>() {
    <span style="font-weight: bold; text-decoration: underline;">Pass</span> <span style="font-weight: bold; font-style: italic;">pass</span> = Sequential(
        {BackwardFoldScaleAxis(), ForwardFoldScaleAxis(), FoldConstant()},
        <span style="font-style: italic;">"FoldScaleAxis"</span>);
    <span style="font-weight: bold;">return</span> pass;
}
</pre>
</div>

<p>
其中:
</p>

<ul class="org-ul">
<li>BackwardFoldScaleAxis 是把某个 Op (例如 Conv2D) 后面的 scale 与 Op fold 在一起</li>
<li>ForwardFoldScaleAxis 是把 Op 前面的 scale 与 Op fold</li>
<li>ForwardFoldScaleAxis 或 BackwardFoldScaleAxis 只是负责把 x*scale 或 y*scale 变
成 w*scale, 最终还需要 FoldConstant 把 w*scale fold 成一个 constant (前提是 w,
scale 都是常量)</li>
</ul>
</div>


<div id="outline-container-orgb6bc6ed" class="outline-4">
<h4 id="orgb6bc6ed"><span class="section-number-4">1.7.1</span> Example</h4>
<div class="outline-text-4" id="text-1-7-1">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-10-14 22:36</span>
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np

<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> te
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform

<span style="font-weight: bold; font-style: italic;">I</span> = <span style="font-weight: bold; font-style: italic;">N</span> = 1
<span style="font-weight: bold; font-style: italic;">O</span> = <span style="font-weight: bold; font-style: italic;">C</span> = 2
<span style="font-weight: bold; font-style: italic;">H</span> = <span style="font-weight: bold; font-style: italic;">W</span> = 3


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">run_opt_pass</span>(expr, opt_pass):
    <span style="font-weight: bold;">assert</span> <span style="font-weight: bold;">isinstance</span>(opt_pass, tvm.transform.Pass)
    <span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule.from_expr(expr)
    <span style="font-weight: bold; font-style: italic;">mod</span> = opt_pass(mod)
    <span style="font-weight: bold; font-style: italic;">entry</span> = mod[<span style="font-style: italic;">"main"</span>]
    <span style="font-weight: bold;">return</span> entry <span style="font-weight: bold;">if</span> <span style="font-weight: bold;">isinstance</span>(expr, relay.Function) <span style="font-weight: bold;">else</span> entry.body


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_fold</span>():
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">get_model</span>(x, weight, scale):
        <span style="font-weight: bold; font-style: italic;">args</span> = [x]
        <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">x = relay.multiply(x, scale)</span>
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.nn.conv2d(
            x,
            weight,
            channels=C,
            kernel_size=(3, 3),
            padding=(1, 1),
        )
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.multiply(y, scale)

        <span style="font-weight: bold;">return</span> relay.Function(args, y)

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">check</span>(shape):
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=shape)

        <span style="font-weight: bold; font-style: italic;">weight</span> = relay.const(np.random.randn(O, I, H, W).astype(<span style="font-style: italic;">"float32"</span>))
        <span style="font-weight: bold; font-style: italic;">scale</span> = relay.const(np.random.randn(C, 1, 1).astype(<span style="font-style: italic;">"float32"</span>))

        <span style="font-weight: bold; font-style: italic;">y</span> = get_model(x, weight, scale)
        <span style="font-weight: bold; font-style: italic;">y</span> = run_opt_pass(y, transform.InferType())
        <span style="font-weight: bold;">print</span>(y)

        <span style="font-weight: bold; font-style: italic;">y_folded</span> = run_opt_pass(y, transform.BackwardFoldScaleAxis())
        <span style="font-weight: bold;">print</span>(y_folded)

        <span style="font-weight: bold; font-style: italic;">y_folded</span> = run_opt_pass(y_folded, transform.FoldConstant())
        <span style="font-weight: bold;">print</span>(y_folded)

    check((1, N, 10, 10))


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_fold()
</pre>
</div>

<p>
fn (%x: Tensor[(1, 1, 10, 10), float32]) -&gt; Tensor[(1, 2, 10, 10), float32] {
  %0 = nn.conv2d(%x, meta[relay.Constant][0] <i>* ty=Tensor[(2, 1, 3, 3), float32] *</i>, padding=[1, 1, 1, 1], channels=2, kernel_size=[3, 3]) <i>* ty=Tensor[(1, 2, 10, 10), float32] *</i>;
  multiply(%0, meta[relay.Constant][1] <i>* ty=Tensor[(2, 1, 1), float32] *</i>) <i>* ty=Tensor[(1, 2, 10, 10), float32] *</i>
}
</p>

<p>
fn (%x: Tensor[(1, 1, 10, 10), float32]) -&gt; Tensor[(1, 2, 10, 10), float32] {
  %0 = squeeze(meta[relay.Constant][1] <i>* ty=Tensor[(2, 1, 1), float32] *</i>, axis=[1, 2]) <i>* ty=Tensor[(2), float32] *</i>;
  %1 = expand_dims(%0, axis=1, num_newaxis=3) <i>* ty=Tensor[(2, 1, 1, 1), float32] *</i>;
  %2 = multiply(meta[relay.Constant][0] <i>* ty=Tensor[(2, 1, 3, 3), float32] *</i>, %1) <i>* ty=Tensor[(2, 1, 3, 3), float32] *</i>;
  nn.conv2d(%x, %2, padding=[1, 1, 1, 1], channels=2, kernel_size=[3, 3]) <i>* ty=Tensor[(1, 2, 10, 10), float32] *</i>
}
</p>

<p>
fn (%x: Tensor[(1, 1, 10, 10), float32]) -&gt; Tensor[(1, 2, 10, 10), float32] {
  nn.conv2d(%x, meta[relay.Constant][0] <i>* ty=Tensor[(2, 1, 3, 3), float32] *</i>, padding=[1, 1, 1, 1], channels=2, kernel_size=[3, 3]) <i>* ty=Tensor[(1, 2, 10, 10), float32] *</i>
}
</p>
</div>
</div>

<div id="outline-container-orgf0a3209" class="outline-4">
<h4 id="orgf0a3209"><span class="section-number-4">1.7.2</span> Impl</h4>
<div class="outline-text-4" id="text-1-7-2">
<div class="org-src-container">
<pre class="src src-C++"><span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">multiply &#20250;&#20808;&#20110; conv2d &#34987;&#36941;&#21382;&#21040;, &#35760;&#19979; scale</span>
<span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold;">MultiplyBackwardTransform</span>(
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Call</span>&amp; <span style="font-weight: bold; font-style: italic;">call</span>, <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Message</span>&amp; <span style="font-weight: bold; font-style: italic;">message</span>, <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Expr</span>&amp; <span style="font-weight: bold; font-style: italic;">scale</span>,
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">BackwardTransformer</span>&amp; <span style="font-weight: bold; font-style: italic;">transformer</span>) {
    ICHECK(!message.defined()) &lt;&lt; <span style="font-style: italic;">"outstanding scale"</span>;
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>* <span style="font-weight: bold; font-style: italic;">tlhs</span> = call-&gt;args[0]-&gt;type_as&lt;TensorTypeNode&gt;();
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold;">auto</span>* <span style="font-weight: bold; font-style: italic;">trhs</span> = call-&gt;args[1]-&gt;type_as&lt;TensorTypeNode&gt;();
    <span style="font-weight: bold; text-decoration: underline;">Message</span> <span style="font-weight: bold; font-style: italic;">lhs_message</span> = transformer-&gt;GetMessage(call-&gt;args[0]);
    <span style="font-weight: bold; text-decoration: underline;">Message</span> <span style="font-weight: bold; font-style: italic;">rhs_message</span> = transformer-&gt;GetMessage(call-&gt;args[1]);
    <span style="font-weight: bold;">if</span> (lhs_message.defined()) {
        <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">rhs</span> = call-&gt;args[1];
        <span style="font-weight: bold;">if</span> (MatchBroadcastToLeftAxes(tlhs, trhs, lhs_message-&gt;axes, &amp;rhs) &amp;&amp;
            (!lhs_message-&gt;require_positive || IsAllPositiveConstant(rhs))) {
            <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">&#20363;&#22914;, mul(conv2d, scale), call-&gt;args[0] &#21363; conv2d, rhs &#21363; scale</span>
            <span style="font-weight: bold;">return</span> transformer-&gt;Transform(call-&gt;args[0], lhs_message, rhs);
        }
    } <span style="font-weight: bold;">else</span> <span style="font-weight: bold;">if</span> (rhs_message.defined()) {
        <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">lhs</span> = call-&gt;args[0];
        <span style="font-weight: bold;">if</span> (MatchBroadcastToLeftAxes(trhs, tlhs, rhs_message-&gt;axes, &amp;lhs) &amp;&amp;
            (!rhs_message-&gt;require_positive || IsAllPositiveConstant(lhs))) {
            <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">mul(scale,conv2d), call-&gt;args[1] &#21363; conv2d, lhs &#21363; scale</span>
            <span style="font-weight: bold;">return</span> transformer-&gt;Transform(call-&gt;args[1], rhs_message, lhs);
        }
    }
    <span style="font-weight: bold;">return</span> transformer-&gt;NormalCallTransform(call.<span style="font-weight: bold;">operator</span>-&gt;());
}

<span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold;">Conv2DBackwardTransform</span>(
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Call</span>&amp; <span style="font-weight: bold; font-style: italic;">call</span>, <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Message</span>&amp; <span style="font-weight: bold; font-style: italic;">message</span>, <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">Expr</span>&amp; <span style="font-weight: bold; font-style: italic;">scale</span>,
    <span style="font-weight: bold;">const</span> <span style="font-weight: bold; text-decoration: underline;">BackwardTransformer</span>&amp; <span style="font-weight: bold; font-style: italic;">transformer</span>) {
    <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">...</span>
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">data</span> = transformer-&gt;Transform(
        call-&gt;args[0], NullValue&lt;<span style="font-weight: bold; text-decoration: underline;">Message</span>&gt;(), NullValue&lt;<span style="font-weight: bold; text-decoration: underline;">Expr</span>&gt;());
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">weight</span> = transformer-&gt;Transform(
        call-&gt;args[1], NullValue&lt;<span style="font-weight: bold; text-decoration: underline;">Message</span>&gt;(), NullValue&lt;<span style="font-weight: bold; text-decoration: underline;">Expr</span>&gt;());
    <span style="font-weight: bold; font-style: italic;">// </span><span style="font-weight: bold; font-style: italic;">scale on input for deptwise.</span>
    <span style="font-weight: bold; text-decoration: underline;">Expr</span> <span style="font-weight: bold; font-style: italic;">wscale</span> =
        ExpandBiasToMatchAxis(scale, kernel_layout.ndim(), {big_ko_axis});
    weight = Multiply(weight, wscale);
    <span style="font-weight: bold;">return</span> Call(call-&gt;op, {data, weight}, call-&gt;attrs, call-&gt;type_args);
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org619e6c6" class="outline-3">
<h3 id="org619e6c6"><span class="section-number-3">1.8</span> CombineParallelDense</h3>
<div class="outline-text-3" id="text-1-8">
<p>
FuseOps, FoldScaleAxis 等 transform 属于纵向的融合, 而 CombineXXX 属于横向的融合
</p>
</div>

<div id="outline-container-org86bd5d3" class="outline-4">
<h4 id="org86bd5d3"><span class="section-number-4">1.8.1</span> Example</h4>
<div class="outline-text-4" id="text-1-8-1">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-10-14 22:36</span>
<span style="font-weight: bold;">import</span> numpy <span style="font-weight: bold;">as</span> np

<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> te
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">run_opt_pass</span>(expr, opt_pass):
    <span style="font-weight: bold;">assert</span> <span style="font-weight: bold;">isinstance</span>(opt_pass, tvm.transform.Pass)
    <span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule.from_expr(expr)
    <span style="font-weight: bold; font-style: italic;">mod</span> = tvm.relay.transform.InferType()(mod)
    <span style="font-weight: bold; font-style: italic;">mod</span> = opt_pass(mod)
    <span style="font-weight: bold;">return</span> mod[<span style="font-style: italic;">"main"</span>]


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_combine_parallel_dense</span>():
    <span style="font-style: italic;">"""Simple testcase. One dense cannot be combined due to shape mismatch"""</span>

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">before</span>(x, w1, w2):
        <span style="font-weight: bold; font-style: italic;">args</span> = [x, w1, w2]
        <span style="font-weight: bold; font-style: italic;">y1</span> = relay.nn.dense(x, w1)
        <span style="font-weight: bold; font-style: italic;">y2</span> = relay.nn.dense(x, w2)

        <span style="font-weight: bold; font-style: italic;">y</span> = relay.Tuple((y1, y2))
        <span style="font-weight: bold;">return</span> relay.Function(args, y)

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">check</span>(i, j, k):
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=(i, k))
        <span style="font-weight: bold; font-style: italic;">w1</span> = relay.var(<span style="font-style: italic;">"w1"</span>, shape=(j, k))
        <span style="font-weight: bold; font-style: italic;">w2</span> = relay.var(<span style="font-style: italic;">"w2"</span>, shape=(j, k))

        <span style="font-weight: bold; font-style: italic;">y_before</span> = before(x, w1, w2)
        <span style="font-weight: bold;">print</span>(y_before)
        <span style="font-weight: bold; font-style: italic;">y</span> = run_opt_pass(y_before, transform.CombineParallelDense(min_num_branches=2))
        <span style="font-weight: bold;">print</span>(y)

    <span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">3x4 &#20998;&#21035;&#21644;&#20004;&#20010; 4x5 &#30456;&#20056;, &#24471;&#21040;&#20004;&#20010; 3x5</span>
    check(3, 5, 4)


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_combine_parallel_dense()
</pre>
</div>

<p>
fn (%x: Tensor[(3, 4), float32], %w1: Tensor[(5, 4), float32], %w2: Tensor[(5, 4), float32]) {
  %0 = nn.dense(%x, %w1, units=None);
  %1 = nn.dense(%x, %w2, units=None);
  (%0, %1)
}
fn (%x: Tensor[(3, 4), float32], %w1: Tensor[(5, 4), float32], %w2: Tensor[(5, 4), float32]) -&gt; (Tensor[(3, 5), float32], Tensor[(3, 5), float32]) {
  %0 = (%x, %x);
  %1 = (%w1, %w2);
  %2 = stack(%0) <i>* ty=Tensor[(2, 3, 4), float32] *</i>;
  %3 = stack(%1) <i>* ty=Tensor[(2, 5, 4), float32] *</i>;
  %4 = nn.batch_matmul(%2, %3, transpose_b=True) <i>* ty=Tensor[(2, 3, 5), float32] *</i>;
  %5 = split(%4, indices_or_sections=2) <i>* ty=(Tensor[(1, 3, 5), float32], Tensor[(1, 3, 5), float32]) *</i>;
  %6 = %5.0;
  %7 = %5.1;
  %8 = squeeze(%6, axis=[0]) <i>* ty=Tensor[(3, 5), float32] *</i>;
  %9 = squeeze(%7, axis=[0]) <i>* ty=Tensor[(3, 5), float32] *</i>;
  (%8, %9)
}
</p>
</div>
</div>
</div>

<div id="outline-container-org4e4f48d" class="outline-3">
<h3 id="org4e4f48d"><span class="section-number-3">1.9</span> CombineParallelConv2D</h3>
<div class="outline-text-3" id="text-1-9">
</div>
<div id="outline-container-org4321a08" class="outline-4">
<h4 id="org4321a08"><span class="section-number-4">1.9.1</span> Example</h4>
<div class="outline-text-4" id="text-1-9-1">
<div class="org-src-container">
<pre class="src src-python"><span style="font-weight: bold; font-style: italic;">#</span><span style="font-weight: bold; font-style: italic;">!/usr/bin/env python3</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">-*- coding: utf-8 -*-</span>
<span style="font-weight: bold; font-style: italic;"># </span><span style="font-weight: bold; font-style: italic;">2021-10-22 16:46</span>

<span style="font-weight: bold;">import</span> tvm
<span style="font-weight: bold;">from</span> tvm <span style="font-weight: bold;">import</span> relay
<span style="font-weight: bold;">from</span> tvm.relay <span style="font-weight: bold;">import</span> transform


<span style="font-weight: bold;">def</span> <span style="font-weight: bold;">test_combine_parallel_conv2d</span>():
    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">before</span>(x, w1, w2):
        <span style="font-weight: bold; font-style: italic;">args</span> = [x, w1, w2]
        <span style="font-weight: bold; font-style: italic;">y1</span> = relay.nn.conv2d(x, w1)
        <span style="font-weight: bold; font-style: italic;">y2</span> = relay.nn.conv2d(x, w2)
        <span style="font-weight: bold; font-style: italic;">y3</span> = relay.nn.max_pool2d(x)
        <span style="font-weight: bold; font-style: italic;">y</span> = relay.Tuple((y1, y2, y3))
        <span style="font-weight: bold; font-style: italic;">func</span> = relay.Function(args, y)
        <span style="font-weight: bold; font-style: italic;">mod</span> = tvm.IRModule.from_expr(func)
        <span style="font-weight: bold; font-style: italic;">mod</span> = tvm.relay.transform.InferType()(mod)
        <span style="font-weight: bold;">return</span> mod

    <span style="font-weight: bold;">def</span> <span style="font-weight: bold;">check</span>(x_shape, channels1, channels2):
        <span style="font-weight: bold; font-style: italic;">x</span> = relay.var(<span style="font-style: italic;">"x"</span>, shape=x_shape)
        <span style="font-weight: bold; font-style: italic;">in_c</span> = x_shape[1]
        <span style="font-weight: bold; font-style: italic;">w1</span> = relay.var(<span style="font-style: italic;">"w1"</span>, shape=(channels1, in_c, 1, 1))
        <span style="font-weight: bold; font-style: italic;">w2</span> = relay.var(<span style="font-style: italic;">"w2"</span>, shape=(channels2, in_c, 1, 1))

        <span style="font-weight: bold; font-style: italic;">mod</span> = before(x, w1, w2)
        <span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"------before------"</span>)
        <span style="font-weight: bold;">print</span>(mod)
        <span style="font-weight: bold; font-style: italic;">mod</span> = transform.CombineParallelConv2D(min_num_branches=2)(mod)
        <span style="font-weight: bold;">print</span>(<span style="font-style: italic;">"------after------"</span>)
        <span style="font-weight: bold;">print</span>(mod)

    check((1, 4, 16, 16), 4, 4)


<span style="font-weight: bold;">if</span> <span style="font-weight: bold;">__name__</span> == <span style="font-style: italic;">"__main__"</span>:
    test_combine_parallel_conv2d()
</pre>
</div>

<p>
-&#x2013;&#x2014;before-&#x2013;&#x2014;
def @main(%x: Tensor[(1, 4, 16, 16), float32], %w1: Tensor[(4, 4, 1, 1), float32], %w2: Tensor[(4, 4, 1, 1), float32]) -&gt; (Tensor[(1, 4, 16, 16), float32], Tensor[(1, 4, 16, 16), float32], Tensor[(1, 4, 16, 16), float32]) {
  %0 = nn.conv2d(%x, %w1, padding=[0, 0, 0, 0]) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  %1 = nn.conv2d(%x, %w2, padding=[0, 0, 0, 0]) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  %2 = nn.max_pool2d(%x, pool_size=[1, 1], padding=[0, 0, 0, 0]) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  (%0, %1, %2)
}
</p>

<p>
-&#x2013;&#x2014;after-&#x2013;&#x2014;
def @main(%x: Tensor[(1, 4, 16, 16), float32], %w1: Tensor[(4, 4, 1, 1), float32], %w2: Tensor[(4, 4, 1, 1), float32]) -&gt; (Tensor[(1, 4, 16, 16), float32], Tensor[(1, 4, 16, 16), float32], Tensor[(1, 4, 16, 16), float32]) {
  %0 = (%w1, %w2);
  %1 = concatenate(%0) <i>* ty=Tensor[(8, 4, 1, 1), float32] *</i>;
  %2 = nn.conv2d(%x, %1, padding=[0, 0, 0, 0], channels=8) <i>* ty=Tensor[(1, 8, 16, 16), float32] *</i>;
  %3 = strided_slice(%2, begin=[0, 0], end=[-1, 4], strides=[1, 1], slice_mode="size", axes=None) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  %4 = strided_slice(%2, begin=[0, 4], end=[-1, 4], strides=[1, 1], slice_mode="size", axes=None) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  %5 = nn.max_pool2d(%x, pool_size=[1, 1], padding=[0, 0, 0, 0]) <i>* ty=Tensor[(1, 4, 16, 16), float32] *</i>;
  (%3, %4, %5)
}
</p>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-10-25 Mon 00:00<br />
Last updated: 2022-01-24 Mon 19:28</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
