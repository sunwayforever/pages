<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Quantization</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Quantization</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000016">1. Quantization</a>
<ul>
<li><a href="#org0000001">1.1. Overview</a></li>
<li><a href="#org0000004">1.2. CMSIS Quantization</a></li>
<li><a href="#org0000007">1.3. Tensorflow Post Training Quantization</a></li>
<li><a href="#org000000a">1.4. TFLite Quantization Details</a></li>
<li><a href="#org000000d">1.5. TVM Quantization</a></li>
<li><a href="#org0000010">1.6. Quantization Aware Training</a></li>
<li><a href="#org0000013">1.7. Gemmlowp Quantization</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000016" class="outline-2">
<h2 id="org0000016"><span class="section-number-2">1</span> Quantization</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="https://arxiv.org/pdf/1806.08342.pdf">https://arxiv.org/pdf/1806.08342.pdf</a>
</p>

<p>
<a href="https://arxiv.org/pdf/1510.00149.pdf">https://arxiv.org/pdf/1510.00149.pdf</a>
</p>
</div>

<div id="outline-container-org0000001" class="outline-3">
<h3 id="org0000001"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
quantization 要解决的问题是: 如何用尽可能多的用简单的整数运算代替浮点数计算.
</p>

<p>
例如要计算 0.1*0.2, 可以转换为如下的形式: (1*2)*0.01
</p>

<ul class="org-ul">
<li>把 0.1 (a) 映射为 1 (Q_a) 的过程称为 quantization, 用 Q 表示.</li>

<li>把 2 (Q_z) 乘 0.01 得到 0.02 (z) 的过程称为 dequantization, 用 DQ 表示.</li>
</ul>

<p>
量化后的 ai 推理过程大约如下图所示:
</p>

<pre class="example" id="org0000000">
           layer_{n-1}                           
         +---------------------+                     layer_{n}                     
---Q(o)--+--------------+      |                 +---------------------+         
         |              +--O---+--DQ(O)----Q(o)--+--------------+      |         
         +   w---Q(w)---+      |                 |              +--O---+--DQ(O)--
         +---------------------+                 +   w---Q(w)---+      |         
                                                 +---------------------+         
</pre>

<p>
其中两个 layer 之间的 `DQ&#x2013;Q` 是一个常量, 可以提前转换成近似的整数操作 (例如
cmsis 的 <a href="cmsis_quantization_example.html#ID-7bbad843-6d6f-47b6-a89e-19ace4a617ed">output_shift</a> 和 tflite 的 <a href="tflite_quantization_detail.html#ID-0aa3a8eb-ef6b-42e7-9192-059a100f2af5">real_multiplier</a>)
</p>

<p>
quantization 时会限制 Q_a 的范围, 例如 int8 量化会限制 Q_a 只能用 int8 表示
([-128, 127))
</p>

<p>
quantization 的核心问题是如何把浮点数`线性`映射为某一范围内的整数, 即求解
\(Q=scale*q+bias\), 其中 q 是浮点数, Q 是它对应的量化后的结果, scale 和 bias 是要求解的值, 即量化参数. 这种量化实际类似于信号处理中的均匀量化 (uniform
quantization)
</p>

<p>
最简单的方法是:
</p>

<p>
把 (q_min,Q_min) 和 (q_max, Q_max) 两个代入, 即可解出 scale 和 bias, bias 也叫做
zero_point.
</p>

<p>
但有时我们不希望有 zero_point, 因为它会使量化后的计算比较复杂, 例如计算 a*b:
</p>

<ul class="org-ul">
<li>量化后有 zero_point 时要计算的式子为 \(\frac{(Q_a-Z_a)*(Q_b-Z_b)}{S_a*S_b}\)</li>

<li>没有 zero_point 时要计算的式子为 \(\frac{Q_a*Q_b}{Sa*Sb}\)</li>
</ul>

<p>
因此有些框架 (例如 cmsis, tvm) 会要求 zero_point 必须为 0. zero_point 为 0 的量化称为对称量化, 反之称为非对称量化.
</p>

<p>
采用对称量化时, 我们不再选择 (q_min,Q_min) 和 (q_max, Q_max) 来求解线性映射, 而是选择 (0,0) 和 (abs(q)_max, Q_max), 即要求线性映射过零点, 另一个点则选择绝对值最大的点
</p>

<p>
scale 正常为浮点数, 但有些框架例如 <a href="cmsis_quantization_example.html#ID-d51beea9-ed7e-4308-9659-7832d723f6e3">CMSIS</a> 会要求 scale 的值必须是 \(2^{n}\) 的形式,
这样可以避免浮点数乘法, 同时可以用移位代替整数乘法. tflite 会使用浮点数 scale,
但是可以用 <a href="tflite_quantization_detail.html#ID-4c9c1064-5ff5-4111-95bf-2916a5b24e9e">rounding_doubling_high_mul</a> 转换为整数乘法.
</p>

<p>
还有一些框架例如 <a href="../tvm/tvm_quantization.html#ID-8eef5155-27a8-41c6-9354-5a0c4c8a56c2">TVM</a> 会采用更复杂的量化方式, 因为选择 (q_min, Q_min), (q_max,
Q_max) 来计算线性映射并不是唯一的方法, 理论上最佳的量化方式可以最小化量化误差,
即 minimize_divergency(x, DQ(Q(x)))
</p>

<p>
计算量化参数 scale 和 bias 时需要确定浮点数的范围, 对于模型的参数 (weight,
bias), 范围是确定的, 可以直接计算. 对于 activation (每一层的输入输出), 范围取决于模型的输入, 因此在量化时需要给模型提供一些校准数据以统计实际推理时 activation
的范围, 当校准数据与实际数据有较大偏差时, 量化误差会很大.
</p>

<p>
由于量化会引入误差, 导致推理时性能下降, 因此存在一种技术称为 <a href="../machine_learning/tensorrt.html#ID-beebccdd-76dc-4f17-afa4-074611a912d2">Quantization Aware
Training</a> ,其核心思想是把 forward (x) 修改为 forward (DQ(Q(x))), 即在训练时就能看到量化误差, 其中的 DQ(Q(x)) 称为 fake quantization
</p>

<p>
除此已外, 还存在一些其它的量化方法, 例如:
</p>

<ul class="org-ul">
<li>fp16, 由于 gpu 支持 float16, 所以在 gpu 上运行的模型可以使用 float16 来`量化`</li>

<li><a href="../toolchain/ieee754.html#ID-b6688902-5a1a-4dcd-8347-b5711482f844">bf16</a>, 使用 float16 时, 如果碰到 gpu 不支持的算子, fallback 到 cpu 会涉及
float16 与 float32 的转换开销. bf16 可以减小这种开销, 一方面, 支持 bf16 的 AI
加速器 (例如 google cloud tpu) 可以减小 fallback 时转换的开销, 另一方面, 部署到不支持 bf16 的 cpu 上也可以通过较小的转换开销支持尺寸更小的模型</li>
</ul>
</div>
</div>

<div id="outline-container-org0000004" class="outline-3">
<h3 id="org0000004"><span class="section-number-3">1.2</span> <a href="cmsis_quantization_example.html#ID-d51beea9-ed7e-4308-9659-7832d723f6e3">CMSIS Quantization</a></h3>
</div>

<div id="outline-container-org0000007" class="outline-3">
<h3 id="org0000007"><span class="section-number-3">1.3</span> <a href="tensorflow_post_training_quantization.html#ID-0d9ef45d-9547-4523-a104-c52a3e9c6673">Tensorflow Post Training Quantization</a></h3>
</div>

<div id="outline-container-org000000a" class="outline-3">
<h3 id="org000000a"><span class="section-number-3">1.4</span> <a href="tflite_quantization_detail.html#ID-58be2f99-33f3-402f-bf0c-eb81e02ce5d5">TFLite Quantization Details</a></h3>
</div>

<div id="outline-container-org000000d" class="outline-3">
<h3 id="org000000d"><span class="section-number-3">1.5</span> <a href="../tvm/tvm_quantization.html#ID-8eef5155-27a8-41c6-9354-5a0c4c8a56c2">TVM Quantization</a></h3>
</div>

<div id="outline-container-org0000010" class="outline-3">
<h3 id="org0000010"><span class="section-number-3">1.6</span> <a href="../machine_learning/tensorrt.html#ID-beebccdd-76dc-4f17-afa4-074611a912d2">Quantization Aware Training</a></h3>
</div>

<div id="outline-container-org0000013" class="outline-3">
<h3 id="org0000013"><span class="section-number-3">1.7</span> <a href="gemmlowp_quantization.html#ID-f7ab0931-3653-420b-8a27-de6829c86a08">Gemmlowp Quantization</a></h3>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2020-08-12 Wed 00:00<br />
Last updated: 2022-03-09 Wed 16:19</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
