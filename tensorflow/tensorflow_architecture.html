<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>Tensorflow Architecture</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">Tensorflow Architecture</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000062">1. Tensorflow Architecture</a>
<ul>
<li><a href="#org0000002">1.1. Overview</a></li>
<li><a href="#org0000005">1.2. grappler</a></li>
<li><a href="#org0000008">1.3. distributed runtime</a></li>
<li><a href="#org000002e">1.4. kernels</a>
<ul>
<li><a href="#org000000b">1.4.1. eigen</a></li>
<li><a href="#org0000011">1.4.2. mkl</a></li>
<li><a href="#org000001f">1.4.3. cuda</a></li>
<li><a href="#org0000028">1.4.4. sycl</a></li>
<li><a href="#org000002b">1.4.5. stream executor</a></li>
</ul>
</li>
<li><a href="#org000003a">1.5. xla</a>
<ul>
<li><a href="#org0000031">1.5.1. tf2xla</a></li>
<li><a href="#org0000034">1.5.2. HloModulePass</a></li>
<li><a href="#org0000037">1.5.3. CpuCompiler/GpuCompiler</a></li>
</ul>
</li>
<li><a href="#org000005f">1.6. Parallelism</a>
<ul>
<li><a href="#org000004c">1.6.1. data parallelism</a></li>
<li><a href="#org0000050">1.6.2. inter_op_parallelism_threads</a></li>
<li><a href="#org0000053">1.6.3. intra_op_parallelism_threads</a></li>
<li><a href="#org0000056">1.6.4. OMP_NUM_THREADS</a></li>
<li><a href="#org0000059">1.6.5. 测试</a></li>
<li><a href="#org000005c">1.6.6. 其它</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000062" class="outline-2">
<h2 id="org0000062"><span class="section-number-2">1</span> Tensorflow Architecture</h2>
<div class="outline-text-2" id="text-1">
<pre class="example" id="org0000000">
$&gt; bazel query --notool_deps --noimplicit_deps "deps(//tensorflow:tensorflow)" --output=package
</pre>

<pre class="example" id="org0000001">
@com_github_grpc_grpc//
@com_google_absl//
@com_google_protobuf//
@com_googlesource_code_re2//
@cub_archive//
@double_conversion//
@eigen_archive//
@farmhash_archive//
@fft2d//
@gemmlowp//
@highwayhash//
@hwloc//
@libxsmm_archive//
@llvm-project//llvm
@llvm-project//mlir
@mkl_dnn//
@mkl_dnn_v1//
@mkl_linux//
@mkl_windows//
@platforms//os
@snappy//
@sobol_data//
@upb//
tensorflow/c
tensorflow/c/eager
tensorflow/c/kernels
tensorflow/cc
tensorflow/cc/saved_model
tensorflow/compiler/jit
tensorflow/compiler/jit/graphcycles
tensorflow/compiler/jit/kernels
tensorflow/compiler/jit/ops
tensorflow/compiler/mlir
tensorflow/compiler/mlir/lite
tensorflow/compiler/mlir/tensorflow
tensorflow/compiler/plugin
tensorflow/compiler/tf2xla
tensorflow/compiler/tf2xla/cc
tensorflow/compiler/tf2xla/kernels
tensorflow/compiler/tf2xla/lib
tensorflow/compiler/tf2xla/ops
tensorflow/compiler/xla
tensorflow/compiler/xla/client
tensorflow/compiler/xla/client/lib
tensorflow/compiler/xla/service
tensorflow/compiler/xla/service/cpu
tensorflow/compiler/xla/service/gpu
tensorflow/compiler/xla/service/llvm_ir
tensorflow/core
tensorflow/core/api_def
tensorflow/core/common_runtime/eager
tensorflow/core/debug
tensorflow/core/distributed_runtime
tensorflow/core/distributed_runtime/eager
tensorflow/core/distributed_runtime/rpc
tensorflow/core/distributed_runtime/rpc/eager
tensorflow/core/example
tensorflow/core/framework
tensorflow/core/graph
tensorflow/core/grappler
tensorflow/core/grappler/clusters
tensorflow/core/grappler/costs
tensorflow/core/grappler/inputs
tensorflow/core/grappler/optimizers
tensorflow/core/grappler/optimizers/data
tensorflow/core/grappler/optimizers/data/vectorization
tensorflow/core/grappler/utils
tensorflow/core/grappler/verifiers
tensorflow/core/kernels
tensorflow/core/kernels/batching_util
tensorflow/core/kernels/boosted_trees
tensorflow/core/kernels/boosted_trees/quantiles
tensorflow/core/kernels/data
tensorflow/core/kernels/data/experimental
tensorflow/core/kernels/data/experimental/sql
tensorflow/core/kernels/neon
tensorflow/core/kernels/rnn
tensorflow/core/kernels/sparse
tensorflow/core/kernels/special_math
tensorflow/core/kernels/tensor_forest
tensorflow/core/lib/bfloat16
tensorflow/core/lib/core
tensorflow/core/lib/db
tensorflow/core/lib/gtl
tensorflow/core/lib/hash
tensorflow/core/lib/histogram
tensorflow/core/lib/io
tensorflow/core/lib/math
tensorflow/core/lib/monitoring
tensorflow/core/lib/png
tensorflow/core/lib/random
tensorflow/core/lib/strings
tensorflow/core/protobuf/tpu
tensorflow/core/tpu
tensorflow/stream_executor
tensorflow/stream_executor/cuda
tensorflow/stream_executor/gpu
tensorflow/stream_executor/host
tensorflow/stream_executor/lib
tensorflow/stream_executor/platform
tensorflow/stream_executor/platform/default
tensorflow/stream_executor/rocm
third_party/eigen3
third_party/fft2d
third_party/hwloc
third_party/mkl
third_party/mkl_dnn
third_party/mlir

</pre>
</div>

<div id="outline-container-org0000002" class="outline-3">
<h3 id="org0000002"><span class="section-number-3">1.1</span> Overview</h3>
<div class="outline-text-3" id="text-1-1">
<p>
从代码的的角度, tensorflow 主要包括:
</p>

<ol class="org-ol">
<li>core
<ol class="org-ol">
<li>grappler</li>
<li>distributed runtime</li>
<li>kernels</li>
</ol></li>
<li>stream executor</li>
<li>compiler
<ol class="org-ol">
<li>mlir</li>
<li>xla</li>
</ol></li>
<li>3rd party
<ol class="org-ol">
<li>eigen</li>
<li>mkl</li>
<li>fft2d</li>
<li>absl</li>
</ol></li>
</ol>

<p>
从功能的角度, 大致的层次是:
</p>

<ul class="org-ul">
<li>grappler</li>
<li>distributed runtime</li>
<li>kernels
<ul class="org-ul">
<li>hand-crafted kernel
<ul class="org-ul">
<li>mkl</li>
<li>eigen
<ul class="org-ul">
<li>cpu</li>
<li>cuda</li>
<li>sycl</li>
</ul></li>
<li>cuda</li>
<li>sycl</li>
<li>stream executor
<ul class="org-ul">
<li>host</li>
<li>cublas</li>
<li>cudnn</li>
</ul></li>
</ul></li>
<li>compiler generated kernel
<ul class="org-ul">
<li>mlir</li>
<li>xla</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org0000005" class="outline-3">
<h3 id="org0000005"><span class="section-number-3">1.2</span> grappler</h3>
<div class="outline-text-3" id="text-1-2">
<p>
<a href="http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf">http://web.stanford.edu/class/cs245/slides/TFGraphOptimizationsStanford.pdf</a>
</p>
</div>
</div>

<div id="outline-container-org0000008" class="outline-3">
<h3 id="org0000008"><span class="section-number-3">1.3</span> distributed runtime</h3>
</div>

<div id="outline-container-org000002e" class="outline-3">
<h3 id="org000002e"><span class="section-number-3">1.4</span> kernels</h3>
<div class="outline-text-3" id="text-1-4">
<p>
默认情况下 tensorflow 使用预先手写的 kernel.
</p>
</div>

<div id="outline-container-org000000b" class="outline-4">
<h4 id="org000000b"><span class="section-number-4">1.4.1</span> eigen</h4>
<div class="outline-text-4" id="text-1-4-1">
<p>
默认情况下 tensorflow 使用 eigen, eigen 支持的特性包括:
</p>

<ol class="org-ol">
<li>在 cpu 直接执行</li>
<li>支持 <a href="https://www.openmp.org/resources/tutorials-articles/">openmp</a></li>
<li>用 cuda 执行</li>
<li>使用指定的 blas 或 mkl</li>
<li>使用 sycl</li>
</ol>

<p>
在 tensorflow 里 eigen 会用到以下几个特性:
</p>

<ol class="org-ol">
<li>在 cpu 直接执行 (使用 thread pool, 不使用 openmp)</li>
<li>用 cuda 执行</li>
<li>使用 mkl_dnn 实现其 contract kernel</li>
<li>使用 sycl</li>
</ol>
</div>
</div>

<div id="outline-container-org0000011" class="outline-4">
<h4 id="org0000011"><span class="section-number-4">1.4.2</span> mkl</h4>
<div class="outline-text-4" id="text-1-4-2">
<p>
当配置了 `&#x2013;config=mkl` 时, 会使用 mkl 实现一些算子, 而不使用 eigen.
</p>

<p>
mkl 会使用 openmp 通过 thread 和 simd 来加速.
</p>

<p>
mkl 的支持与 cuda 和 sycl 不太一样: 它会提前用一个 MklToTfConversionPass 把它支持的 op 修改成自己的名字, 例如 _MklSoftmax
</p>
</div>

<div id="outline-container-org000000e" class="outline-5">
<h5 id="org000000e"><span class="section-number-5">1.4.2.1</span> MklSoftmaxOp</h5>
<div class="outline-text-5" id="text-1-4-2-1">
<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#ifdef</span> INTEL_MKL
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">MklSoftmaxOp</span> : <span class="org-keyword">public</span> <span class="org-type">OpKernel</span> {
   <span class="org-keyword">public</span>:
    ~<span class="org-function-name">MklSoftmaxOp</span>() {}

    <span class="org-keyword">explicit</span> <span class="org-function-name">MklSoftmaxOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">context</span>) : OpKernel(context) {}

    <span class="org-type">void</span> <span class="org-function-name">Compute</span>(<span class="org-type">OpKernelContext</span>* <span class="org-variable-name">context</span>) <span class="org-keyword">override</span> <span class="org-keyword">try</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: softmax_fwd &#26159; mkldnn::softmax_forward</span>
        softmax_fwd-&gt;Execute(src_data, dst_data);
    }
};
<span class="org-preprocessor">#endif</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org000001f" class="outline-4">
<h4 id="org000001f"><span class="section-number-4">1.4.3</span> cuda</h4>
<div class="outline-text-4" id="text-1-4-3">
<p>
当配置了 `&#x2013;config=cuda` 时, 会使用 cuda 实现一些算子:
</p>

<ol class="org-ol">
<li>有些是用 cuda 直接实现</li>

<li>有些是通过 eigen 实现, 间接利用 eigen 的 cuda 支持</li>

<li>有的是通过 stream executor (使用 cublas 和 cudnn)</li>
</ol>
</div>

<div id="outline-container-org0000016" class="outline-5">
<h5 id="org0000016"><span class="section-number-5">1.4.3.1</span> SoftmaxOpGPU</h5>
<div class="outline-text-5" id="text-1-4-3-1">
<p>
softmax 的 gpu 实现是直接用 cuda 实现的
</p>

<ol class="org-ol">
<li><p>
首先定义 cuda 这个 config
</p>

<pre class="example" id="org0000014">
build:using_cuda --define=using_cuda=true
build:using_cuda --action_env TF_NEED_CUDA=1
build:using_cuda --crosstool_top=@local_config_cuda//crosstool:toolchain

build:cuda --config=using_cuda
build:cuda --define=using_cuda_nvcc=true
</pre></li>

<li><p>
using_cuda_nvcc 会导致 GOOGLE_CUDA 宏被定义
</p>

<pre class="example" id="org0000015">
def if_cuda(if_true, if_false = []):
    return select({
        "@local_config_cuda//cuda:using_nvcc": if_true,
        "@local_config_cuda//cuda:using_clang": if_true,
        "//conditions:default": if_false,
    })

if_cuda(["-DGOOGLE_CUDA=1"])
</pre></li>

<li><p>
GOOGLE_CUDA 宏导致 softmax_op_gpu.cu.cc 被编译, 相关 kernel 被注册
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#if</span> GOOGLE_CUDA || TENSORFLOW_USE_ROCM
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">SoftmaxOpGPU</span> : <span class="org-keyword">public</span> <span class="org-type">OpKernel</span> {
    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    <span class="org-type">void</span> <span class="org-function-name">Compute</span>(<span class="org-type">OpKernelContext</span>* <span class="org-variable-name">context</span>) <span class="org-keyword">override</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        GpuLaunchKernel(
            GenerateNormalizedProb&lt;T, acc_type&gt;, numBlocks, numThreadsPerBlock,
            0, cu_stream,
            <span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-keyword">const</span> <span class="org-type">T</span>*&gt;(logits_in_.flat&lt;<span class="org-type">T</span>&gt;().data()),
            <span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-keyword">const</span> <span class="org-type">acc_type</span>*&gt;(
                sum_probs.flat&lt;<span class="org-type">acc_type</span>&gt;().data()),
            <span class="org-keyword">reinterpret_cast</span>&lt;<span class="org-keyword">const</span> <span class="org-type">T</span>*&gt;(max_logits.flat&lt;<span class="org-type">T</span>&gt;().data()),
            <span class="org-keyword">const_cast</span>&lt;<span class="org-type">T</span>*&gt;(softmax_out-&gt;flat&lt;<span class="org-type">T</span>&gt;().data()), rows, cols, log_);
    }
};

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-keyword">typename</span> <span class="org-type">U</span>&gt;
__global__ <span class="org-type">void</span> <span class="org-function-name">GenerateNormalizedProb</span>(
    <span class="org-keyword">const</span> <span class="org-type">T</span>* <span class="org-variable-name">logits</span>, <span class="org-keyword">const</span> <span class="org-type">U</span>* <span class="org-variable-name">sum_probs</span>, <span class="org-keyword">const</span> <span class="org-type">T</span>* <span class="org-variable-name">max_logits</span>, <span class="org-type">T</span>* <span class="org-variable-name">output</span>,
    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">num_rows</span>, <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">num_cols</span>, <span class="org-keyword">const</span> <span class="org-type">bool</span> <span class="org-variable-name">in_log_space</span>) {
    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">tid</span> = blockIdx.x * blockDim.x + threadIdx.x;
    <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
}

REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"Softmax"</span>).Device(DEVICE_GPU).TypeConstraint&lt;<span class="org-constant">Eigen</span>::half&gt;(<span class="org-string">"T"</span>),
    <span class="org-type">SoftmaxOpGPU</span>&lt;<span class="org-constant">Eigen</span>::half&gt;);
REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"Softmax"</span>).Device(DEVICE_GPU).TypeConstraint&lt;<span class="org-type">float</span>&gt;(<span class="org-string">"T"</span>),
    <span class="org-type">SoftmaxOpGPU</span>&lt;<span class="org-type">float</span>&gt;);
<span class="org-preprocessor">#endif</span>
</pre>
</div></li>
</ol>
</div>
</div>

<div id="outline-container-org0000019" class="outline-5">
<h5 id="org0000019"><span class="section-number-5">1.4.3.2</span> argmax</h5>
<div class="outline-text-5" id="text-1-4-3-2">
<p>
argmax 的 gpu 实现是通过 eigen 的 cuda 实现的
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#if</span> <span class="org-preprocessor">defined</span>(GOOGLE_CUDA)
<span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: EIGEN_USE_GPU &#20250;</span>
<span class="org-preprocessor">#define</span> <span class="org-variable-name">EIGEN_USE_GPU</span>
REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"ArgMax"</span>)
        .Device(DEVICE_GPU)
        .TypeConstraint&lt;type&gt;(<span class="org-string">"T"</span>)
        .TypeConstraint&lt;int64&gt;(<span class="org-string">"output_type"</span>)
        .TypeConstraint&lt;int32&gt;(<span class="org-string">"Tidx"</span>)
        .HostMemory(<span class="org-string">"dimension"</span>),
    <span class="org-type">ArgMaxOp</span>&lt;GPUDevice, type, int64&gt;);
<span class="org-preprocessor">#endif</span>

<span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: ArgMaxOp &#26159; CPU/GPU &#36890;&#29992;&#30340;, &#19981;&#38656;&#35201;&#21253;&#22312; GOOGLE_CUDA &#23439;&#20013;</span>
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-keyword">typename</span> <span class="org-type">Tout</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">ArgMaxOp</span>
    : <span class="org-keyword">public</span> <span class="org-type">ArgOp</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>, <span class="org-type">Tout</span>, <span class="org-constant">functor</span>::<span class="org-type">ArgMax</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>, <span class="org-type">Tout</span>&gt; &gt; {
   <span class="org-keyword">public</span>:
    <span class="org-keyword">explicit</span> <span class="org-function-name">ArgMaxOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">context</span>)
        : ArgOp&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>, <span class="org-type">Tout</span>, <span class="org-constant">functor</span>::<span class="org-type">ArgMax</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>, <span class="org-type">Tout</span>&gt; &gt;(context) {}
};

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-keyword">typename</span> <span class="org-type">Tout</span>, <span class="org-keyword">typename</span> <span class="org-type">ArgFunctor</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">ArgOp</span> : <span class="org-keyword">public</span> <span class="org-type">OpKernel</span> {
   <span class="org-keyword">public</span>:
    <span class="org-keyword">explicit</span> <span class="org-function-name">ArgOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">context</span>) : OpKernel(context) {}

    <span class="org-type">void</span> <span class="org-function-name">Compute</span>(<span class="org-type">OpKernelContext</span>* <span class="org-variable-name">context</span>) <span class="org-keyword">override</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>

<span class="org-preprocessor">#define</span> <span class="org-function-name">HANDLE_DIM</span>(<span class="org-variable-name">NDIM</span>) \
    <span class="org-keyword">case</span> NDIM:           \
        <span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: &#26368;&#32456;&#20250;&#35843;&#29992; ArgFunctor::Reduce1 &#31561;, &#20363;&#22914; ArgMax::Reduce1 \</span>
<span class="org-comment">        ArgFunctor::Reduce##NDIM(                                           \</span>
<span class="org-comment">            context-&gt;eigen_device&lt;Device&gt;(), input.tensor&lt;T, NDIM&gt;(), axis, \</span>
<span class="org-comment">            output-&gt;tensor&lt;Tout, NDIM - 1&gt;());                              \</span>
<span class="org-comment">        break;</span>

        <span class="org-keyword">switch</span> (input_dims) {
            HANDLE_DIM(1);
            HANDLE_DIM(2);
            HANDLE_DIM(3);
            HANDLE_DIM(4);
            HANDLE_DIM(5);
            HANDLE_DIM(6);
            HANDLE_DIM(7);

            <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        }
    }
<span class="org-preprocessor">#undef</span> HANDLE_DIM
};

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-keyword">typename</span> <span class="org-type">Tout</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">ArgMax</span> {
<span class="org-preprocessor">#define</span> <span class="org-function-name">DECLARE_COMPUTE_SPEC</span>(<span class="org-variable-name">Dims</span>)                                    \
    EIGEN_ALWAYS_INLINE <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-variable-name">Reduce</span>##Dims(                     \
        <span class="org-keyword">const</span> <span class="org-type">Device</span>&amp; <span class="org-variable-name">d</span>, <span class="org-keyword">typename</span> <span class="org-type">TTypes</span>&lt;T, Dims&gt;::ConstTensor input, \
        <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">dimension</span>,                                        \
        <span class="org-keyword">typename</span> <span class="org-type">TTypes</span>&lt;Tout, Dims - 1&gt;::Tensor output) {             \
        output.device(d) = input.argmax(dimension).<span class="org-keyword">template</span> cast&lt;Tout&gt;();
}

<span class="org-variable-name">DECLARE_COMPUTE_SPEC</span>(1);
DECLARE_COMPUTE_SPEC(2);
DECLARE_COMPUTE_SPEC(3);
DECLARE_COMPUTE_SPEC(4);
DECLARE_COMPUTE_SPEC(5);
DECLARE_COMPUTE_SPEC(6);
DECLARE_COMPUTE_SPEC(7);

<span class="org-preprocessor">#undef</span> DECLARE_COMPUTE_SPEC
}
;

<span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: input &#26159; Eigen::TensorMap, &#36825;&#37324;&#20250;&#35843;&#29992; engien &#30340; argmax &#26041;&#27861;,</span>
<span class="org-comment-delimiter">// </span><span class="org-comment">&#26368;&#32456;&#20250;&#35843;&#29992;&#21040; TensorReductionGpu.h &#20013;&#30456;&#24212;&#30340; kernel &#20363;&#22914; FullReductionKernel</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org000001c" class="outline-5">
<h5 id="org000001c"><span class="section-number-5">1.4.3.3</span> matmul</h5>
<div class="outline-text-5" id="text-1-4-3-3">
<p>
matmul 的 gpu 实现是通过 stream executor 实现的
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#if</span> GOOGLE_CUDA
REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"MatMul"</span>).Device(DEVICE_GPU).TypeConstraint&lt;T&gt;(<span class="org-string">"T"</span>),
    <span class="org-type">MatMulOp</span>&lt;GPUDevice, T, <span class="org-constant">true</span> <span class="org-comment-delimiter">/* </span><span class="org-comment">cublas, true by default</span><span class="org-comment-delimiter"> */</span>&gt;);
<span class="org-preprocessor">#endif</span>

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>, <span class="org-type">bool</span> <span class="org-variable-name">USE_CUBLAS</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">MatMulOp</span> : <span class="org-keyword">public</span> <span class="org-type">OpKernel</span> {
   <span class="org-keyword">public</span>:
    <span class="org-keyword">explicit</span> <span class="org-function-name">MatMulOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">ctx</span>)
        : OpKernel(ctx), algorithms_set_already_(<span class="org-constant">false</span>) {
        <span class="org-constant">LaunchMatMul</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>, USE_CUBLAS&gt;::GetBlasGemmAlgorithm(
            ctx, &amp;algorithms_, &amp;algorithms_set_already_);
    }

    <span class="org-type">void</span> <span class="org-function-name">Compute</span>(<span class="org-type">OpKernelContext</span>* <span class="org-variable-name">ctx</span>) <span class="org-keyword">override</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-constant">LaunchMatMul</span>&lt;<span class="org-type">Device</span>, <span class="org-type">float</span>, USE_CUBLAS&gt;::launch(
            ctx, a_float, b_float, dim_pair, &amp;algorithms_, use_autotune_,
            &amp;out_float);
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    }
};

<span class="org-preprocessor">#if</span> GOOGLE_CUD
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">LaunchMatMul</span>&lt;GPUDevice, <span class="org-type">T</span>, <span class="org-constant">true</span> <span class="org-comment-delimiter">/* </span><span class="org-comment">USE_CUBLAS</span><span class="org-comment-delimiter"> */</span>&gt; {
    <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-variable-name">launch</span>() {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-keyword">auto</span>* <span class="org-variable-name">stream</span> = ctx-&gt;op_device_context()-&gt;stream();
        OP_REQUIRES(ctx, stream, <span class="org-constant">errors</span>::Internal(<span class="org-string">"No GPU stream available."</span>));

        <span class="org-keyword">auto</span> <span class="org-variable-name">a_ptr</span> = AsDeviceMemory(
            a.<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().data(), a.<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().size());
        <span class="org-keyword">auto</span> <span class="org-variable-name">b_ptr</span> = AsDeviceMemory(
            b.<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().data(), b.<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().size());
        <span class="org-keyword">auto</span> <span class="org-variable-name">c_ptr</span> = AsDeviceMemory(
            out-&gt;<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().data(), out-&gt;<span class="org-keyword">template</span> flat&lt;<span class="org-type">T</span>&gt;().size());
        <span class="org-keyword">auto</span> <span class="org-variable-name">alpha</span> = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">T</span>&gt;(1.0);
        <span class="org-keyword">auto</span> <span class="org-variable-name">beta</span> = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">T</span>&gt;(0.0);

        <span class="org-type">int</span> <span class="org-variable-name">device_id</span> = stream-&gt;parent()-&gt;device_ordinal();
        <span class="org-type">DataType</span> <span class="org-variable-name">dtype</span> = a.dtype();
        <span class="org-type">MatmulParameters</span> <span class="org-variable-name">matmul_parameters</span> = {
            transpose_a, transpose_b, m, n, k, dtype, device_id,
        };
        <span class="org-type">AlgorithmConfig</span> <span class="org-variable-name">algorithm_config</span>(kNoAlgorithm);

        <span class="org-type">bool</span> <span class="org-variable-name">blas_launch_status</span> =
            stream
                -&gt;ThenBlasGemm(
                    blas_transpose_b, blas_transpose_a, n, m, k, 1.0f, b_ptr,
                    transpose_b ? k : n, a_ptr, transpose_a ? m : k, 0.0f,
                    &amp;c_ptr, n)
                .ok();
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    }
    <span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: ThenBlasGemm &#26159; stream_executor &#30340;&#25509;&#21475;</span>
<span class="org-preprocessor">#endif</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000028" class="outline-4">
<h4 id="org0000028"><span class="section-number-4">1.4.4</span> sycl</h4>
<div class="outline-text-4" id="text-1-4-4">
<p>
当配置了 `&#x2013;config=sycl` 时, 会使用 sycl 实现一些算子.
</p>

<p>
sycl 的支持与 GPU 的支持类似 (除了 stream executor) 的方式:
</p>

<ol class="org-ol">
<li>有些是用 sycl 直接实现</li>

<li>有些是通过 eigen 实现</li>
</ol>
</div>

<div id="outline-container-org0000022" class="outline-5">
<h5 id="org0000022"><span class="section-number-5">1.4.4.1</span> SoftmaxOp</h5>
<div class="outline-text-5" id="text-1-4-4-1">
<p>
softmax 的 sycl 支持是通过 eigen 实现的
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">SoftmaxEigenImpl</span> {
    <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-function-name">Compute</span>(
        <span class="org-keyword">const</span> <span class="org-type">Device</span>&amp; <span class="org-variable-name">d</span>, <span class="org-keyword">typename</span> <span class="org-constant">TTypes</span>&lt;<span class="org-type">T</span>&gt;::<span class="org-type">ConstMatrix</span> <span class="org-variable-name">logits</span>,
        <span class="org-keyword">typename</span> <span class="org-constant">TTypes</span>&lt;<span class="org-type">T</span>&gt;::<span class="org-type">Matrix</span> <span class="org-variable-name">softmax</span>, <span class="org-keyword">const</span> <span class="org-type">bool</span> <span class="org-variable-name">log</span>) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-comment-delimiter">// </span><span class="org-comment">logits &#26159; Eigen TensorMap</span>
        <span class="org-keyword">auto</span> <span class="org-variable-name">shifted_logits</span> =
            (logits - logits.maximum(along_class)
                          .eval()
                          .reshape(batch_by_one)
                          .broadcast(one_by_class));
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
    }
};

<span class="org-keyword">namespace</span> <span class="org-constant">functor</span> {
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">SoftmaxFunctorBase</span> {
    <span class="org-type">void</span> <span class="org-keyword">operator</span><span class="org-function-name">()</span>(
        <span class="org-keyword">const</span> <span class="org-type">Device</span>&amp; <span class="org-variable-name">d</span>, <span class="org-keyword">typename</span> <span class="org-constant">TTypes</span>&lt;<span class="org-type">T</span>&gt;::<span class="org-type">ConstMatrix</span> <span class="org-variable-name">logits</span>,
        <span class="org-keyword">typename</span> <span class="org-constant">TTypes</span>&lt;<span class="org-type">T</span>&gt;::<span class="org-type">Matrix</span> <span class="org-variable-name">softmax</span>, <span class="org-keyword">const</span> <span class="org-type">bool</span> <span class="org-variable-name">log</span>) {
        <span class="org-constant">SoftmaxEigenImpl</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>&gt;::Compute(d, logits, softmax, log);
    }
};
<span class="org-preprocessor">#ifdef</span> TENSORFLOW_USE_SYCL
<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">struct</span> <span class="org-type">SoftmaxFunctor</span>&lt;SYCLDevice, <span class="org-type">T</span>&gt; : <span class="org-type">SoftmaxFunctorBase</span>&lt;SYCLDevice, <span class="org-type">T</span>&gt; {};
<span class="org-preprocessor">#endif</span>
}  <span class="org-comment-delimiter">// </span><span class="org-comment">namespace functor</span>

<span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">Device</span>, <span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">class</span> <span class="org-type">SoftmaxOp</span> : <span class="org-keyword">public</span> <span class="org-type">OpKernel</span> {
   <span class="org-keyword">public</span>:
    <span class="org-keyword">explicit</span> <span class="org-function-name">SoftmaxOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">context</span>) : OpKernel(context) {
        log_ = <span class="org-constant">absl</span>::StartsWith(type_string(), <span class="org-string">"Log"</span>);
    }

    <span class="org-type">void</span> <span class="org-function-name">Compute</span>(<span class="org-type">OpKernelContext</span>* <span class="org-variable-name">context</span>) <span class="org-keyword">override</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-keyword">if</span> (logits_in.NumElements() &gt; 0) {
            <span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: &#36825;&#37324;&#30340; functor &#20250;&#26368;&#32456;&#35843;&#29992;&#21040; SoftmaxEigenImpl&lt;SyclDevice&gt;</span>
            <span class="org-constant">functor</span>::<span class="org-type">SoftmaxFunctor</span>&lt;<span class="org-type">Device</span>, <span class="org-type">T</span>&gt; <span class="org-variable-name">functor</span>;
            functor(
                context-&gt;eigen_device&lt;<span class="org-type">Device</span>&gt;(), logits_in.flat_inner_dims&lt;<span class="org-type">T</span>&gt;(),
                softmax_out-&gt;flat_inner_dims&lt;<span class="org-type">T</span>&gt;(), log_);
        }
    }
};

<span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: &#27880;&#20876;</span>
REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"Softmax"</span>).Device(DEVICE_SYCL).TypeConstraint&lt;<span class="org-type">float</span>&gt;(<span class="org-string">"T"</span>),
    SoftmaxOp&lt;SYCLDevice, <span class="org-type">float</span>&gt;);
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000025" class="outline-5">
<h5 id="org0000025"><span class="section-number-5">1.4.4.2</span> Pooling3DOp</h5>
<div class="outline-text-5" id="text-1-4-4-2">
<p>
Pooling3DOp 是直接用 sycl 实现的
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#ifdef</span> TENSORFLOW_USE_SYCL
REGISTER_KERNEL_BUILDER(
    Name(<span class="org-string">"MaxPool3DGradGrad"</span>).Device(DEVICE_SYCL).TypeConstraint&lt;T&gt;(<span class="org-string">"T"</span>),
    <span class="org-type">MaxPooling3dGradGradOp</span>&lt;SYCL_Device, T&gt;);
<span class="org-preprocessor">#endif</span>  <span class="org-comment-delimiter">// </span><span class="org-comment">TENSORFLOW_USE_SYCL</span>

<span class="org-keyword">template</span> &lt; <span class="org-keyword">typename</span> <span class="org-type">T</span>&gt; <span class="org-keyword">struct</span> <span class="org-type">LaunchAvgPooling3dGradOp</span> &lt; SYCLDevice,<span class="org-type">T</span>&gt; {
    <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-variable-name">launch</span>(
        OpKernelContext* context, <span class="org-keyword">const</span> TensorShape&amp; tensor_in_shape,
        <span class="org-keyword">const</span> Tensor&amp; out_backprop, <span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">array</span>&lt;int64, 3&gt;&amp; window,
        <span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">array</span>&lt;int64, 3&gt;&amp; stride,
        <span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">array</span>&lt;int64, 3&gt;&amp; output_shape,
        <span class="org-keyword">const</span> <span class="org-constant">std</span>::<span class="org-type">array</span>&lt;int64, 3&gt;&amp; padding, TensorFormat data_format,
        Tensor* output) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        device.sycl_queue().submit([&amp;](<span class="org-constant">cl</span>::<span class="org-constant">sycl</span>::<span class="org-type">handler</span>&amp; <span class="org-variable-name">cgh</span>) {
            <span class="org-keyword">auto</span> <span class="org-variable-name">input_backprop_access</span> =
                input_backprop_buffer
                    .<span class="org-keyword">template</span> get_access&lt;<span class="org-constant">cl</span>::<span class="org-constant">sycl</span>::<span class="org-constant">access</span>::<span class="org-constant">mode</span>::read&gt;(cgh);
            <span class="org-keyword">auto</span> <span class="org-variable-name">output_backprop_access</span> =
                output_backprop_buffer
                    .<span class="org-keyword">template</span> get_access&lt;<span class="org-constant">cl</span>::<span class="org-constant">sycl</span>::<span class="org-constant">access</span>::<span class="org-constant">mode</span>::write&gt;(cgh);
            <span class="org-type">AvgPool3DGradSYCL</span>&lt;<span class="org-type">T</span>&gt; <span class="org-variable-name">functor</span>(
                depth, batch, in_planes, in_rows, in_cols, output_shape, window,
                stride, padding, input_backprop_access, output_backprop_access);

            cgh.parallel_for(<span class="org-constant">cl</span>::<span class="org-constant">sycl</span>::range&lt;1&gt;(<span class="org-variable-name">num_threads</span>), functor);
        });
    }
};
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org000002b" class="outline-4">
<h4 id="org000002b"><span class="section-number-4">1.4.5</span> stream executor</h4>
<div class="outline-text-4" id="text-1-4-5">
<p>
使用 cublas 和 cudnn 的 op 需要用 stream executor 来执行, 例如:
</p>

<ol class="org-ol">
<li>matmul_op</li>
<li>conv_ops</li>
<li>where_op</li>
<li>fused_batch_norm_op</li>
<li>pooling_ops</li>
</ol>

<p>
stream executor 定义了几个功能集合, 例如:
</p>

<ol class="org-ol">
<li>BlasSupport</li>
<li>DnnSupport</li>
<li>RngSupport</li>
<li>FftSupport</li>
</ol>

<p>
cuda 和 rocm 均有对应的具体实现, 例如 BlasSupport 中的 DoBlasAsum, DoBlasDot,
&#x2026; 以及 DnnSupport 中的 DoMatmul, DoConvolve, &#x2026;
</p>
</div>
</div>
</div>

<div id="outline-container-org000003a" class="outline-3">
<h3 id="org000003a"><span class="section-number-3">1.5</span> xla</h3>
<div class="outline-text-3" id="text-1-5">
<p>
xla (Accelerated Linear Algebra) 是一个针对 graph 的 jit 编译器.
</p>

<p>
当 tensorflow 指定了 jit_compile 选项时, 则不再使用前面手写的 kernel (gemm 和
conv 除外), 而是使用 xla 编译生成的 kernel.
</p>

<p>
xla 编译的步骤大约是:
</p>

<ol class="org-ol">
<li>把图翻译成 HLO (类似于 <a href="../tvm/tvm_relay_ir.html#ID-404b926a-32a8-4ab8-8bdd-6f28a7ae0961">TVM Relay IR</a>), 新版本的 tensorflow 会先翻译成 MLIR 再翻译成 HLO</li>
<li>对 HLO 进行优化</li>
<li>xla 后端 (cpu, gpu) 把 HLO 转换为 LLIR</li>
<li>llvm 对 LLIR 进行优化</li>
<li>使用 llvm 后端生成 cpu 代码或 nvptx</li>
<li>nvptx (以及 cpu 代码) 通过 stream_executor 执行</li>
</ol>
</div>

<div id="outline-container-org0000031" class="outline-4">
<h4 id="org0000031"><span class="section-number-4">1.5.1</span> tf2xla</h4>
<div class="outline-text-4" id="text-1-5-1">
<p>
xla 并不能凭空生成 kernel, 它需要 tf2xla 告诉它 op 要怎么计算 (以 IR 的形式), 例如:
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-keyword">class</span> <span class="org-type">SoftmaxOp</span> : <span class="org-keyword">public</span> <span class="org-type">XlaOpKernel</span> {
   <span class="org-keyword">public</span>:
    <span class="org-keyword">explicit</span> <span class="org-function-name">SoftmaxOp</span>(<span class="org-type">OpKernelConstruction</span>* <span class="org-variable-name">ctx</span>) : XlaOpKernel(ctx) {
        log_ = <span class="org-constant">absl</span>::StartsWith(type_string(), <span class="org-string">"Log"</span>);
    }

    <span class="org-type">void</span> <span class="org-function-name">Compile</span>(<span class="org-type">XlaOpKernelContext</span>* <span class="org-variable-name">ctx</span>) <span class="org-keyword">override</span> {
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-constant">xla</span>::<span class="org-type">XlaBuilder</span>* <span class="org-keyword">const</span> <span class="org-variable-name">b</span> = ctx-&gt;builder();
        <span class="org-keyword">const</span> <span class="org-constant">xla</span>::<span class="org-type">XlaComputation</span>&amp; <span class="org-variable-name">max_func</span> = *ctx-&gt;GetOrCreateMax(type);

        <span class="org-keyword">auto</span> <span class="org-variable-name">logits_max</span> = <span class="org-constant">xla</span>::Reduce(
            logits, <span class="org-constant">xla</span>::MinValue(b, xla_type), max_func, {kClassDim});
        <span class="org-keyword">auto</span> <span class="org-variable-name">shifted_logits</span> = <span class="org-constant">xla</span>::Sub(logits, logits_max, batch_dims);
        <span class="org-keyword">auto</span> <span class="org-variable-name">exp_shifted</span> = <span class="org-constant">xla</span>::Exp(shifted_logits);
        <span class="org-comment-delimiter">// </span><span class="org-comment">...</span>
        <span class="org-keyword">auto</span> <span class="org-variable-name">reduce</span> = <span class="org-constant">xla</span>::Reduce(
            converted, <span class="org-constant">xla</span>::Zero(b, xla_accumulation_type),
            *ctx-&gt;GetOrCreateAdd(accumulation_type), {kClassDim});
        <span class="org-keyword">auto</span> <span class="org-variable-name">sum</span> = <span class="org-constant">XlaHelpers</span>::ConvertElementType(reduce, type);
        <span class="org-comment-delimiter">// </span><span class="org-comment">NOTE: softmax=exp/sum, sum &#21448;&#26469;&#33258; reduce (..., add)</span>
        <span class="org-keyword">auto</span> <span class="org-variable-name">softmax</span> = log_
                           ? <span class="org-constant">xla</span>::Sub(shifted_logits, <span class="org-constant">xla</span>::Log(sum), batch_dims)
                           : <span class="org-constant">xla</span>::Div(exp_shifted, sum, batch_dims);
        ctx-&gt;SetOutput(0, softmax);
    }
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000034" class="outline-4">
<h4 id="org0000034"><span class="section-number-4">1.5.2</span> HloModulePass</h4>
<div class="outline-text-4" id="text-1-5-2">
<ul class="org-ul">
<li>hlo_cse</li>
<li>hlo_dce</li>
<li>hlo_constant_folding</li>
<li>horizontal_fusion</li>
<li>instruction_fusion</li>
</ul>
</div>
</div>

<div id="outline-container-org0000037" class="outline-4">
<h4 id="org0000037"><span class="section-number-4">1.5.3</span> CpuCompiler/GpuCompiler</h4>
<div class="outline-text-4" id="text-1-5-3">
<ol class="org-ol">
<li>backend 通过 `RunHloPasses` 针对 HLO 做优化</li>

<li>`RunBackend` 先将 HLO 转换成 LLVM</li>

<li>`LinkAndOptimizeModule` 对 LLVM 做优化</li>

<li>`CompileTargetBinary` 把 llvm 转换成 ptx (或 cpu binary)</li>

<li>GpuExecutable 通过 stream_executor 把 ptx 转换成 cubin</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org000005f" class="outline-3">
<h3 id="org000005f"><span class="section-number-3">1.6</span> Parallelism</h3>
<div class="outline-text-3" id="text-1-6">
<p>
parallelism 主要涉及到几个方面:
</p>

<ol class="org-ol">
<li>dataset (及其预处理) 的并行, 主要是 dataset 的 num_parallel_calls 以及
prefetch 功能</li>

<li>inter_op_parallelism_threads</li>

<li>intra_op_parallelism_threads</li>

<li>OMP_NUM_THREADS</li>
</ol>

<p>
使用 <a href="https://www.tensorflow.org/guide/data_performance_analysis">TF Profiler</a> 可以分析 cpu 和 data 的并行情况.
</p>
</div>

<div id="outline-container-org000004c" class="outline-4">
<h4 id="org000004c"><span class="section-number-4">1.6.1</span> data parallelism</h4>
<div class="outline-text-4" id="text-1-6-1">
<p>
<a href="https://www.tensorflow.org/guide/data_performance">https://www.tensorflow.org/guide/data_performance</a>
</p>

<p>
dataset 的处理分为两步:
</p>

<ol class="org-ol">
<li>数据加载</li>
<li>数据增强和预处理</li>
</ol>

<p>
前者是 io bound, 并行会有效果, 但受限于数据传输速度; 后者是 cpu bound, 可以更好的利用并行.
</p>

<p>
tensorflow dataset api 用 prefetch, interleave, map 并行, 用 tf profiler 可以分析并行效果.
</p>

<p>
另外, nvidia 的 <a href="https://developer.nvidia.cn/zh-cn/dali">DALI</a> 库可以用 GPU 加快 dataset 并行处理.
</p>
</div>

<div id="outline-container-org000003d" class="outline-5">
<h5 id="org000003d"><span class="section-number-5">1.6.1.1</span> generator</h5>
<div class="outline-text-5" id="text-1-6-1-1">
<p>
tensorflow dataset api 是一个 <a href="../tech/coroutine.html#ID-e969e80b-4060-4209-91e1-baa79e4076b2">generator</a> 模式的 api, 用来构造 data pipeline, 例如:
</p>

<div class="org-src-container">
<pre class="src src-C++">dataset(generator)
  .map(fn1)
  .filter(fn2)
  .shuffle()
  .prefetch();
</pre>
</div>

<p>
上面的例子中, generator 负责加载数据, 后面的 map, filter 等负责数据增强和预处理.
</p>

<p>
generator 模式适合并行: cuda/sycl/opencl/openmp 等并行架构实际上与 generator 的
map 类似: 把 `kernel` map 到某个序列上产生另一个序列
</p>
</div>
</div>

<div id="outline-container-org0000041" class="outline-5">
<h5 id="org0000041"><span class="section-number-5">1.6.1.2</span> prefetch</h5>
<div class="outline-text-5" id="text-1-6-1-2">
<p>
单个 dataset (generator) 是底层数据文件的抽象, 无法并行使用, 但是可以
prefetch. 例如这样:
</p>

<pre class="example" id="org0000040">
no prefetch:

generator.next()                       generator.next()                  
----------------                       ----------------               
                map                                    map            
                ---------                              ---------      
                         train                                  train 
                         --------------                         --------------

prefetch:

generator.next()  generator.next()                  
----------------  ----------------
                map               map                                  
                ---------         ---------                            
                         train             train                       
                         --------------    --------------
</pre>
</div>
</div>

<div id="outline-container-org0000045" class="outline-5">
<h5 id="org0000045"><span class="section-number-5">1.6.1.3</span> interleave</h5>
<div class="outline-text-5" id="text-1-6-1-3">
<p>
如果 prefetch 不够, 可以用 interleave, interleave 通过 num_parallel_calls 参数可以并行的加载 <b>多个</b> dataset, 例如这样:
</p>

<pre class="example" id="org0000044">
interleave:

generator.next()                    
----------  
generator2.next()               
----------                                  
          map                             
          ---------                       
                   train                  
                   --------------
</pre>
</div>
</div>

<div id="outline-container-org0000049" class="outline-5">
<h5 id="org0000049"><span class="section-number-5">1.6.1.4</span> map</h5>
<div class="outline-text-5" id="text-1-6-1-4">
<p>
map 也提供了 num_parallel_calls 可以并行的 map, 例如这样
</p>

<pre class="example" id="org0000048">
generator.next()               
----------------                                  
                map                             
                ------
                map                             
                ------
                      train                  
                      --------------
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000050" class="outline-4">
<h4 id="org0000050"><span class="section-number-4">1.6.2</span> inter_op_parallelism_threads</h4>
<div class="outline-text-4" id="text-1-6-2">
<p>
指相互独立的 op 可以并行的计算, 例如:
</p>

<pre class="example" id="org000004f">
x=a+b
y=c+d
z=x+y
</pre>

<p>
`a+b` 和 `c+d` 可以并行计算.
</p>

<p>
用 inter_op_parallelism_threads 可以限定相应上层 tensorflow executor 的 thread
pool 的大小, 例如 DirectSession 的 thread_pools_ 和 grpc_server 的 compute_pool
</p>
</div>
</div>

<div id="outline-container-org0000053" class="outline-4">
<h4 id="org0000053"><span class="section-number-4">1.6.3</span> intra_op_parallelism_threads</h4>
<div class="outline-text-4" id="text-1-6-3">
<p>
eigen 的 TensorDeviceThreadPool 支持对单个 op 并行计算, tensorflow 通过
intra_op_parallelism_threads 设置 eigen threadpool 的大小
</p>
</div>
</div>

<div id="outline-container-org0000056" class="outline-4">
<h4 id="org0000056"><span class="section-number-4">1.6.4</span> OMP_NUM_THREADS</h4>
<div class="outline-text-4" id="text-1-6-4">
<p>
mkl 算子的并行是通过 openmp 实现, OMP_NUM_THREADS 可以控制 openmp 使用的线程个数.
OMP_NUM_THREADS 之于 mkl 相当于 intra_op_parallelism_threads 之于 eigen.
</p>
</div>
</div>

<div id="outline-container-org0000059" class="outline-4">
<h4 id="org0000059"><span class="section-number-4">1.6.5</span> 测试</h4>
<div class="outline-text-4" id="text-1-6-5">
<p>
通过测试一个简单的 mnist 模型, 有以下结论:
</p>

<ol class="org-ol">
<li>inter_op_parallelism_threads 作用不大, 可能是因为 mnist 模型没有分支结构, 换成 <a href="../machine_learning/inception.html#ID-c8e4ddb9-1372-4a28-99c3-93f7f3e56bdf">Inception</a> 应该会明显起作用.</li>

<li>intra_op_parallelism_threads 对 eigen 作用很大, OMP_NUM_THREADS 对 eigen 没有作用</li>

<li>OMP_NUM_THREADS 对 mkl 作用很大, intra_op_parallelism_threads 的作用不明显</li>

<li><p>
设定各个 pool 的大小时需要考虑 core 的个数以避免 thread oversubscription, 例如当 `inter_op_parallelism_threads * OMP_NUM_THREADS 超过 core` 时 mkl 性能会下降很多.
</p>

<p>
另外, 看起来 tensorflow 的代码会强制 inter_op_parallelism_threads 不超过 core
的个数, 且 mkl 默认的 intra_op_parallelism_threads 等于 core/OMP_NUM_THREADS
</p></li>
</ol>
</div>
</div>

<div id="outline-container-org000005c" class="outline-4">
<h4 id="org000005c"><span class="section-number-4">1.6.6</span> 其它</h4>
<div class="outline-text-4" id="text-1-6-6">
<ul class="org-ul">
<li><p>
xla
</p>

<p>
虽然 xla 有可能通过并行加速编译
(<a href="https://groups.google.com/g/xla-dev/c/VZMMD44BC7Y">https://groups.google.com/g/xla-dev/c/VZMMD44BC7Y</a>), 但由于 xla 只需要编译一次,
所以对整个训练过程影响不大.
</p></li>

<li><p>
cuda stream
</p>

<p>
tensorflow 只会使用一个 (a single) cuda stream:
<a href="https://github.com/tensorflow/tensorflow/issues/36634">https://github.com/tensorflow/tensorflow/issues/36634</a>
</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunway@dogdog.run)<br />
Date: 2022-10-17 Mon 15:18<br />
Last updated: 2022-10-24 Mon 19:21</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
