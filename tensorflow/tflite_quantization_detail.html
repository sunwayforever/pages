<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-01-25 二 15:54 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>TFLite Quantization Details</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Wei Sun (孙伟)" />
<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">TFLite Quantization Details</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgd9b34bb">1. TFLite Quantization Details</a>
<ul>
<li><a href="#orgc381aec">1.1. Mul Reference Kernel</a>
<ul>
<li><a href="#org129b235">1.1.1. Overview</a></li>
<li><a href="#orgb9a723e">1.1.2. rounding_doubling_high_mul</a></li>
<li><a href="#org1487cc0">1.1.3. Prepare</a></li>
<li><a href="#org5a82901">1.1.4. EvalQuantized</a></li>
<li><a href="#org8ba9a00">1.1.5. Reference Kernel</a></li>
</ul>
</li>
<li><a href="#orgc4858e7">1.2. FullyConnected Reference Kernel</a>
<ul>
<li><a href="#org6fc611b">1.2.1. Quantize</a></li>
<li><a href="#orgbc44914">1.2.2. Prepare</a></li>
<li><a href="#org69cc5cf">1.2.3. EvalQuantized</a></li>
<li><a href="#orge5d4058">1.2.4. Reference Kernel</a></li>
</ul>
</li>
<li><a href="#orgc9b6b32">1.3. BatchNorm 导致很大的量化误差</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgd9b34bb" class="outline-2">
<h2 id="orgd9b34bb"><span class="section-number-2">1</span> TFLite Quantization Details</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc#org6cafb69">file:///home/sunway/source/tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc#org6cafb69</a>
</p>
</div>

<div id="outline-container-orgc381aec" class="outline-3">
<h3 id="orgc381aec"><span class="section-number-3">1.1</span> Mul Reference Kernel</h3>
<div class="outline-text-3" id="text-1-1">
<p>
以最简单的 Mul 的 reference kernel 为例, 介绍 EvalQuantized 的流程
</p>
</div>

<div id="outline-container-org129b235" class="outline-4">
<h4 id="org129b235"><span class="section-number-4">1.1.1</span> Overview</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
Mul, 即 a*b=c 操作, 量化后实际上需要计算的是:
</p>

<p>
已知 \(S_a, Q_a, Z_a, S_b, Q_b, Z_b, S_c, Z_c\), 根据 \(S_a(Q_a-Z_a)*S_b(Q_b-Z_b)=S_c(Q_c-Z_c)\)  计算出 \(Q_c\)
</p>

<p>
tflite 量化时会保证所有的 \(Z\) 都是整数, 但 \(S\) 是浮点数
</p>
</div>
</div>

<div id="outline-container-orgb9a723e" class="outline-4">
<h4 id="orgb9a723e"><span class="section-number-4">1.1.2</span> rounding_doubling_high_mul</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
rounding_doubling_high_mul 解决的问题是如何用整数运算来计算 int*float
</p>

<div class="org-src-container">
<pre class="src src-C++"><span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;cmath&gt;</span>
<span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;cstdint&gt;</span>
<span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;cstdio&gt;</span>
<span style="color: #268bd2;">#include</span> <span style="color: #2aa198;">&lt;limits&gt;</span>

<span style="color: #859900;">inline</span> <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">SaturatingRoundingDoublingHighMul</span><span style="color: #757575;">(</span><span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">a</span><span style="color: #757575;">,</span> <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">b</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #b58900;">bool</span> <span style="color: #268bd2;">overflow</span> = a == b &amp;&amp; a == <span style="color: #268bd2; font-weight: bold;">std</span>::<span style="color: #268bd2; font-weight: bold;">numeric_limits</span>&lt;<span style="color: #b58900;">int32_t</span>&gt;::min<span style="color: #757575;">()</span>;
    <span style="color: #b58900;">int64_t</span> <span style="color: #268bd2;">a_64</span><span style="color: #757575;">(</span>a<span style="color: #757575;">)</span>;
    <span style="color: #b58900;">int64_t</span> <span style="color: #268bd2;">b_64</span><span style="color: #757575;">(</span>b<span style="color: #757575;">)</span>;
    <span style="color: #b58900;">int64_t</span> <span style="color: #268bd2;">ab_64</span> = a_64 * b_64;
    <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">nudge</span> = ab_64 &gt;= 0 ? <span style="color: #757575;">(</span>1 &lt;&lt; 30<span style="color: #757575;">)</span> : <span style="color: #757575;">(</span>1 - <span style="color: #757575;">(</span>1 &lt;&lt; 30<span style="color: #757575;">))</span>;
    <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">ab_x2_high32</span> = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">int32_t</span>&gt;<span style="color: #757575;">((</span>ab_64 + nudge<span style="color: #757575;">)</span> / <span style="color: #757575;">(</span>1ll &lt;&lt; 31<span style="color: #757575;">))</span>;
    <span style="color: #859900;">return</span> overflow ? <span style="color: #268bd2; font-weight: bold;">std</span>::<span style="color: #268bd2; font-weight: bold;">numeric_limits</span>&lt;<span style="color: #b58900;">int32_t</span>&gt;::max<span style="color: #757575;">()</span> : ab_x2_high32;
<span style="color: #757575;">}</span>

<span style="color: #b58900;">int</span> <span style="color: #268bd2;">quantized_mulitply</span><span style="color: #757575;">(</span><span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">a</span><span style="color: #757575;">,</span> <span style="color: #b58900;">float</span> <span style="color: #268bd2;">b</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">prepare</span>
    <span style="color: #b58900;">int</span> <span style="color: #268bd2;">shift</span> = 0;
    <span style="color: #b58900;">double</span> <span style="color: #268bd2;">q</span> = <span style="color: #268bd2; font-weight: bold;">std</span>::frexp<span style="color: #757575;">(</span>b<span style="color: #757575;">,</span> &amp;shift<span style="color: #757575;">)</span>;
    printf<span style="color: #757575;">(</span><span style="color: #2aa198;">"-- %f %d\n"</span><span style="color: #757575;">,</span> q<span style="color: #757575;">,</span> shift<span style="color: #757575;">)</span>;
    <span style="color: #859900;">auto</span> <span style="color: #268bd2;">q_fixed</span> = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">int64_t</span>&gt;<span style="color: #757575;">(</span><span style="color: #b58900;">int</span><span style="color: #757575;">(</span>q * <span style="color: #757575;">(</span>1ll &lt;&lt; 31<span style="color: #757575;">)))</span>;
    printf<span style="color: #757575;">(</span><span style="color: #2aa198;">"-- %ld\n"</span><span style="color: #757575;">,</span> q_fixed<span style="color: #757575;">)</span>;
    <span style="color: #586e75;">// </span><span style="color: #586e75;">eval</span>
    <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">x</span> = SaturatingRoundingDoublingHighMul<span style="color: #757575;">(</span>100<span style="color: #757575;">,</span> <span style="color: #757575;">(</span><span style="color: #b58900;">int32_t</span><span style="color: #757575;">)</span>q_fixed<span style="color: #757575;">)</span>;
    printf<span style="color: #757575;">(</span><span style="color: #2aa198;">"-- %d\n"</span><span style="color: #757575;">,</span> x<span style="color: #757575;">)</span>;
    <span style="color: #859900;">return</span> x &gt;&gt; <span style="color: #757575;">(</span>-shift<span style="color: #757575;">)</span>;
<span style="color: #757575;">}</span>

<span style="color: #b58900;">int</span> <span style="color: #268bd2;">main</span><span style="color: #757575;">(</span><span style="color: #b58900;">int</span> <span style="color: #268bd2;">argc</span><span style="color: #757575;">,</span> <span style="color: #b58900;">char</span> *<span style="color: #268bd2;">argv</span>[]<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #b58900;">int32_t</span> <span style="color: #268bd2;">a</span> = 100;
    <span style="color: #b58900;">float</span> <span style="color: #268bd2;">b</span> = 0.12;
    printf<span style="color: #757575;">(</span><span style="color: #2aa198;">"%d * %f = %d\n"</span><span style="color: #757575;">,</span> a<span style="color: #757575;">,</span> b<span style="color: #757575;">,</span> quantized_mulitply<span style="color: #757575;">(</span>a<span style="color: #757575;">,</span> b<span style="color: #757575;">))</span>;
    <span style="color: #859900;">return</span> 0;
<span style="color: #757575;">}</span>

</pre>
</div>

<p>
&#x2013; 0.768000 -6
&#x2013; 1649267456
&#x2013; 77
100 * 0.012000 = 1
</p>
</div>
</div>

<div id="outline-container-org1487cc0" class="outline-4">
<h4 id="org1487cc0"><span class="section-number-4">1.1.3</span> Prepare</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>

</p>

<div class="org-src-container">
<pre class="src src-c++">mul.<span style="color: #268bd2; font-weight: bold;">cc</span>:<span style="color: #268bd2; font-weight: bold;">Prepare</span>:
    <span style="color: #586e75;">// </span><span style="color: #586e75;">S_a(Q_a-Z_a)*S_b(Q_b-Z_b)=S_c(Q_c-Z_c)</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">real_multiplier &#20026; (S_a*S_b)/(S_c)</span>
    <span style="color: #b58900;">double</span> <span style="color: #268bd2;">real_multiplier</span> =
        input1-&gt;params.scale * input2-&gt;params.scale / output-&gt;params.scale;

    <span style="color: #586e75;">// </span><span style="color: #586e75;">QuantizeMultiplier &#20250;&#25226; real_multiplier &#36716;&#25442;&#20026;&#19968;&#20010;&#25972;&#25968;&#21644;&#19968;&#20010; shift &#30340;&#24418;&#24335;</span>
    QuantizeMultiplier<span style="color: #757575;">(</span>real_multiplier<span style="color: #757575;">,</span> &amp;data-&gt;output_multiplier<span style="color: #757575;">,</span>
                       &amp;data-&gt;output_shift<span style="color: #757575;">)</span>;
</pre>
</div>
</div>

<div id="outline-container-orge700191" class="outline-5">
<h5 id="orge700191"><span class="section-number-5">1.1.3.1</span> QuantizeMultiplier</h5>
<div class="outline-text-5" id="text-1-1-3-1">
<p>
QuantizeMultiplier 会把浮点的 real_multiplier (即 scale) 变为 int/shift 的形式,
例如, 若 real_multiplier 为 0.012, 则 quantized_multiplier 为 1649267456, shift
为 -6 (参考 <a href="#orgb9a723e">rounding_doubling_high_mul</a>)
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="color: #b58900;">void</span> <span style="color: #268bd2;">QuantizeMultiplier</span><span style="color: #757575;">(</span><span style="color: #b58900;">double</span> <span style="color: #268bd2;">double_multiplier</span><span style="color: #757575;">,</span> <span style="color: #b58900;">int32_t</span>* <span style="color: #268bd2;">quantized_multiplier</span><span style="color: #757575;">,</span>
                        <span style="color: #b58900;">int</span>* <span style="color: #268bd2;">shift</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
  <span style="color: #859900;">const</span> <span style="color: #b58900;">double</span> <span style="color: #268bd2;">q</span> = <span style="color: #268bd2; font-weight: bold;">std</span>::frexp<span style="color: #757575;">(</span>double_multiplier<span style="color: #757575;">,</span> shift<span style="color: #757575;">)</span>;
  <span style="color: #859900;">auto</span> <span style="color: #268bd2;">q_fixed</span> = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">int64_t</span>&gt;<span style="color: #757575;">(</span>TfLiteRound<span style="color: #757575;">(</span>q * <span style="color: #757575;">(</span>1ll &lt;&lt; 31<span style="color: #757575;">)))</span>;
  *quantized_multiplier = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">int32_t</span>&gt;<span style="color: #757575;">(</span>q_fixed<span style="color: #757575;">)</span>;
<span style="color: #757575;">}</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org5a82901" class="outline-4">
<h4 id="org5a82901"><span class="section-number-4">1.1.4</span> EvalQuantized</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>

</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="color: #859900;">template</span> &lt;<span style="color: #b58900;">KernelType</span> <span style="color: #268bd2;">kernel_type</span>&gt;
<span style="color: #b58900;">TfLiteStatus</span> <span style="color: #268bd2;">EvalQuantized</span><span style="color: #757575;">(</span>
    <span style="color: #b58900;">TfLiteContext</span>* <span style="color: #268bd2;">context</span><span style="color: #757575;">,</span> <span style="color: #b58900;">TfLiteNode</span>* <span style="color: #268bd2;">node</span><span style="color: #757575;">,</span> <span style="color: #b58900;">TfLiteMulParams</span>* <span style="color: #268bd2;">params</span><span style="color: #757575;">,</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">OpData</span>* <span style="color: #268bd2;">data</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">TfLiteTensor</span>* <span style="color: #268bd2;">input1</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">TfLiteTensor</span>* <span style="color: #268bd2;">input2</span><span style="color: #757575;">,</span>
    <span style="color: #b58900;">TfLiteTensor</span>* <span style="color: #268bd2;">output</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>

    <span style="color: #268bd2; font-weight: bold;">tflite</span>::<span style="color: #b58900;">ArithmeticParams</span> <span style="color: #268bd2;">op_params</span>;
    op_params.input1_offset = -input1-&gt;params.zero_point;
    op_params.input2_offset = -input2-&gt;params.zero_point;
    op_params.output_offset = output-&gt;params.zero_point;
    op_params.output_multiplier = data-&gt;output_multiplier;
    op_params.output_shift = data-&gt;output_shift;

<span style="color: #268bd2;">#define</span> <span style="color: #268bd2;">TF_LITE_MUL</span><span style="color: #757575;">(</span><span style="color: #268bd2;">type</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">opname</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">dtype</span><span style="color: #757575;">)</span>                                 \
    <span style="color: #268bd2; font-weight: bold;">type</span>::opname<span style="color: #757575;">(</span>                                                        \
        op_params<span style="color: #757575;">,</span> GetTensorShape<span style="color: #757575;">(</span>input1<span style="color: #757575;">),</span> GetTensorData&lt;dtype&gt;<span style="color: #757575;">(</span>input1<span style="color: #757575;">),</span> \
        GetTensorShape<span style="color: #757575;">(</span>input2<span style="color: #757575;">),</span> GetTensorData&lt;dtype&gt;<span style="color: #757575;">(</span>input2<span style="color: #757575;">),</span>            \
        GetTensorShape<span style="color: #757575;">(</span>output<span style="color: #757575;">),</span> GetTensorData&lt;dtype&gt;<span style="color: #757575;">(</span>output<span style="color: #757575;">))</span>

    <span style="color: #859900;">if</span> <span style="color: #757575;">(</span>input1-&gt;type == kTfLiteInt8<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
        TF_LITE_MUL<span style="color: #757575;">(</span>reference_integer_ops<span style="color: #757575;">,</span> Mul<span style="color: #757575;">,</span> int8_t<span style="color: #757575;">)</span>;
    <span style="color: #757575;">}</span>
<span style="color: #757575;">}</span>
</pre>
</div>

<ul class="org-ul">
<li>reference kernel 比较低效: 它会针对每个元素计算乘积, 实际上在 x86 上的 mul
optimized kernel 同样使用 Elementwise Mul.</li>

<li><p>
tflite 的 optimized kernel 只对 fully_connected, conv 等使用 来做 gemm
</p>

<p>

</p></li>
</ul>
</div>
</div>

<div id="outline-container-org8ba9a00" class="outline-4">
<h4 id="org8ba9a00"><span class="section-number-4">1.1.5</span> Reference Kernel</h4>
<div class="outline-text-4" id="text-1-1-5">
<div class="org-src-container">
<pre class="src src-c++"><span style="color: #859900;">template</span> &lt;<span style="color: #859900;">typename</span> <span style="color: #b58900;">T</span>&gt;
<span style="color: #859900;">inline</span> <span style="color: #b58900;">void</span> <span style="color: #268bd2;">MulElementwise</span><span style="color: #757575;">(</span>
    <span style="color: #b58900;">int</span> <span style="color: #268bd2;">size</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">ArithmeticParams</span>&amp; <span style="color: #268bd2;">params</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">T</span>* <span style="color: #268bd2;">input1_data</span><span style="color: #757575;">,</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">T</span>* <span style="color: #268bd2;">input2_data</span><span style="color: #757575;">,</span> <span style="color: #b58900;">T</span>* <span style="color: #268bd2;">output_data</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #859900;">for</span> <span style="color: #757575;">(</span><span style="color: #b58900;">int</span> <span style="color: #268bd2;">i</span> = 0; i &lt; size; ++i<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
        <span style="color: #586e75;">// </span><span style="color: #586e75;">Q_a-Z_a</span>
        <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">input1_val</span> = params.input1_offset + input1_data[i];
        <span style="color: #586e75;">// </span><span style="color: #586e75;">Q_b-Z_b</span>
        <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">input2_val</span> = params.input2_offset + input2_data[i];
        <span style="color: #586e75;">// </span><span style="color: #586e75;">Q_c=Z_c+real_multiplier*((Q_a-Z_a)*(Q_b-Z_b))</span>
        <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">output</span> = params.output_offset +
                             MultiplyByQuantizedMultiplier<span style="color: #757575;">(</span>
                                 input1_val * input2_val<span style="color: #757575;">,</span>
                                 params.output_multiplier<span style="color: #757575;">,</span> params.output_shift<span style="color: #757575;">)</span>;
        output_data[i] = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">T</span>&gt;<span style="color: #757575;">(</span>output<span style="color: #757575;">)</span>;
    <span style="color: #757575;">}</span>
<span style="color: #757575;">}</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-c++"><span style="color: #859900;">inline</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">MultiplyByQuantizedMultiplier</span><span style="color: #757575;">(</span>
    <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">x</span><span style="color: #757575;">,</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">quantized_multiplier</span><span style="color: #757575;">,</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">shift</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #859900;">using</span> <span style="color: #268bd2; font-weight: bold;">gemmlowp</span>::<span style="color: #b58900;">RoundingDivideByPOT</span>;
    <span style="color: #859900;">using</span> <span style="color: #268bd2; font-weight: bold;">gemmlowp</span>::<span style="color: #b58900;">SaturatingRoundingDoublingHighMul</span>;
    <span style="color: #b58900;">int</span> <span style="color: #268bd2;">right_shift</span> = -shift;
    <span style="color: #859900;">return</span> RoundingDivideByPOT<span style="color: #757575;">(</span>
        SaturatingRoundingDoublingHighMul<span style="color: #757575;">(</span>x<span style="color: #757575;">,</span> quantized_multiplier<span style="color: #757575;">),</span>
        right_shift<span style="color: #757575;">)</span>;
<span style="color: #757575;">}</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc4858e7" class="outline-3">
<h3 id="orgc4858e7"><span class="section-number-3">1.2</span> FullyConnected Reference Kernel</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org6fc611b" class="outline-4">
<h4 id="org6fc611b"><span class="section-number-4">1.2.1</span> Quantize</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/toco/graph_transformations/quantize.cc#org598be18">file:///home/sunway/source/tensorflow/tensorflow/lite/toco/graph_transformations/quantize.cc#org598be18</a>
</p>

<p>
针对 FullyConnected 进行 quantize 时, 先强制要求 bias 的 scale == input_scale *
weight_scale, 以便后面 \(Q_w*Q_x+Q_b\) 可以在相同的 scale 下进行计算
</p>

<div class="org-src-container">
<pre class="src src-c++"><span style="color: #859900;">if</span> <span style="color: #757575;">(</span>is_bias_vector<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">Quantization of bias vector.</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">We need both of the mandatory inputs (input activations and weights) to</span>
    <span style="color: #586e75;">// </span><span style="color: #586e75;">have been already quantized.</span>
    <span style="color: #859900;">const</span> <span style="color: #859900;">auto</span>&amp; <span style="color: #268bd2;">input_activations</span> =
        model-&gt;GetArray<span style="color: #757575;">(</span>op.inputs[activations_input_index]<span style="color: #757575;">)</span>;
    <span style="color: #859900;">const</span> <span style="color: #859900;">auto</span>&amp; <span style="color: #268bd2;">input_weights</span> = model-&gt;GetArray<span style="color: #757575;">(</span>op.inputs[weights_input_index]<span style="color: #757575;">)</span>;

    <span style="color: #859900;">const</span> <span style="color: #859900;">auto</span> <span style="color: #268bd2;">input_activations_scale</span> =
        input_activations.quantization_params-&gt;scale;
    <span style="color: #859900;">const</span> <span style="color: #859900;">auto</span> <span style="color: #268bd2;">input_weights_scale</span> = input_weights.quantization_params-&gt;scale;
    quantization_params-&gt;scale = input_activations_scale * input_weights_scale;
    quantization_params-&gt;zero_point = 0;
    *quantized_data_type = GetQuantizedDataType<span style="color: #757575;">(</span>array<span style="color: #757575;">,</span> <span style="color: #268bd2; font-weight: bold;">ArrayDataType</span>::kInt32<span style="color: #757575;">)</span>;
    <span style="color: #859900;">return</span> <span style="color: #268bd2; font-weight: bold;">true</span>;
<span style="color: #757575;">}</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgbc44914" class="outline-4">
<h4 id="orgbc44914"><span class="section-number-4">1.2.2</span> Prepare</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#orgc2fb496">file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#orgc2fb496</a>
Prepare 与 Mul 的 Prepare 基本相同: 它也会提前计算 real_multiplier =
input_multiplier * weight_multiplier, 以及 QuantizeMultiplier. 需要注意的是 bias
的 scale 不需要考虑, 因为 quantize_model 时保证了它与 real_multiplier 是相同的
</p>
</div>
</div>

<div id="outline-container-org69cc5cf" class="outline-4">
<h4 id="org69cc5cf"><span class="section-number-4">1.2.3</span> EvalQuantized</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#orga6562ce">file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#orga6562ce</a>
</p>

<p>
EvalQuantized 与 Mul 的 EvalQuantized 也是基本相同
</p>
</div>
</div>

<div id="outline-container-orge5d4058" class="outline-4">
<h4 id="orge5d4058"><span class="section-number-4">1.2.4</span> Reference Kernel</h4>
<div class="outline-text-4" id="text-1-2-4">
<div class="org-src-container">
<pre class="src src-c++"><span style="color: #859900;">inline</span> <span style="color: #b58900;">void</span> <span style="color: #268bd2;">FullyConnected</span><span style="color: #757575;">(</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">FullyConnectedParams</span>&amp; <span style="color: #268bd2;">params</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">RuntimeShape</span>&amp; <span style="color: #268bd2;">input_shape</span><span style="color: #757575;">,</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int8_t</span>* <span style="color: #268bd2;">input_data</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">RuntimeShape</span>&amp; <span style="color: #268bd2;">filter_shape</span><span style="color: #757575;">,</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int8_t</span>* <span style="color: #268bd2;">filter_data</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">RuntimeShape</span>&amp; <span style="color: #268bd2;">bias_shape</span><span style="color: #757575;">,</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span>* <span style="color: #268bd2;">bias_data</span><span style="color: #757575;">,</span> <span style="color: #859900;">const</span> <span style="color: #b58900;">RuntimeShape</span>&amp; <span style="color: #268bd2;">output_shape</span><span style="color: #757575;">,</span>
    <span style="color: #b58900;">int8_t</span>* <span style="color: #268bd2;">output_data</span><span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">input_offset</span> = params.input_offset;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">filter_offset</span> = params.weights_offset;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">output_offset</span> = params.output_offset;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">output_multiplier</span> = params.output_multiplier;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">output_shift</span> = params.output_shift;

    <span style="color: #859900;">const</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">filter_dim_count</span> = filter_shape.DimensionsCount<span style="color: #757575;">()</span>;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">batches</span> = output_shape.Dims<span style="color: #757575;">(</span>0<span style="color: #757575;">)</span>;
    <span style="color: #859900;">const</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">output_depth</span> = output_shape.Dims<span style="color: #757575;">(</span>1<span style="color: #757575;">)</span>;

    <span style="color: #859900;">const</span> <span style="color: #b58900;">int</span> <span style="color: #268bd2;">accum_depth</span> = filter_shape.Dims<span style="color: #757575;">(</span>filter_dim_count - 1<span style="color: #757575;">)</span>;
    <span style="color: #859900;">for</span> <span style="color: #757575;">(</span><span style="color: #b58900;">int</span> <span style="color: #268bd2;">b</span> = 0; b &lt; batches; ++b<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
        <span style="color: #859900;">for</span> <span style="color: #757575;">(</span><span style="color: #b58900;">int</span> <span style="color: #268bd2;">out_c</span> = 0; out_c &lt; output_depth; ++out_c<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
            <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">acc</span> = 0;
            <span style="color: #859900;">for</span> <span style="color: #757575;">(</span><span style="color: #b58900;">int</span> <span style="color: #268bd2;">d</span> = 0; d &lt; accum_depth; ++d<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
                <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">input_val</span> = input_data[b * accum_depth + d];
                <span style="color: #b58900;">int32</span> <span style="color: #268bd2;">filter_val</span> = filter_data[out_c * accum_depth + d];
                acc +=
                    <span style="color: #757575;">(</span>filter_val + filter_offset<span style="color: #757575;">)</span> * <span style="color: #757575;">(</span>input_val + input_offset<span style="color: #757575;">)</span>;
            <span style="color: #757575;">}</span>
            <span style="color: #859900;">if</span> <span style="color: #757575;">(</span>bias_data<span style="color: #757575;">)</span> <span style="color: #757575;">{</span>
                <span style="color: #586e75;">// </span><span style="color: #586e75;">&#36825;&#37324; bias &#21487;&#20197;&#30452;&#25509;&#30456;&#21152;, &#22240;&#20026; bias &#19982; w*x &#30340; scale &#30456;&#21516;</span>
                acc += bias_data[out_c];
            <span style="color: #757575;">}</span>
            acc = MultiplyByQuantizedMultiplier<span style="color: #757575;">(</span>
                acc<span style="color: #757575;">,</span> output_multiplier<span style="color: #757575;">,</span> output_shift<span style="color: #757575;">)</span>;
            acc += output_offset;
            output_data[out_c + output_depth * b] = <span style="color: #859900;">static_cast</span>&lt;<span style="color: #b58900;">int8_t</span>&gt;<span style="color: #757575;">(</span>acc<span style="color: #757575;">)</span>;
        <span style="color: #757575;">}</span>
    <span style="color: #757575;">}</span>
<span style="color: #757575;">}</span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc9b6b32" class="outline-3">
<h3 id="orgc9b6b32"><span class="section-number-3">1.3</span> BatchNorm 导致很大的量化误差</h3>
<div class="outline-text-3" id="text-1-3">
<p>
当训练很多个 epoch 后, 会发现量化误差很大, 通过观察 tflite 量化的数据,发现原因是
batchnorm 对应的 mul 操作的 scale 的最大值很大, 导致很大的量化误差.
</p>

<p>
根据 batchnorm 的公式, scale = gamma / (np.sqrt(var + eps)), 通过 log 可以看到,
随着训练 epoch 的增加, min(var) 和 min(mean) 都变得很小 (例如 1e-3&#x2026;), 导致
scale 变大.
</p>

<p>
var 和 mean 很接近 0, 说明 hidden layer 输出有零， 有两种可能的原因:
</p>

<ol class="org-ol">
<li><p>
hidden layer 中对应的权重接近于零
</p>

<p>
\(\left\{\begin{array}{c}w*x_1+b=0\\w*x_2+b=0\\x_1 \ne x_2\end{array} \implies
   w=0\)
</p></li>

<li>或者 hidden layer 输出因为 relu 变为零 (Gradient Starvation)</li>
</ol>

<p>
解决方法:
</p>

<ol class="org-ol">
<li>减少 batchnorm 前面 layer 的规模, 避免出现接近零的权重</li>
<li>使用 leaky relu 等 relu 变种，避免 relu 导致的神经元死亡的情况.</li>
<li>relu 放在 batchnorm 之后而不是之前</li>
</ol>

<p>
实际上在这个例子中，出现问题的原因是 relu 导致 dense 输出的某些点针对所有样本都为零, 即有些神经元已经死亡.
</p>

<p>
通过下面的脚本可以观察网络中 batchnorm 的 scale 的情况.
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #586e75;">#</span><span style="color: #586e75;">!/usr/bin/env python3</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">-*- coding: utf-8 -*-</span>
<span style="color: #586e75;"># </span><span style="color: #586e75;">2021-08-13 09:58</span>
<span style="color: #859900;">import</span> numpy <span style="color: #859900;">as</span> np

<span style="color: #859900;">from</span> models <span style="color: #859900;">import</span> deep_cnn
<span style="color: #859900;">import</span> config


<span style="color: #859900;">def</span> <span style="color: #268bd2;">check_model</span><span style="color: #757575;">(</span>path<span style="color: #757575;">)</span>:
    <span style="color: #859900;">print</span><span style="color: #757575;">(</span>f<span style="color: #2aa198;">"------{path}------"</span><span style="color: #757575;">)</span>
    <span style="color: #268bd2;">model</span> = deep_cnn<span style="color: #757575;">()</span>
    model.load_weights<span style="color: #757575;">(</span>path<span style="color: #757575;">)</span>
    <span style="color: #268bd2;">bn</span> = [x <span style="color: #859900;">for</span> x <span style="color: #859900;">in</span> model.layers <span style="color: #859900;">if</span> x.name.startswith<span style="color: #757575;">(</span><span style="color: #2aa198;">"batch"</span><span style="color: #757575;">)</span>]
    <span style="color: #859900;">for</span> layer <span style="color: #859900;">in</span> bn:
        <span style="color: #859900;">print</span><span style="color: #757575;">(</span>layer.name<span style="color: #757575;">)</span>
        <span style="color: #586e75;"># </span><span style="color: #586e75;">y=gamma * (x - mean) / sqrt(var+eps) + beta.</span>
        <span style="color: #268bd2;">gamma</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">beta</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">mean</span><span style="color: #757575;">,</span> <span style="color: #268bd2;">var</span> = layer.get_weights<span style="color: #757575;">()</span>
        <span style="color: #268bd2;">eps</span> = 0.001
        <span style="color: #268bd2;">scale</span> = gamma / <span style="color: #757575;">(</span>np.sqrt<span style="color: #757575;">(</span>var + eps<span style="color: #757575;">))</span>
        <span style="color: #268bd2;">bias</span> = beta - gamma * mean
        <span style="color: #859900;">print</span><span style="color: #757575;">(</span>f<span style="color: #2aa198;">"---{layer.name}---"</span><span style="color: #757575;">)</span>
        <span style="color: #859900;">print</span><span style="color: #757575;">(</span>np.<span style="color: #839496;">min</span><span style="color: #757575;">(</span>scale<span style="color: #757575;">),</span> np.<span style="color: #839496;">max</span><span style="color: #757575;">(</span>scale<span style="color: #757575;">))</span>
        <span style="color: #859900;">print</span><span style="color: #757575;">(</span>np.<span style="color: #839496;">min</span><span style="color: #757575;">(</span>bias<span style="color: #757575;">),</span> np.<span style="color: #839496;">max</span><span style="color: #757575;">(</span>bias<span style="color: #757575;">))</span>

        <span style="color: #268bd2;">index</span> = np.argmax<span style="color: #757575;">(</span>scale<span style="color: #757575;">)</span>
        <span style="color: #859900;">print</span><span style="color: #757575;">(</span>index<span style="color: #757575;">,</span> gamma[index]<span style="color: #757575;">,</span> var[index]<span style="color: #757575;">,</span> mean[index]<span style="color: #757575;">)</span>


check_model<span style="color: #757575;">(</span>config.SAVED_MODEL_PATH<span style="color: #757575;">)</span>
</pre>
</div>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-07-21 三 00:00<br />
Last updated: 2022-01-25 二 14:11</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
