<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<title>TFLite Quantization Details</title>

<link rel="stylesheet" type="text/css" href="/main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="../main.css" media="screen" />
<link rel="stylesheet" type="text/css" href="./main.css" media="screen" />
<link rel = "icon" href = "/icon.png"  type = "image/x-icon">
<script type="text/javascript">
// @license magnet:?xt=urn:btih:e95b018ef3580986a04669f1b5879592219e2a7a&dn=public-domain.txt Public Domain
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.add("code-highlighted");
         target.classList.add("code-highlighted");
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.classList.remove("code-highlighted");
         target.classList.remove("code-highlighted");
       }
     }
    /*]]>*///-->
// @license-end
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">TFLite Quantization Details</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0000027">1. TFLite Quantization Details</a>
<ul>
<li><a href="#org0000012">1.1. Mul Reference Kernel</a>
<ul>
<li><a href="#org0000000">1.1.1. Overview</a></li>
<li><a href="#org0000003">1.1.2. rounding_doubling_high_mul</a></li>
<li><a href="#org0000009">1.1.3. Prepare</a></li>
<li><a href="#org000000c">1.1.4. EvalQuantized</a></li>
<li><a href="#org000000f">1.1.5. Reference Kernel</a></li>
</ul>
</li>
<li><a href="#org0000021">1.2. FullyConnected Reference Kernel</a>
<ul>
<li><a href="#org0000015">1.2.1. Quantize</a></li>
<li><a href="#org0000018">1.2.2. Prepare</a></li>
<li><a href="#org000001b">1.2.3. EvalQuantized</a></li>
<li><a href="#org000001e">1.2.4. Reference Kernel</a></li>
</ul>
</li>
<li><a href="#ID-6470583f-2076-4f0e-96e4-aef98fff8a01">1.3. BatchNorm 导致很大的量化误差</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org0000027" class="outline-2">
<h2 id="org0000027"><span class="section-number-2">1</span> TFLite Quantization Details</h2>
<div class="outline-text-2" id="text-1">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc#org0000000">file:///home/sunway/source/tensorflow/tensorflow/lite/tools/optimize/quantize_model.cc#org0000000</a>
</p>
</div>

<div id="outline-container-org0000012" class="outline-3">
<h3 id="org0000012"><span class="section-number-3">1.1</span> Mul Reference Kernel</h3>
<div class="outline-text-3" id="text-1-1">
<p>
以最简单的 Mul 的 reference kernel 为例, 介绍 EvalQuantized 的流程
</p>
</div>

<div id="outline-container-org0000000" class="outline-4">
<h4 id="org0000000"><span class="section-number-4">1.1.1</span> Overview</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
Mul, 即 a*b=c 操作, 量化后实际上需要计算的是:
</p>

<p>
已知 \(S_a, Q_a, Z_a, S_b, Q_b, Z_b, S_c, Z_c\), 根据 \(S_a(Q_a-Z_a)*S_b(Q_b-Z_b)=S_c(Q_c-Z_c)\)  计算出 \(Q_c\)
</p>

<p>
tflite 量化时会保证所有的 \(Z\) 都是整数, 但 \(S\) 是浮点数
</p>
</div>
</div>

<div id="outline-container-org0000003" class="outline-4">
<h4 id="org0000003"><span class="section-number-4">1.1.2</span> rounding_doubling_high_mul</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
rounding_doubling_high_mul 解决的问题是如何用整数运算来计算 int*float
</p>

<div class="org-src-container">
<pre class="src src-C++"><span class="org-preprocessor">#include</span> <span class="org-string">&lt;cmath&gt;</span>
<span class="org-preprocessor">#include</span> <span class="org-string">&lt;cstdint&gt;</span>
<span class="org-preprocessor">#include</span> <span class="org-string">&lt;cstdio&gt;</span>
<span class="org-preprocessor">#include</span> <span class="org-string">&lt;limits&gt;</span>

<span class="org-keyword">inline</span> <span class="org-type">int32_t</span> <span class="org-function-name">SaturatingRoundingDoublingHighMul</span>(<span class="org-type">int32_t</span> <span class="org-variable-name">a</span>, <span class="org-type">int32_t</span> <span class="org-variable-name">b</span>) {
    <span class="org-type">bool</span> <span class="org-variable-name">overflow</span> = a == b &amp;&amp; a == <span class="org-constant">std</span>::<span class="org-constant">numeric_limits</span>&lt;<span class="org-type">int32_t</span>&gt;::min();
    <span class="org-type">int64_t</span> <span class="org-variable-name">a_64</span>(a);
    <span class="org-type">int64_t</span> <span class="org-variable-name">b_64</span>(b);
    <span class="org-type">int64_t</span> <span class="org-variable-name">ab_64</span> = a_64 * b_64;
    <span class="org-type">int32_t</span> <span class="org-variable-name">nudge</span> = ab_64 &gt;= 0 ? (1 &lt;&lt; 30) : (1 - (1 &lt;&lt; 30));
    <span class="org-type">int32_t</span> <span class="org-variable-name">ab_x2_high32</span> = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int32_t</span>&gt;((ab_64 + nudge) / (1ll &lt;&lt; 31));
    <span class="org-keyword">return</span> overflow ? <span class="org-constant">std</span>::<span class="org-constant">numeric_limits</span>&lt;<span class="org-type">int32_t</span>&gt;::max() : ab_x2_high32;
}

<span class="org-type">int</span> <span class="org-function-name">quantized_mulitply</span>(<span class="org-type">int32_t</span> <span class="org-variable-name">a</span>, <span class="org-type">float</span> <span class="org-variable-name">b</span>) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">prepare</span>
    <span class="org-type">int</span> <span class="org-variable-name">shift</span> = 0;
    <span class="org-type">double</span> <span class="org-variable-name">q</span> = <span class="org-constant">std</span>::frexp(b, &amp;shift);
    printf(<span class="org-string">"-- %f %d\n"</span>, q, shift);
    <span class="org-keyword">auto</span> <span class="org-variable-name">q_fixed</span> = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int64_t</span>&gt;(<span class="org-type">int</span>(q * (1ll &lt;&lt; 31)));
    printf(<span class="org-string">"-- %ld\n"</span>, q_fixed);
    <span class="org-comment-delimiter">// </span><span class="org-comment">eval</span>
    <span class="org-type">int32_t</span> <span class="org-variable-name">x</span> = SaturatingRoundingDoublingHighMul(100, (<span class="org-type">int32_t</span>)q_fixed);
    printf(<span class="org-string">"-- %d\n"</span>, x);
    <span class="org-keyword">return</span> x &gt;&gt; (-shift);
}

<span class="org-type">int</span> <span class="org-function-name">main</span>(<span class="org-type">int</span> <span class="org-variable-name">argc</span>, <span class="org-type">char</span> *<span class="org-variable-name">argv</span>[]) {
    <span class="org-type">int32_t</span> <span class="org-variable-name">a</span> = 100;
    <span class="org-type">float</span> <span class="org-variable-name">b</span> = 0.12;
    printf(<span class="org-string">"%d * %f = %d\n"</span>, a, b, quantized_mulitply(a, b));
    <span class="org-keyword">return</span> 0;
}

</pre>
</div>

<p>
&#x2013; 0.768000 -6
&#x2013; 1649267456
&#x2013; 77
100 * 0.012000 = 1
</p>
</div>
</div>

<div id="outline-container-org0000009" class="outline-4">
<h4 id="org0000009"><span class="section-number-4">1.1.3</span> Prepare</h4>
<div class="outline-text-4" id="text-1-1-3">
<p>

</p>

<div class="org-src-container">
<pre class="src src-c++">mul.<span class="org-constant">cc</span>:<span class="org-constant">Prepare</span>:
    <span class="org-comment-delimiter">// </span><span class="org-comment">S_a(Q_a-Z_a)*S_b(Q_b-Z_b)=S_c(Q_c-Z_c)</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">real_multiplier &#20026; (S_a*S_b)/(S_c)</span>
    <span class="org-type">double</span> <span class="org-variable-name">real_multiplier</span> =
        input1-&gt;params.scale * input2-&gt;params.scale / output-&gt;params.scale;

    <span class="org-comment-delimiter">// </span><span class="org-comment">QuantizeMultiplier &#20250;&#25226; real_multiplier &#36716;&#25442;&#20026;&#19968;&#20010;&#25972;&#25968;&#21644;&#19968;&#20010; shift &#30340;&#24418;&#24335;</span>
    QuantizeMultiplier(real_multiplier, &amp;data-&gt;output_multiplier,
                       &amp;data-&gt;output_shift);
</pre>
</div>
</div>

<div id="outline-container-org0000006" class="outline-5">
<h5 id="org0000006"><span class="section-number-5">1.1.3.1</span> QuantizeMultiplier</h5>
<div class="outline-text-5" id="text-1-1-3-1">
<p>
QuantizeMultiplier 会把浮点的 real_multiplier (即 scale) 变为 int/shift 的形式,
例如, 若 real_multiplier 为 0.012, 则 quantized_multiplier 为 1649267456, shift
为 -6 (参考 <a href="#org0000003">rounding_doubling_high_mul</a>)
</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-type">void</span> <span class="org-function-name">QuantizeMultiplier</span>(<span class="org-type">double</span> <span class="org-variable-name">double_multiplier</span>, <span class="org-type">int32_t</span>* <span class="org-variable-name">quantized_multiplier</span>,
                        <span class="org-type">int</span>* <span class="org-variable-name">shift</span>) {
  <span class="org-keyword">const</span> <span class="org-type">double</span> <span class="org-variable-name">q</span> = <span class="org-constant">std</span>::frexp(double_multiplier, shift);
  <span class="org-keyword">auto</span> <span class="org-variable-name">q_fixed</span> = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int64_t</span>&gt;(TfLiteRound(q * (1ll &lt;&lt; 31)));
  *quantized_multiplier = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int32_t</span>&gt;(q_fixed);
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org000000c" class="outline-4">
<h4 id="org000000c"><span class="section-number-4">1.1.4</span> EvalQuantized</h4>
<div class="outline-text-4" id="text-1-1-4">
<p>

</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">template</span> &lt;<span class="org-type">KernelType</span> <span class="org-variable-name">kernel_type</span>&gt;
<span class="org-type">TfLiteStatus</span> <span class="org-function-name">EvalQuantized</span>(
    <span class="org-type">TfLiteContext</span>* <span class="org-variable-name">context</span>, <span class="org-type">TfLiteNode</span>* <span class="org-variable-name">node</span>, <span class="org-type">TfLiteMulParams</span>* <span class="org-variable-name">params</span>,
    <span class="org-keyword">const</span> <span class="org-type">OpData</span>* <span class="org-variable-name">data</span>, <span class="org-keyword">const</span> <span class="org-type">TfLiteTensor</span>* <span class="org-variable-name">input1</span>, <span class="org-keyword">const</span> <span class="org-type">TfLiteTensor</span>* <span class="org-variable-name">input2</span>,
    <span class="org-type">TfLiteTensor</span>* <span class="org-variable-name">output</span>) {

    <span class="org-constant">tflite</span>::<span class="org-type">ArithmeticParams</span> <span class="org-variable-name">op_params</span>;
    op_params.input1_offset = -input1-&gt;params.zero_point;
    op_params.input2_offset = -input2-&gt;params.zero_point;
    op_params.output_offset = output-&gt;params.zero_point;
    op_params.output_multiplier = data-&gt;output_multiplier;
    op_params.output_shift = data-&gt;output_shift;

<span class="org-preprocessor">#define</span> <span class="org-function-name">TF_LITE_MUL</span>(<span class="org-variable-name">type</span>, <span class="org-variable-name">opname</span>, <span class="org-variable-name">dtype</span>)                                 \
    <span class="org-constant">type</span>::opname(                                                        \
        op_params, GetTensorShape(input1), GetTensorData&lt;dtype&gt;(input1), \
        GetTensorShape(input2), GetTensorData&lt;dtype&gt;(input2),            \
        GetTensorShape(output), GetTensorData&lt;dtype&gt;(output))

    <span class="org-keyword">if</span> (input1-&gt;type == kTfLiteInt8) {
        TF_LITE_MUL(reference_integer_ops, Mul, int8_t);
    }
}
</pre>
</div>

<ul class="org-ul">
<li>reference kernel 比较低效: 它会针对每个元素计算乘积, 实际上在 x86 上的 mul
optimized kernel 同样使用 Elementwise Mul.</li>

<li><p>
tflite 的 optimized kernel 只对 fully_connected, conv 等使用 来做 gemm
</p>

<p>

</p></li>
</ul>
</div>
</div>

<div id="outline-container-org000000f" class="outline-4">
<h4 id="org000000f"><span class="section-number-4">1.1.5</span> Reference Kernel</h4>
<div class="outline-text-4" id="text-1-1-5">
<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">template</span> &lt;<span class="org-keyword">typename</span> <span class="org-type">T</span>&gt;
<span class="org-keyword">inline</span> <span class="org-type">void</span> <span class="org-function-name">MulElementwise</span>(
    <span class="org-type">int</span> <span class="org-variable-name">size</span>, <span class="org-keyword">const</span> <span class="org-type">ArithmeticParams</span>&amp; <span class="org-variable-name">params</span>, <span class="org-keyword">const</span> <span class="org-type">T</span>* <span class="org-variable-name">input1_data</span>,
    <span class="org-keyword">const</span> <span class="org-type">T</span>* <span class="org-variable-name">input2_data</span>, <span class="org-type">T</span>* <span class="org-variable-name">output_data</span>) {
    <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">i</span> = 0; i &lt; size; ++i) {
        <span class="org-comment-delimiter">// </span><span class="org-comment">Q_a-Z_a</span>
        <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">input1_val</span> = params.input1_offset + input1_data[i];
        <span class="org-comment-delimiter">// </span><span class="org-comment">Q_b-Z_b</span>
        <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">input2_val</span> = params.input2_offset + input2_data[i];
        <span class="org-comment-delimiter">// </span><span class="org-comment">Q_c=Z_c+real_multiplier*((Q_a-Z_a)*(Q_b-Z_b))</span>
        <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">output</span> = params.output_offset +
                             MultiplyByQuantizedMultiplier(
                                 input1_val * input2_val,
                                 params.output_multiplier, params.output_shift);
        output_data[i] = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">T</span>&gt;(output);
    }
}
</pre>
</div>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">inline</span> <span class="org-type">int32</span> <span class="org-function-name">MultiplyByQuantizedMultiplier</span>(
    <span class="org-type">int32</span> <span class="org-variable-name">x</span>, <span class="org-type">int32</span> <span class="org-variable-name">quantized_multiplier</span>, <span class="org-type">int</span> <span class="org-variable-name">shift</span>) {
    <span class="org-keyword">using</span> <span class="org-constant">gemmlowp</span>::<span class="org-type">RoundingDivideByPOT</span>;
    <span class="org-keyword">using</span> <span class="org-constant">gemmlowp</span>::<span class="org-type">SaturatingRoundingDoublingHighMul</span>;
    <span class="org-type">int</span> <span class="org-variable-name">right_shift</span> = -shift;
    <span class="org-keyword">return</span> RoundingDivideByPOT(
        SaturatingRoundingDoublingHighMul(x, quantized_multiplier),
        right_shift);
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org0000021" class="outline-3">
<h3 id="org0000021"><span class="section-number-3">1.2</span> FullyConnected Reference Kernel</h3>
<div class="outline-text-3" id="text-1-2">
</div>
<div id="outline-container-org0000015" class="outline-4">
<h4 id="org0000015"><span class="section-number-4">1.2.1</span> Quantize</h4>
<div class="outline-text-4" id="text-1-2-1">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/toco/graph_transformations/quantize.cc#org0000000">file:///home/sunway/source/tensorflow/tensorflow/lite/toco/graph_transformations/quantize.cc#org0000000</a>
</p>

<p>
针对 FullyConnected 进行 quantize 时, 先强制要求 bias 的 scale == input_scale *
weight_scale, 以便后面 \(Q_w*Q_x+Q_b\) 可以在相同的 scale 下进行计算
</p>

<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">if</span> (is_bias_vector) {
    <span class="org-comment-delimiter">// </span><span class="org-comment">Quantization of bias vector.</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">We need both of the mandatory inputs (input activations and weights) to</span>
    <span class="org-comment-delimiter">// </span><span class="org-comment">have been already quantized.</span>
    <span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">input_activations</span> =
        model-&gt;GetArray(op.inputs[activations_input_index]);
    <span class="org-keyword">const</span> <span class="org-keyword">auto</span>&amp; <span class="org-variable-name">input_weights</span> = model-&gt;GetArray(op.inputs[weights_input_index]);

    <span class="org-keyword">const</span> <span class="org-keyword">auto</span> <span class="org-variable-name">input_activations_scale</span> =
        input_activations.quantization_params-&gt;scale;
    <span class="org-keyword">const</span> <span class="org-keyword">auto</span> <span class="org-variable-name">input_weights_scale</span> = input_weights.quantization_params-&gt;scale;
    quantization_params-&gt;scale = input_activations_scale * input_weights_scale;
    quantization_params-&gt;zero_point = 0;
    *quantized_data_type = GetQuantizedDataType(array, <span class="org-constant">ArrayDataType</span>::kInt32);
    <span class="org-keyword">return</span> <span class="org-constant">true</span>;
}
</pre>
</div>
</div>
</div>

<div id="outline-container-org0000018" class="outline-4">
<h4 id="org0000018"><span class="section-number-4">1.2.2</span> Prepare</h4>
<div class="outline-text-4" id="text-1-2-2">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#org0000000">file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#org0000000</a>
Prepare 与 Mul 的 Prepare 基本相同: 它也会提前计算 real_multiplier =
input_multiplier * weight_multiplier, 以及 QuantizeMultiplier. 需要注意的是 bias
的 scale 不需要考虑, 因为 quantize_model 时保证了它与 real_multiplier 是相同的
</p>
</div>
</div>

<div id="outline-container-org000001b" class="outline-4">
<h4 id="org000001b"><span class="section-number-4">1.2.3</span> EvalQuantized</h4>
<div class="outline-text-4" id="text-1-2-3">
<p>
<a href="file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#org0000002">file:///home/sunway/source/tensorflow/tensorflow/lite/kernels/fully_connected.cc#org0000002</a>
</p>

<p>
EvalQuantized 与 Mul 的 EvalQuantized 也是基本相同
</p>
</div>
</div>

<div id="outline-container-org000001e" class="outline-4">
<h4 id="org000001e"><span class="section-number-4">1.2.4</span> Reference Kernel</h4>
<div class="outline-text-4" id="text-1-2-4">
<div class="org-src-container">
<pre class="src src-c++"><span class="org-keyword">inline</span> <span class="org-type">void</span> <span class="org-function-name">FullyConnected</span>(
    <span class="org-keyword">const</span> <span class="org-type">FullyConnectedParams</span>&amp; <span class="org-variable-name">params</span>, <span class="org-keyword">const</span> <span class="org-type">RuntimeShape</span>&amp; <span class="org-variable-name">input_shape</span>,
    <span class="org-keyword">const</span> <span class="org-type">int8_t</span>* <span class="org-variable-name">input_data</span>, <span class="org-keyword">const</span> <span class="org-type">RuntimeShape</span>&amp; <span class="org-variable-name">filter_shape</span>,
    <span class="org-keyword">const</span> <span class="org-type">int8_t</span>* <span class="org-variable-name">filter_data</span>, <span class="org-keyword">const</span> <span class="org-type">RuntimeShape</span>&amp; <span class="org-variable-name">bias_shape</span>,
    <span class="org-keyword">const</span> <span class="org-type">int32</span>* <span class="org-variable-name">bias_data</span>, <span class="org-keyword">const</span> <span class="org-type">RuntimeShape</span>&amp; <span class="org-variable-name">output_shape</span>,
    <span class="org-type">int8_t</span>* <span class="org-variable-name">output_data</span>) {
    <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">input_offset</span> = params.input_offset;
    <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">filter_offset</span> = params.weights_offset;
    <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">output_offset</span> = params.output_offset;
    <span class="org-keyword">const</span> <span class="org-type">int32</span> <span class="org-variable-name">output_multiplier</span> = params.output_multiplier;
    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">output_shift</span> = params.output_shift;

    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">filter_dim_count</span> = filter_shape.DimensionsCount();
    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">batches</span> = output_shape.Dims(0);
    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">output_depth</span> = output_shape.Dims(1);

    <span class="org-keyword">const</span> <span class="org-type">int</span> <span class="org-variable-name">accum_depth</span> = filter_shape.Dims(filter_dim_count - 1);
    <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">b</span> = 0; b &lt; batches; ++b) {
        <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">out_c</span> = 0; out_c &lt; output_depth; ++out_c) {
            <span class="org-type">int32</span> <span class="org-variable-name">acc</span> = 0;
            <span class="org-keyword">for</span> (<span class="org-type">int</span> <span class="org-variable-name">d</span> = 0; d &lt; accum_depth; ++d) {
                <span class="org-type">int32</span> <span class="org-variable-name">input_val</span> = input_data[b * accum_depth + d];
                <span class="org-type">int32</span> <span class="org-variable-name">filter_val</span> = filter_data[out_c * accum_depth + d];
                acc +=
                    (filter_val + filter_offset) * (input_val + input_offset);
            }
            <span class="org-keyword">if</span> (bias_data) {
                <span class="org-comment-delimiter">// </span><span class="org-comment">&#36825;&#37324; bias &#21487;&#20197;&#30452;&#25509;&#30456;&#21152;, &#22240;&#20026; bias &#19982; w*x &#30340; scale &#30456;&#21516;</span>
                acc += bias_data[out_c];
            }
            acc = MultiplyByQuantizedMultiplier(
                acc, output_multiplier, output_shift);
            acc += output_offset;
            output_data[out_c + output_depth * b] = <span class="org-keyword">static_cast</span>&lt;<span class="org-type">int8_t</span>&gt;(acc);
        }
    }
}
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-ID-6470583f-2076-4f0e-96e4-aef98fff8a01" class="outline-3">
<h3 id="ID-6470583f-2076-4f0e-96e4-aef98fff8a01"><span class="section-number-3">1.3</span> BatchNorm 导致很大的量化误差</h3>
<div class="outline-text-3" id="text-1-3">
<p>
当训练很多个 epoch 后, 会发现量化误差很大, 通过观察 tflite 量化的数据,发现原因是
batchnorm 对应的 mul 操作的 scale 的最大值很大, 导致很大的量化误差.
</p>

<p>
根据 batchnorm 的公式, scale = gamma / (np.sqrt(var + eps)), 通过 log 可以看到,
随着训练 epoch 的增加, min(var) 和 min(mean) 都变得很小 (例如 1e-3&#x2026;), 导致
scale 变大.
</p>

<p>
var 和 mean 很接近 0, 说明 hidden layer 输出有零， 有两种可能的原因:
</p>

<ol class="org-ol">
<li><p>
hidden layer 中对应的权重接近于零
</p>

<p>
\(\left\{\begin{array}{c}w*x_1+b=0\\w*x_2+b=0\\x_1 \ne x_2\end{array} \implies
   w=0\)
</p></li>

<li>或者 hidden layer 输出因为 relu 变为零 (Gradient Starvation)</li>
</ol>

<p>
解决方法:
</p>

<ol class="org-ol">
<li>减少 batchnorm 前面 layer 的规模, 避免出现接近零的权重</li>
<li>使用 leaky relu 等 relu 变种，避免 relu 导致的神经元死亡的情况.</li>
<li>relu 放在 batchnorm 之后而不是之前</li>
</ol>

<p>
实际上在这个例子中，出现问题的原因是 relu 导致 dense 输出的某些点针对所有样本都为零, 即有些神经元已经死亡.
</p>

<p>
通过下面的脚本可以观察网络中 batchnorm 的 scale 的情况.
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python3</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">-*- coding: utf-8 -*-</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">2021-08-13 09:58</span>
<span class="org-keyword">import</span> numpy <span class="org-keyword">as</span> np

<span class="org-keyword">from</span> models <span class="org-keyword">import</span> deep_cnn
<span class="org-keyword">import</span> config


<span class="org-keyword">def</span> <span class="org-function-name">check_model</span>(path):
    <span class="org-keyword">print</span>(f<span class="org-string">"------{path}------"</span>)
    <span class="org-variable-name">model</span> = deep_cnn()
    model.load_weights(path)
    <span class="org-variable-name">bn</span> = [x <span class="org-keyword">for</span> x <span class="org-keyword">in</span> model.layers <span class="org-keyword">if</span> x.name.startswith(<span class="org-string">"batch"</span>)]
    <span class="org-keyword">for</span> layer <span class="org-keyword">in</span> bn:
        <span class="org-keyword">print</span>(layer.name)
        <span class="org-comment-delimiter"># </span><span class="org-comment">y=gamma * (x - mean) / sqrt(var+eps) + beta.</span>
        <span class="org-variable-name">gamma</span>, <span class="org-variable-name">beta</span>, <span class="org-variable-name">mean</span>, <span class="org-variable-name">var</span> = layer.get_weights()
        <span class="org-variable-name">eps</span> = 0.001
        <span class="org-variable-name">scale</span> = gamma / (np.sqrt(var + eps))
        <span class="org-variable-name">bias</span> = beta - gamma * mean
        <span class="org-keyword">print</span>(f<span class="org-string">"---{layer.name}---"</span>)
        <span class="org-keyword">print</span>(np.<span class="org-builtin">min</span>(scale), np.<span class="org-builtin">max</span>(scale))
        <span class="org-keyword">print</span>(np.<span class="org-builtin">min</span>(bias), np.<span class="org-builtin">max</span>(bias))

        <span class="org-variable-name">index</span> = np.argmax(scale)
        <span class="org-keyword">print</span>(index, gamma[index], var[index], mean[index])


check_model(config.SAVED_MODEL_PATH)
</pre>
</div>
</div>


<div id="outline-container-org0000024" class="outline-4 references">
<h4 id="org0000024">Backlinks</h4>
<div class="outline-text-4" id="text-org0000024">
<p>
<a href="../machine_learning/batchnorm.html#ID-ffa885c6-f5a3-4939-8e61-4ebd2bbbe836">Batch Normalization</a>
(<i>Batch Normalization &gt; batchnorm-&gt;relu vs. relu-&gt;batchnorm &gt; batchnorm-&gt;relu 的优点</i>):  另一方面, relu 放在 batchnorm 之前可能会出来 <a href="#ID-6470583f-2076-4f0e-96e4-aef98fff8a01">BatchNorm 导致很大的量化误差</a> 的问题
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-org000002a" class="outline-2 references">
<h2 id="org000002a">Backlinks</h2>
<div class="outline-text-2" id="text-org000002a">
<p>
<a href="quantization.html#ID-6ad56dfa-9772-478b-9872-6f8294d6cb19">Quantization</a>
(<i>Quantization &gt; TFLite Quantization Details</i>):   <a href="tflite_quantization_detail.html#ID-58be2f99-33f3-402f-bf0c-eb81e02ce5d5">TFLite Quantization Details</a>
</p>
</div>
</div>
</div>
<div id="postamble" class="status">

<p class="author">Author: sunway (sunwayforever@gmail.com)<br />
Date: 2021-07-21 Wed 00:00<br />
Last updated: 2022-01-25 Tue 14:14</p>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="知识共享许可协议" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br />本作品采用<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>进行许可。
<br />

<div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
         var d = document, s = d.createElement('script');
         s.src = '//sunwayforever-github-io.disqus.com/embed.js';
         s.setAttribute('data-timestamp', +new Date());
         (d.head || d.body).appendChild(s);
         })();
</script>
</div>
</body>
</html>
